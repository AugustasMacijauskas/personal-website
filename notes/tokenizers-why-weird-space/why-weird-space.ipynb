{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Why is the Ġ Unicode character all over the place in LLM tokenizers?\n",
    "description: There does not seem to be any good reasons for the Ġ character to be used in LLM tokenizers. This article explores the history of the Ġ character in LLM tokenizers and why it is used.\n",
    "author: Augustas Macijauskas\n",
    "date: \"2024/04/24\"\n",
    "image: dalle-3-why-weird-space.webp\n",
    "image-alt: LLM making a writing mistake as imaged by DALL-E 3\n",
    "toc: true\n",
    "categories: [deep learning, LLMs, tokenization]\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TLDR**: <ins>WHY</ins> do tokenizers represent spaces `\" \"` with the Unicode character `Ġ`?\n",
    "\n",
    "Hello, World!\n",
    "\n",
    "While working on a different blog post where I plan to dive deeper into training LLM tokenizers using the Hugging Face `tokenizers` library, I could not help but notice the `Ġ` Unicode character being omnipresent in the vocabularies of seemingly **all** tokenizers based on the <a href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding\" target=\"_blank\">*byte pair encoding* (BPE)</a> algorithm. I also noticed that it has something to do with replacing the space `\" \"` character, for example, a `Llama 3 8B Instruct` tokenizer would produce the following outputs:\n",
    "\n",
    "```python\n",
    ">>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    ">>> tokenizer.tokenize(\"Some text to be tokenized\")\n",
    "['Some', 'Ġtext', 'Ġto', 'Ġbe', 'Ġtoken', 'ized']\n",
    "```\n",
    "\n",
    "I have tried looking for an answer online, but found out that people were as confused as I was, below are a few issues on GitHub only:\n",
    "\n",
    "1. <a href=\"https://github.com/openai/gpt-2/issues/80\" target=\"_blank\">Why \\u0120 (Ġ) is in so many pairs?</a>\n",
    "1. <a href=\"https://github.com/openai/gpt-2/issues/185\" target=\"_blank\">Confused about vocab.bpe and encoder.json</a>\n",
    "1. <a href=\"https://github.com/allenai/OLMo/issues/505\" target=\"_blank\">Could some one help why the tokens begin with G.</a>\n",
    "\n",
    "There are many other similar questions, but all of the answers seem to simply suggest that <a href=\"https://github.com/huggingface/transformers/issues/3867#issuecomment-616956437)\" target=\"_blank\">it is a feature of the BPE algorithm</a>, but neither of the discussions answered **why** should such a feature exists in the first place.\n",
    "\n",
    "This blog post is my attempt at finding a more satisfying answer which (spoiler alert!) seems to be that replacing spaces with the character `Ġ` could well be a historical artifact and not at all a strict design requirement. Read on to see what I came up with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "After doing some background reading, I found that it is essentially the <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\" target=\"_blank\">`GPT-2`</a> paper that popularized the use of BPE-based tokenizers. Digging through <a href=\"https://github.com/openai/gpt-2/\" target=\"_blank\">their code</a>, I came up with the following idea: <ins>*if there was never a good reason to replace spaces with the character `Ġ`, could we revert back to using spaces for encoding and decoding strings?*</ins>\n",
    "\n",
    "More specifically, we are going to do the following:\n",
    "\n",
    "1. Refactor OpenAI's code of the original `GPT-2` tokenizer to be able to operate spaces instead of `Ġ's`.\n",
    "1. Test on a diverse dataset to see if the encoding behaviour remains the same.\n",
    "    - It turns out that we will not need to change the decoding functionality, so long as encoding behaves the same, we do not need to test the decoding bit.\n",
    "\n",
    "Let's jump into the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring the GPT-2 tokenizer code\n",
    "\n",
    "First, let us see how the original `GPT-2` tokenizer's code has to be changed to accommodate spaces. The original implementation can be found <a href=\"https://github.com/openai/gpt-2/blob/master/src/encoder.py\" target=\"_blank\">here</a>. The collapsed cell below downloads the files necessary for the `GPT-2` tokenizer to work. \n",
    "\n",
    ":::{.callout-note}\n",
    "Somewhat confusingly, the original authors of the repository named the files `vocab.bpe` and `encoder.json`, whereas more appropriate names would be `marges.bpe` and `vocab.json` based on the contents of the files.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Oi5gbToTf2Nb"
   },
   "outputs": [],
   "source": [
    "#|code-fold: true\n",
    "#|output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|code-fold: true\n",
    "#|output: false\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "Path(\"gpt2-tokenizer\").mkdir(exist_ok=True)\n",
    "\n",
    "GPT2_URL_BASE = \"https://openaipublic.blob.core.windows.net/gpt-2/models/1558M\"\n",
    "if not Path(\"gpt2-tokenizer/vocab.bpe\").exists():\n",
    "    !wget -q {GPT2_URL_BASE}/vocab.bpe -P gpt2-tokenizer/\n",
    "if not Path(\"gpt2-tokenizer/encoder.json\").exists():\n",
    "    !wget -q {GPT2_URL_BASE}/encoder.json -P gpt2-tokenizer/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">The first change that we have to make is...</font>\n",
    "\n",
    "<font color=\"red\">Explain what the `get_bytes_to_unicode_base_mapping()` is and why it exists, highlight that it is where the `Ġ's` would be introduced, but emphasize that it does not seem like spaces would not work</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, '!'), (34, '\"'), (35, '#'), (36, '$'), (37, '%')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt2_tokenizer_custom import get_bytes_to_unicode_base_mapping\n",
    "\n",
    "list(get_bytes_to_unicode_base_mapping().items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 0),\n",
       " ('\"', 1),\n",
       " ('#', 2),\n",
       " ('$', 3),\n",
       " ('%', 4),\n",
       " ('&', 5),\n",
       " (\"'\", 6),\n",
       " ('(', 7),\n",
       " (')', 8),\n",
       " ('*', 9),\n",
       " ('+', 10),\n",
       " (',', 11),\n",
       " ('-', 12),\n",
       " ('.', 13),\n",
       " ('/', 14),\n",
       " ('0', 15),\n",
       " ('1', 16),\n",
       " ('2', 17),\n",
       " ('3', 18),\n",
       " ('4', 19),\n",
       " ('5', 20),\n",
       " ('6', 21),\n",
       " ('7', 22),\n",
       " ('8', 23),\n",
       " ('9', 24),\n",
       " (':', 25),\n",
       " (';', 26),\n",
       " ('<', 27),\n",
       " ('=', 28),\n",
       " ('>', 29),\n",
       " ('?', 30),\n",
       " ('@', 31),\n",
       " ('A', 32),\n",
       " ('B', 33),\n",
       " ('C', 34),\n",
       " ('D', 35),\n",
       " ('E', 36),\n",
       " ('F', 37),\n",
       " ('G', 38),\n",
       " ('H', 39),\n",
       " ('I', 40),\n",
       " ('J', 41),\n",
       " ('K', 42),\n",
       " ('L', 43),\n",
       " ('M', 44),\n",
       " ('N', 45),\n",
       " ('O', 46),\n",
       " ('P', 47),\n",
       " ('Q', 48),\n",
       " ('R', 49),\n",
       " ('S', 50),\n",
       " ('T', 51),\n",
       " ('U', 52),\n",
       " ('V', 53),\n",
       " ('W', 54),\n",
       " ('X', 55),\n",
       " ('Y', 56),\n",
       " ('Z', 57),\n",
       " ('[', 58),\n",
       " ('\\\\', 59),\n",
       " (']', 60),\n",
       " ('^', 61),\n",
       " ('_', 62),\n",
       " ('`', 63),\n",
       " ('a', 64),\n",
       " ('b', 65),\n",
       " ('c', 66),\n",
       " ('d', 67),\n",
       " ('e', 68),\n",
       " ('f', 69),\n",
       " ('g', 70),\n",
       " ('h', 71),\n",
       " ('i', 72),\n",
       " ('j', 73),\n",
       " ('k', 74),\n",
       " ('l', 75),\n",
       " ('m', 76),\n",
       " ('n', 77),\n",
       " ('o', 78),\n",
       " ('p', 79),\n",
       " ('q', 80),\n",
       " ('r', 81),\n",
       " ('s', 82),\n",
       " ('t', 83),\n",
       " ('u', 84),\n",
       " ('v', 85),\n",
       " ('w', 86),\n",
       " ('x', 87),\n",
       " ('y', 88),\n",
       " ('z', 89),\n",
       " ('{', 90),\n",
       " ('|', 91),\n",
       " ('}', 92),\n",
       " ('~', 93),\n",
       " ('¡', 94),\n",
       " ('¢', 95),\n",
       " ('£', 96),\n",
       " ('¤', 97),\n",
       " ('¥', 98),\n",
       " ('¦', 99),\n",
       " ('§', 100),\n",
       " ('¨', 101),\n",
       " ('©', 102),\n",
       " ('ª', 103),\n",
       " ('«', 104),\n",
       " ('¬', 105),\n",
       " ('®', 106),\n",
       " ('¯', 107),\n",
       " ('°', 108),\n",
       " ('±', 109),\n",
       " ('²', 110),\n",
       " ('³', 111),\n",
       " ('´', 112),\n",
       " ('µ', 113),\n",
       " ('¶', 114),\n",
       " ('·', 115),\n",
       " ('¸', 116),\n",
       " ('¹', 117),\n",
       " ('º', 118),\n",
       " ('»', 119),\n",
       " ('¼', 120),\n",
       " ('½', 121),\n",
       " ('¾', 122),\n",
       " ('¿', 123),\n",
       " ('À', 124),\n",
       " ('Á', 125),\n",
       " ('Â', 126),\n",
       " ('Ã', 127),\n",
       " ('Ä', 128),\n",
       " ('Å', 129),\n",
       " ('Æ', 130),\n",
       " ('Ç', 131),\n",
       " ('È', 132),\n",
       " ('É', 133),\n",
       " ('Ê', 134),\n",
       " ('Ë', 135),\n",
       " ('Ì', 136),\n",
       " ('Í', 137),\n",
       " ('Î', 138),\n",
       " ('Ï', 139),\n",
       " ('Ð', 140),\n",
       " ('Ñ', 141),\n",
       " ('Ò', 142),\n",
       " ('Ó', 143),\n",
       " ('Ô', 144),\n",
       " ('Õ', 145),\n",
       " ('Ö', 146),\n",
       " ('×', 147),\n",
       " ('Ø', 148),\n",
       " ('Ù', 149),\n",
       " ('Ú', 150),\n",
       " ('Û', 151),\n",
       " ('Ü', 152),\n",
       " ('Ý', 153),\n",
       " ('Þ', 154),\n",
       " ('ß', 155),\n",
       " ('à', 156),\n",
       " ('á', 157),\n",
       " ('â', 158),\n",
       " ('ã', 159),\n",
       " ('ä', 160),\n",
       " ('å', 161),\n",
       " ('æ', 162),\n",
       " ('ç', 163),\n",
       " ('è', 164),\n",
       " ('é', 165),\n",
       " ('ê', 166),\n",
       " ('ë', 167),\n",
       " ('ì', 168),\n",
       " ('í', 169),\n",
       " ('î', 170),\n",
       " ('ï', 171),\n",
       " ('ð', 172),\n",
       " ('ñ', 173),\n",
       " ('ò', 174),\n",
       " ('ó', 175),\n",
       " ('ô', 176),\n",
       " ('õ', 177),\n",
       " ('ö', 178),\n",
       " ('÷', 179),\n",
       " ('ø', 180),\n",
       " ('ù', 181),\n",
       " ('ú', 182),\n",
       " ('û', 183),\n",
       " ('ü', 184),\n",
       " ('ý', 185),\n",
       " ('þ', 186),\n",
       " ('ÿ', 187),\n",
       " ('Ā', 188),\n",
       " ('ā', 189),\n",
       " ('Ă', 190),\n",
       " ('ă', 191),\n",
       " ('Ą', 192),\n",
       " ('ą', 193),\n",
       " ('Ć', 194),\n",
       " ('ć', 195),\n",
       " ('Ĉ', 196),\n",
       " ('ĉ', 197),\n",
       " ('Ċ', 198),\n",
       " ('ċ', 199),\n",
       " ('Č', 200),\n",
       " ('č', 201),\n",
       " ('Ď', 202),\n",
       " ('ď', 203),\n",
       " ('Đ', 204),\n",
       " ('đ', 205),\n",
       " ('Ē', 206),\n",
       " ('ē', 207),\n",
       " ('Ĕ', 208),\n",
       " ('ĕ', 209),\n",
       " ('Ė', 210),\n",
       " ('ė', 211),\n",
       " ('Ę', 212),\n",
       " ('ę', 213),\n",
       " ('Ě', 214),\n",
       " ('ě', 215),\n",
       " ('Ĝ', 216),\n",
       " ('ĝ', 217),\n",
       " ('Ğ', 218),\n",
       " ('ğ', 219),\n",
       " ('Ġ', 220),\n",
       " ('ġ', 221),\n",
       " ('Ģ', 222),\n",
       " ('ģ', 223),\n",
       " ('Ĥ', 224),\n",
       " ('ĥ', 225),\n",
       " ('Ħ', 226),\n",
       " ('ħ', 227),\n",
       " ('Ĩ', 228),\n",
       " ('ĩ', 229),\n",
       " ('Ī', 230),\n",
       " ('ī', 231),\n",
       " ('Ĭ', 232),\n",
       " ('ĭ', 233),\n",
       " ('Į', 234),\n",
       " ('į', 235),\n",
       " ('İ', 236),\n",
       " ('ı', 237),\n",
       " ('Ĳ', 238),\n",
       " ('ĳ', 239),\n",
       " ('Ĵ', 240),\n",
       " ('ĵ', 241),\n",
       " ('Ķ', 242),\n",
       " ('ķ', 243),\n",
       " ('ĸ', 244),\n",
       " ('Ĺ', 245),\n",
       " ('ĺ', 246),\n",
       " ('Ļ', 247),\n",
       " ('ļ', 248),\n",
       " ('Ľ', 249),\n",
       " ('ľ', 250),\n",
       " ('Ŀ', 251),\n",
       " ('ŀ', 252),\n",
       " ('Ł', 253),\n",
       " ('ł', 254),\n",
       " ('Ń', 255)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer_original.encoder.items())[:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Other code changes are ...</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def bytes_to_unicode():\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "# --- vs ---\n",
    "\n",
    "def get_bytes_to_unicode_base_mapping():\n",
    "    # Return almost exactly the same mapping, just with the space character \"Ġ\" replaced with \" \"\n",
    "    return {**bytes_to_unicode(), 32: \" \"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class GPT2TokenizerCustom(Encoder):\n",
    "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
    "        # Only the byte_encoder attribute is changed, everything else is the same\n",
    "        super().__init__(encoder, bpe_merges, errors)\n",
    "        self.byte_encoder = get_bytes_to_unicode_base_mapping()\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            \n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            \n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        \n",
    "        # --- New code ---\n",
    "        \n",
    "        # Old code: `word =  \" \".join(word)`\n",
    "\n",
    "        # New code:\n",
    "        word = tuple(word)\n",
    "\n",
    "        # --- End of new code ---\n",
    "        \n",
    "        self.cache[token] = word\n",
    "        \n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "            \n",
    "            # --- New code ---\n",
    "            \n",
    "            # Old code:\n",
    "            # `bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))`\n",
    "            \n",
    "            # New code: the split(\" \") is removed:\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token))\n",
    "            \n",
    "            # --- End of new code ---\n",
    "        \n",
    "        return bpe_tokens\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the refactored code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2_tokenizer import get_encoder\n",
    "from gpt2_tokenizer_custom import get_encoder_custom\n",
    "\n",
    "tokenizer_original = get_encoder(\"\", \"gpt2-tokenizer\")\n",
    "tokenizer_custom = get_encoder_custom(\"\", \"gpt2-tokenizer\")\n",
    "\n",
    "simple_test_string = \"ĠHello've world123 how's     it going!!!?   \\u0120 anotherĠ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128, 254, 15496, 1053, 995, 10163, 703, 338, 220, 220, 220, 220, 340, 1016, 10185, 30, 220, 220, 34754, 254, 1194, 128, 254\n",
      "128, 254, 15496, 1053, 995, 10163, 703, 338, 220, 220, 220, 220, 340, 1016, 10185, 30, 220, 220, 34754, 254, 1194, 128, 254\n"
     ]
    }
   ],
   "source": [
    "expected_output = tokenizer_original.encode(simple_test_string)\n",
    "print(\", \".join(map(str, expected_output)))\n",
    "\n",
    "output = tokenizer_custom.encode(simple_test_string)\n",
    "print(\", \".join(map(str, output)))\n",
    "\n",
    "assert output == expected_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a more diverse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(final_dataset)=30000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"\n",
    "english_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=split)\n",
    "korean_dataset = load_dataset(\"lcw99/wikipedia-korean-20221001\", split=split)\n",
    "code_dataset = load_dataset(\"code_search_net\", \"python\", split=split, trust_remote_code=True)\n",
    "code_dataset = code_dataset.rename_column(\"whole_func_string\", \"text\")  # Rename whole_func_string to text\n",
    "\n",
    "n = 10000\n",
    "final_dataset = (\n",
    "    english_dataset.shuffle(42).select(range(n))[\"text\"] +\n",
    "    korean_dataset.shuffle(42).select(range(n))[\"text\"] +\n",
    "    code_dataset.shuffle(42).select(range(n))[\"text\"]\n",
    ")\n",
    "print(f\"{len(final_dataset)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the Korean data is in there somewhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'김혜리(1969년 12월 23일 ~ )는 대한민국의 배우이며, 1988년 미스코리아 선 출'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[10001][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c2bcbfe14b4decb2a8ae8e62d250b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, test_string in tqdm(enumerate(final_dataset), total=len(final_dataset)):\n",
    "    expected_output = tokenizer_original.encode(test_string)\n",
    "    output = tokenizer_custom.encode(test_string)\n",
    "\n",
    "    if output != expected_output:\n",
    "        print(f\"{idx=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cYlBfxxkkS5"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The tests above suggest that:\n",
    "\n",
    "1. It might be an artifact:\n",
    "    - Could it be a bug?\n",
    "    - Maybe our tests are not extensive enough?\n",
    "1. Is there something that we do not know?\n",
    "    - We might never know...\n",
    "\n",
    "Result: much more satisfying, but still inconclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. Let's build the GPT Tokenizer (<a href=\"https://www.youtube.com/watch?v=zduSFxRajkE\" target=\"_blank\">video</a>) by Andrej Karpathy.\n",
    "1. gpt-2 (<a href=\"https://github.com/openai/gpt-2/\" target=\"_blank\">GitHub repository</a>)\n",
    "1. https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
    "1. https://huggingface.co/learn/nlp-course/en/chapter6/5#implementing-bpe\n",
    "1. <a href=\"https://github.com/huggingface/tokenizers/blob/71c2a8d01a56cd7bd28148c309e210c47dac78e7/bindings/python/py_src/tokenizers/__init__.pyi#L420\" target=\"_blank\">`NormalizedString`</a>\n",
    "1. <a href=\"https://github.com/huggingface/tokenizers/blob/b24a2fc1781d5da4e6ebcd3ecb5b91edffc0a05f/bindings/python/examples/custom_components.py\" target=\"_blank\">Custom HF components</a>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "easy-captcha-fastai-v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
