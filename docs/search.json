[
  {
    "objectID": "notes/llama-3-vs-gpt-4-tokenizer/llama-3-vs-gpt-4-tokenizer.html",
    "href": "notes/llama-3-vs-gpt-4-tokenizer/llama-3-vs-gpt-4-tokenizer.html",
    "title": "Are Llama 3 and GPT-4 tokenizers the same?",
    "section": "",
    "text": "# Autoreload modules\n%load_ext autoreload\n%autoreload 2\nfrom concurrent.futures import ThreadPoolExecutor\nimport multiprocessing as mp\n\nimport tiktoken\nfrom transformers import AutoTokenizer\n\n# Number of parallel threads (adjust as needed)\nNUM_CPUS = mp.cpu_count()\nllama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\ngpt_4_tokenizer = AutoTokenizer.from_pretrained(\"Xenova/gpt-4\")\n\nllama_vocab = llama_tokenizer.get_vocab()\ngpt_4_vocab = gpt_4_tokenizer.get_vocab()\n\nlen(llama_vocab), len(gpt_4_vocab)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n(128256, 100263)\n# Check exactly equal\nvocab_1 = [x[0] for x in sorted(llama_vocab.items(), key=lambda x: x[1])]\nvocab_2 = [x[0] for x in sorted(gpt_4_vocab.items(), key=lambda x: x[1])]\nprint(len(vocab_1), len(vocab_2))\n\nfor idx, (token_1, token_2) in enumerate(zip(vocab_1, vocab_2)):\n    if token_1 != token_2:\n        print(f\"Token mismatch at {idx}: {token_1} != {token_2}\")\n\n128256 100263\nToken mismatch at 100256: ĠÙ != &lt;|endoftext|&gt;\nToken mismatch at 100257: Ø§Ù != &lt;|fim_prefix|&gt;\nToken mismatch at 100258: à¸²à¸ != &lt;|fim_middle|&gt;\nToken mismatch at 100259: ÑŁ != &lt;|fim_suffix|&gt;\nToken mismatch at 100260: ÑŁÑŁ != &lt;|im_start|&gt;\nToken mismatch at 100261: Ġà¸ != &lt;|im_end|&gt;\nToken mismatch at 100262: à¹Ģà¸ != &lt;|endofprompt|&gt;"
  },
  {
    "objectID": "notes/llama-3-vs-gpt-4-tokenizer/llama-3-vs-gpt-4-tokenizer.html#compare-gpt-4-to-tiktoken",
    "href": "notes/llama-3-vs-gpt-4-tokenizer/llama-3-vs-gpt-4-tokenizer.html#compare-gpt-4-to-tiktoken",
    "title": "Are Llama 3 and GPT-4 tokenizers the same?",
    "section": "Compare GPT-4 to tiktoken",
    "text": "Compare GPT-4 to tiktoken\n\nfrom datasets import load_dataset\n\nsplit = \"train\"\nenglish_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=split)\nkorean_dataset = load_dataset(\"lcw99/wikipedia-korean-20221001\", split=split)\ncode_dataset = load_dataset(\"code_search_net\", \"python\", split=split, trust_remote_code=True)\ncode_dataset = code_dataset.rename_column(\"whole_func_string\", \"text\")  # Rename whole_func_string to text\nprint(len(english_dataset), len(korean_dataset), len(code_dataset))\nprint(len(english_dataset) + len(korean_dataset) + len(code_dataset))\n\nn = 100000\nfinal_dataset = (\n    english_dataset.shuffle(42).select(range(min(n, len(english_dataset))))[\"text\"] +\n    korean_dataset.shuffle(42).select(range(min(n, len(korean_dataset))))[\"text\"] +\n    code_dataset.shuffle(42).select(range(min(n, len(code_dataset))))[\"text\"]\n)\nprint(f\"{len(final_dataset)=}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n1801350 607256 412178\n2820784\nlen(final_dataset)=300000\n\n\n\nfinal_dataset[min(n, len(english_dataset))][:50]\n\n'왕종린(, 1961년 ~ )은 미국의 중국계 물리학자로 중국 과학원 외국계 원사이다. 해양'\n\n\n\ngpt_4_tiktoken_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\n\ndef check_tokenizers_worker(test_string):\n    hf_output = gpt_4_tokenizer.encode(test_string)\n    tiktoken_output = gpt_4_tiktoken_tokenizer.encode(test_string)\n    return hf_output == tiktoken_output\n\n\nwith ThreadPoolExecutor(max_workers=NUM_CPUS) as executor:\n    # Apply the function to each item in parallel\n    results = list(executor.map(check_tokenizers_worker, final_dataset))\n\nall(results)\n\nTrue\n\n\nSo the conclusions (at least for the dataset sampled above) seem to be:\n\nThe first 100256 tokens of the Hugging Face implementations of Llama 3 and GPT-4 tokenizers seem to be the same.\nHugging Face’s GPT-4 tokenizer is identical to the one from tiktoken, at least for the dataset sampled above.\nSo Llama 3 and GPT-4 have very similar vocabularies, even though OpenAI never detailed how they trained their tokenizer.\nWhat’s going on???"
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "",
    "text": "A follow up on this post."
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#imports",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#imports",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "Imports",
    "text": "Imports\nToggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer"
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#utils",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#utils",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "Utils",
    "text": "Utils\nUse [CLS] pooling according to this:\n\ndef compute_sentence_embedding(sentence: str, model, tokenizer):\n    sentence_tokenized = tokenizer(sentence, return_tensors=\"pt\")\n\n    print(f\"Num tokens: {sentence_tokenized[\"input_ids\"].shape[1]}\")\n\n    with torch.no_grad():\n        return model(**sentence_tokenized).last_hidden_state[0, 0, :]\n\n\ndef perform_distance_comparison(s1, s2, s3):\n    euclidean_dist_1 = torch.linalg.vector_norm(s1 - s2).item()\n    euclidean_dist_2 = torch.linalg.vector_norm(s1 - s3).item()\n\n    print(f\"|s1 - s2| = {euclidean_dist_1:.3f}\")\n    print(f\"|s1 - s3| = {euclidean_dist_2:.3f}\")\n    print(f\"|s1 - s2| &lt; |s1 - s3| = {euclidean_dist_1 &lt; euclidean_dist_2}\")\n\n    cosine_sim_1 = F.cosine_similarity(s1[None, :], s2[None, :])[0].item()\n    cosine_sim_2 = F.cosine_similarity(s1[None, :], s3[None, :])[0].item()\n\n    print(f\"sim(s1, s2) = {cosine_sim_1:.3f}\")\n    print(f\"sim(s1, s3) = {cosine_sim_2:.3f}\")\n    print(f\"sim(s1, s2) &gt; sim(s1, s3) = {cosine_sim_1 &gt; cosine_sim_2}\")"
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#easier-example",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#easier-example",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "Easier example",
    "text": "Easier example\n\nmodel_name = \"google-bert/bert-base-cased\"\nmodel = AutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\nsentence_1 = \"sensitive information\"\nsentence_2 = \"confidential details\"\nsentence_3 = \"sensitive individual\"\n\n\nsentence_1_transformers = compute_sentence_embedding(sentence_1, model, tokenizer)\nsentence_2_transformers = compute_sentence_embedding(sentence_2, model, tokenizer)\nsentence_3_transformers = compute_sentence_embedding(sentence_3, model, tokenizer)\n\nsentence_1_transformers.shape, sentence_2_transformers.shape, sentence_3_transformers.shape\n\nNum tokens: 4\nNum tokens: 4\nNum tokens: 4\n\n\n(torch.Size([768]), torch.Size([768]), torch.Size([768]))\n\n\n\n# Both should be true\nperform_distance_comparison(\n    sentence_1_transformers, sentence_2_transformers, sentence_3_transformers\n)\n\n|s1 - s2| = 4.981\n|s1 - s3| = 6.788\n|s1 - s2| &lt; |s1 - s3| = True\nsim(s1, s2) = 0.955\nsim(s1, s3) = 0.900\nsim(s1, s2) &gt; sim(s1, s3) = True"
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#harder-example",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#harder-example",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "Harder example",
    "text": "Harder example\n\nsentence_1 = \"your data removal request has been reviewed and concluded\"\nsentence_2 = \"the sensitive personal information has been deleted\"\nsentence_3 = \"she has been a sensitive person\"\n\n\nsentence_1_transformers = compute_sentence_embedding(sentence_1, model, tokenizer)\nsentence_2_transformers = compute_sentence_embedding(sentence_2, model, tokenizer)\nsentence_3_transformers = compute_sentence_embedding(sentence_3, model, tokenizer)\n\nsentence_1_transformers.shape, sentence_2_transformers.shape, sentence_3_transformers.shape\n\nNum tokens: 11\nNum tokens: 9\nNum tokens: 8\n\n\n(torch.Size([768]), torch.Size([768]), torch.Size([768]))\n\n\n\n# Both should be true\nperform_distance_comparison(\n    sentence_1_transformers, sentence_2_transformers, sentence_3_transformers\n)\n\n|s1 - s2| = 5.626\n|s1 - s3| = 7.529\n|s1 - s2| &lt; |s1 - s3| = True\nsim(s1, s2) = 0.941\nsim(s1, s3) = 0.893\nsim(s1, s2) &gt; sim(s1, s3) = True"
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#try-the-same-with-a-text-embedding-model",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#try-the-same-with-a-text-embedding-model",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "Try the same with a text embedding model",
    "text": "Try the same with a text embedding model\n\nmodel_name = \"mixedbread-ai/mxbai-embed-large-v1\"\nmodel = AutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\nsentence_1_transformers = compute_sentence_embedding(sentence_1, model, tokenizer)\nsentence_2_transformers = compute_sentence_embedding(sentence_2, model, tokenizer)\nsentence_3_transformers = compute_sentence_embedding(sentence_3, model, tokenizer)\n\nsentence_1_transformers.shape, sentence_2_transformers.shape, sentence_3_transformers.shape\n\nNum tokens: 11\nNum tokens: 9\nNum tokens: 8\n\n\n(torch.Size([1024]), torch.Size([1024]), torch.Size([1024]))\n\n\n\nperform_distance_comparison(\n    sentence_1_transformers, sentence_2_transformers, sentence_3_transformers\n)\n\n|s1 - s2| = 14.404\n|s1 - s3| = 19.134\n|s1 - s2| &lt; |s1 - s3| = True\nsim(s1, s2) = 0.672\nsim(s1, s3) = 0.372\nsim(s1, s2) &gt; sim(s1, s3) = True"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is Augustas Macijauskas and I am working as an ML engineer applying LLMs in healthcare. Previously, I did a master’s in machine learning at the University of Cambridge. Excited about making ML models efficient, safe and accessible to everyone."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nSpike Technologies | Palo Alto, California, United States\nMachine Learning Engineer | January 2024 - present\nBPTI | Vilnius, Lithuania\nMachine Learning Research Assistant | September 2020 - September 2022\nMachine Learning Intern | July 2020 - September 2020\nGenus AI | Vilnius, Lithuania\nData Science Intern | July 2021 - September 2021"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Cambridge | Cambridge, United Kingdom\nMPhil in Machine Learning | September 2022 - October 2023\nUniversity of Manchester | Manchester, United Kingdom\nBSc Mathematics | September 2019 - June 2022"
  },
  {
    "objectID": "learning/mm-quantization/mm-quantization.html",
    "href": "learning/mm-quantization/mm-quantization.html",
    "title": "Augustas Macijauskas",
    "section": "",
    "text": "import numpy as np\n\n\ndef quantization(x, s, z, alpha_q, beta_q):\n\n    x_q = np.round(1 / s * x + z, decimals=0)\n    x_q = np.clip(x_q, a_min=alpha_q, a_max=beta_q)\n\n    return x_q\n\n\ndef quantization_int8(x, s, z):\n\n    x_q = quantization(x, s, z, alpha_q=-128, beta_q=127)\n    x_q = x_q.astype(np.int8)\n\n    return x_q\n\n\ndef dequantization(x_q, s, z):\n\n    # x_q - z might go outside the quantization range.\n    x_q = x_q.astype(np.int32)\n    x = s * (x_q - z)\n    x = x.astype(np.float32)\n\n    return x\n\n\ndef generate_quantization_constants(alpha, beta, alpha_q, beta_q):\n\n    # Affine quantization mapping\n    s = (beta - alpha) / (beta_q - alpha_q)\n    z = int((beta * alpha_q - alpha * beta_q) / (beta - alpha))\n\n    return s, z\n\n\ndef generate_quantization_int8_constants(alpha, beta):\n\n    b = 8\n    alpha_q = -(2 ** (b - 1))\n    beta_q = 2 ** (b - 1) - 1\n\n    s, z = generate_quantization_constants(alpha=alpha, beta=beta, alpha_q=alpha_q, beta_q=beta_q)\n\n    return s, z\n\n\ndef quantization_matrix_multiplication_int8(X_q, W_q, b_q, s_X, z_X, s_W, z_W, s_b, z_b, s_Y, z_Y):\n\n    p = W_q.shape[0]\n\n    # Y_q_simulated is FP32\n    Y_q_simulated = (\n        z_Y\n        + (s_b / s_Y * (b_q.astype(np.int32) - z_b))\n        + (\n            (s_X * s_W / s_Y)\n            * (\n                np.matmul(X_q.astype(np.int32), W_q.astype(np.int32))\n                - z_W * np.sum(X_q.astype(np.int32), axis=1, keepdims=True)\n                - z_X * np.sum(W_q.astype(np.int32), axis=0, keepdims=True)\n                + p * z_X * z_W\n            )\n        )\n    )\n\n    Y_q_simulated = np.round(Y_q_simulated, decimals=0)\n    Y_q_simulated = np.clip(Y_q_simulated, a_min=-128, a_max=127)\n    Y_q_simulated = Y_q_simulated.astype(np.int8)\n\n    return Y_q_simulated\n\n\n# Set random seed for reproducibility\nrandom_seed = 0\nnp.random.seed(random_seed)\n\n# Random matrices\nm = 2\np = 3\nn = 4\n\n# X\nalpha_X = -100.0\nbeta_X = 80.0\ns_X, z_X = generate_quantization_int8_constants(alpha=alpha_X, beta=beta_X)\nX = np.random.uniform(low=alpha_X, high=beta_X, size=(m, p)).astype(np.float32)\nX_q = quantization_int8(x=X, s=s_X, z=z_X)\nX_q_dq = dequantization(x_q=X_q, s=s_X, z=z_X)\n\n# W\nalpha_W = -20.0\nbeta_W = 10.0\ns_W, z_W = generate_quantization_int8_constants(alpha=alpha_W, beta=beta_W)\nW = np.random.uniform(low=alpha_W, high=beta_W, size=(p, n)).astype(np.float32)\nW_q = quantization_int8(x=W, s=s_W, z=z_W)\nW_q_dq = dequantization(x_q=W_q, s=s_W, z=z_W)\n\n# b\nalpha_b = -500.0\nbeta_b = 500.0\ns_b, z_b = generate_quantization_int8_constants(alpha=alpha_b, beta=beta_b)\nb = np.random.uniform(low=alpha_b, high=beta_b, size=(1, n)).astype(np.float32)\nb_q = quantization_int8(x=b, s=s_b, z=z_b)\nb_q_dq = dequantization(x_q=b_q, s=s_b, z=z_b)\n\n# Y\nalpha_Y = -3000.0\nbeta_Y = 3000.0\ns_Y, z_Y = generate_quantization_int8_constants(alpha=alpha_Y, beta=beta_Y)\nY_expected = np.matmul(X, W) + b\nY_q_expected = quantization_int8(x=Y_expected, s=s_Y, z=z_Y)\n\nY_expected_prime = np.matmul(X_q_dq, W_q_dq) + b_q_dq\nY_expected_prime_q = quantization_int8(x=Y_expected_prime, s=s_Y, z=z_Y)\nY_expected_prime_q_dq = dequantization(x_q=Y_expected_prime_q, s=s_Y, z=z_Y)\n\nprint(\"FP32 Y:\")\nprint(Y_expected)\nprint(\"FP32 Y Quantized:\")\nprint(Y_q_expected)\n\nY_q_simulated = quantization_matrix_multiplication_int8(\n    X_q=X_q, W_q=W_q, b_q=b_q, s_X=s_X, z_X=z_X, s_W=s_W, z_W=z_W, s_b=s_b, z_b=z_b, s_Y=s_Y, z_Y=z_Y\n)\nY_simulated = dequantization(x_q=Y_q_simulated, s=s_Y, z=z_Y)\n\nprint(\"Expected Quantized Y_q from Quantized Matrix Multiplication:\")\nprint(Y_q_simulated)\nprint(\"Expected Quantized Y_q from Quantized Matrix Multiplication Dequantized:\")\nprint(Y_simulated)\n\n# Ensure the algorithm implementation is correct\nassert np.array_equal(Y_simulated, Y_expected_prime_q_dq)\nassert np.array_equal(Y_q_simulated, Y_expected_prime_q)\n\nFP32 Y:\n[[242.46051  95.31735 217.99707 574.97864]\n [-88.28122 172.45425 216.39151 212.0112 ]]\nFP32 Y Quantized:\n[[10  4  9 24]\n [-4  7  9  9]]\nExpected Quantized Y_q from Quantized Matrix Multiplication:\n[[10  4  9 25]\n [-4  7  9  9]]\nExpected Quantized Y_q from Quantized Matrix Multiplication Dequantized:\n[[235.29411   94.117645 211.76471  588.2353  ]\n [-94.117645 164.70589  211.76471  211.76471 ]]\n\n\n\n((Y_expected - Y_simulated) ** 2).mean() ** 0.5\n\n6.9186957573422765"
  },
  {
    "objectID": "learning/quantization-fundamentals/L3_models_with_different_data_types.html",
    "href": "learning/quantization-fundamentals/L3_models_with_different_data_types.html",
    "title": "Lesson 3: Loading ML Models with Different Data Types",
    "section": "",
    "text": "In this lab, you will load ML models in different datatypes.\nfrom helper import DummyModel\nmodel = DummyModel()\nmodel\n\nDummyModel(\n  (token_embedding): Embedding(2, 2)\n  (linear_1): Linear(in_features=2, out_features=2, bias=True)\n  (layernorm_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n  (linear_2): Linear(in_features=2, out_features=2, bias=True)\n  (layernorm_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n  (head): Linear(in_features=2, out_features=2, bias=True)\n)\ndef print_param_dtype(model):\n    for name, param in model.named_parameters():\n        print(f\"{name} is loaded in {param.dtype}\")\nprint_param_dtype(model)\n\ntoken_embedding.weight is loaded in torch.float32\nlinear_1.weight is loaded in torch.float32\nlinear_1.bias is loaded in torch.float32\nlayernorm_1.weight is loaded in torch.float32\nlayernorm_1.bias is loaded in torch.float32\nlinear_2.weight is loaded in torch.float32\nlinear_2.bias is loaded in torch.float32\nlayernorm_2.weight is loaded in torch.float32\nlayernorm_2.bias is loaded in torch.float32\nhead.weight is loaded in torch.float32\nhead.bias is loaded in torch.float32"
  },
  {
    "objectID": "learning/quantization-fundamentals/L3_models_with_different_data_types.html#model-casting-float16",
    "href": "learning/quantization-fundamentals/L3_models_with_different_data_types.html#model-casting-float16",
    "title": "Lesson 3: Loading ML Models with Different Data Types",
    "section": "Model Casting: float16",
    "text": "Model Casting: float16\n\nCast the model into a different precision.\n\n\n# float 16\nmodel_fp16 = DummyModel().half()\n\n\nInspect the data types of the parameters.\n\n\nprint_param_dtype(model_fp16)\n\ntoken_embedding.weight is loaded in torch.float16\nlinear_1.weight is loaded in torch.float16\nlinear_1.bias is loaded in torch.float16\nlayernorm_1.weight is loaded in torch.float16\nlayernorm_1.bias is loaded in torch.float16\nlinear_2.weight is loaded in torch.float16\nlinear_2.bias is loaded in torch.float16\nlayernorm_2.weight is loaded in torch.float16\nlayernorm_2.bias is loaded in torch.float16\nhead.weight is loaded in torch.float16\nhead.bias is loaded in torch.float16\n\n\n\nmodel_fp16\n\nDummyModel(\n  (token_embedding): Embedding(2, 2)\n  (linear_1): Linear(in_features=2, out_features=2, bias=True)\n  (layernorm_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n  (linear_2): Linear(in_features=2, out_features=2, bias=True)\n  (layernorm_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n  (head): Linear(in_features=2, out_features=2, bias=True)\n)\n\n\n\nRun simple inference using model.\n\n\nimport torch\n\n\ndummy_input = torch.LongTensor([[1, 0], [0, 1]])\n\n\n# inference using float32 model\nlogits_fp32 = model(dummy_input)\nlogits_fp32\n\ntensor([[[-0.6872,  0.7132],\n         [-0.6872,  0.7132]],\n\n        [[-0.6872,  0.7132],\n         [-0.6872,  0.7132]]], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\n# inference using float16 model\ntry:\n    logits_fp16 = model_fp16(dummy_input)\nexcept Exception as error:\n    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
  },
  {
    "objectID": "learning/quantization-fundamentals/L3_models_with_different_data_types.html#model-casting-bfloat16",
    "href": "learning/quantization-fundamentals/L3_models_with_different_data_types.html#model-casting-bfloat16",
    "title": "Lesson 3: Loading ML Models with Different Data Types",
    "section": "Model Casting: bfloat16",
    "text": "Model Casting: bfloat16\n\nNote about deepcopy\n\ncopy.deepcopy makes a copy of the model that is independent of the original. Modifications you make to the copy will not affect the original, because you’re making a “deep copy”. For more details, see the Python docs on the [copy][https://docs.python.org/3/library/copy.html] library.\n\n\nfrom copy import deepcopy\n\n\nmodel_bf16 = deepcopy(model)\n\n\nmodel_bf16 = model_bf16.to(torch.bfloat16)\n\n\nprint_param_dtype(model_bf16)\n\ntoken_embedding.weight is loaded in torch.bfloat16\nlinear_1.weight is loaded in torch.bfloat16\nlinear_1.bias is loaded in torch.bfloat16\nlayernorm_1.weight is loaded in torch.bfloat16\nlayernorm_1.bias is loaded in torch.bfloat16\nlinear_2.weight is loaded in torch.bfloat16\nlinear_2.bias is loaded in torch.bfloat16\nlayernorm_2.weight is loaded in torch.bfloat16\nlayernorm_2.bias is loaded in torch.bfloat16\nhead.weight is loaded in torch.bfloat16\nhead.bias is loaded in torch.bfloat16\n\n\n\nlogits_bf16 = model_bf16(dummy_input)\n\n\nNow, compare the difference between logits_fp32 and logits_bf16.\n\n\nmean_diff = torch.abs(logits_bf16 - logits_fp32).mean().item()\nmax_diff = torch.abs(logits_bf16 - logits_fp32).max().item()\n\nprint(f\"Mean diff: {mean_diff} | Max diff: {max_diff}\")\n\nMean diff: 0.000997886061668396 | Max diff: 0.0016907453536987305"
  },
  {
    "objectID": "learning/quantization-fundamentals/L3_models_with_different_data_types.html#using-popular-generative-models-in-different-data-types",
    "href": "learning/quantization-fundamentals/L3_models_with_different_data_types.html#using-popular-generative-models-in-different-data-types",
    "title": "Lesson 3: Loading ML Models with Different Data Types",
    "section": "Using Popular Generative Models in Different Data Types",
    "text": "Using Popular Generative Models in Different Data Types\n\nLoad Salesforce/blip-image-captioning-base to perform image captioning.\n\n\nTo get the sample code that Younes showed:\n\nClick on the “Model Card” tab.\nOn the right, click on the button “&lt;&gt; Use in Transformers”, you’ll see a popup with sample code for loading this model.\n\n# Load model directly\nfrom transformers import AutoProcessor, AutoModelForSeq2SeqLM\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\nTo see the sample code with an example, click on “Read model documentation” at the bottom of the popup. It opens a new tab. https://huggingface.co/docs/transformers/main/en/model_doc/blip#transformers.BlipForConditionalGeneration\nOn this page, scroll down a bit, past the “parameters”, section, and you’ll see “Examples:”\n\nfrom PIL import Image\nimport requests\nfrom transformers import AutoProcessor, BlipForConditionalGeneration\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntext = \"A picture of\"\n\ninputs = processor(images=image, text=text, return_tensors=\"pt\")\n\noutputs = model(**inputs)\n\nfrom transformers import BlipForConditionalGeneration\n\n\nmodel_name = \"Salesforce/blip-image-captioning-base\"\n\n\nmodel = BlipForConditionalGeneration.from_pretrained(model_name)\n\n\n# inspect the default data types of the model\n\n# print_param_dtype(model)\n\n\nCheck the memory footprint of the model.\n\n\nfp32_mem_footprint = model.get_memory_footprint()\n\n\nprint(\"Footprint of the fp32 model in bytes:\", fp32_mem_footprint)\nprint(\"Footprint of the fp32 model in GBs:\", fp32_mem_footprint / (1024 ** 3))\n\nFootprint of the fp32 model in bytes: 989660400\nFootprint of the fp32 model in GBs: 0.9216930717229843\n\n\n\nLoad the same model in bfloat16.\n\n\nmodel_bf16 = BlipForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n\n\nbf16_mem_footprint = model_bf16.get_memory_footprint()\n\n\n# Get the relative difference\nrelative_diff = bf16_mem_footprint / fp32_mem_footprint\n\nprint(\"Footprint of the bf16 model in MBs:\", bf16_mem_footprint / (1024 ** 3))\nprint(f\"Relative diff: {relative_diff}\")\n\nFootprint of the bf16 model in MBs: 0.46084844321012497\nRelative diff: 0.5000020693967345\n\n\n\n\nModel Performance: float32 vs bfloat16\n\nNow, compare the generation results of the two model.\n\n\nfrom transformers import BlipProcessor\n\n\nprocessor = BlipProcessor.from_pretrained(model_name)\n\n\nLoad the image.\n\n\nfrom helper import load_image, get_generation\nfrom IPython.display import display\n\nimg_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"\n\nimage = load_image(img_url)\ndisplay(image.resize((500, 350)))\n\n\n\n\n\n\n\n\n\nresults_fp32 = get_generation(model, processor, image, torch.float32)\n\n\nprint(\"fp32 Model Results:\\n\", results_fp32)\n\nfp32 Model Results:\n a woman sitting on the beach with her dog\n\n\n\nresults_bf16 = get_generation(model_bf16, processor, image, torch.bfloat16)\n\n\nprint(\"bf16 Model Results:\\n\", results_bf16)\n\nbf16 Model Results:\n a woman sitting on the beach with a dog\n\n\n\n\nDefault Data Type\n\nFor Hugging Face Transformers library, the default data type to load the models in is float32\nYou can set the “default data type” as what you want.\n\n\ndesired_dtype = torch.bfloat16\ntorch.set_default_dtype(desired_dtype)\n\n\ndummy_model_bf16 = DummyModel()\n\n\nprint_param_dtype(dummy_model_bf16)\n\ntoken_embedding.weight is loaded in torch.bfloat16\nlinear_1.weight is loaded in torch.bfloat16\nlinear_1.bias is loaded in torch.bfloat16\nlayernorm_1.weight is loaded in torch.bfloat16\nlayernorm_1.bias is loaded in torch.bfloat16\nlinear_2.weight is loaded in torch.bfloat16\nlinear_2.bias is loaded in torch.bfloat16\nlayernorm_2.weight is loaded in torch.bfloat16\nlayernorm_2.bias is loaded in torch.bfloat16\nhead.weight is loaded in torch.bfloat16\nhead.bias is loaded in torch.bfloat16\n\n\n\nSimilarly, you can reset the default data type to float32.\n\n\ntorch.set_default_dtype(torch.float32)\n\n\nprint_param_dtype(dummy_model_bf16)\n\ntoken_embedding.weight is loaded in torch.bfloat16\nlinear_1.weight is loaded in torch.bfloat16\nlinear_1.bias is loaded in torch.bfloat16\nlayernorm_1.weight is loaded in torch.bfloat16\nlayernorm_1.bias is loaded in torch.bfloat16\nlinear_2.weight is loaded in torch.bfloat16\nlinear_2.bias is loaded in torch.bfloat16\nlayernorm_2.weight is loaded in torch.bfloat16\nlayernorm_2.bias is loaded in torch.bfloat16\nhead.weight is loaded in torch.bfloat16\nhead.bias is loaded in torch.bfloat16\n\n\n\n\nNote\n\nYou just used a simple form of quantization, in which the model’s parameters are saved in a more compact data type (bfloat16). During inference, the model performs its calculations in this data type, and its activations are in this data type.\nIn the next lesson, you will use another quantization method, “linear quantization”, which enables the quantized model to maintain performance much closer to the original model by converting from the compressed data type back to the original FP32 data type during inference."
  },
  {
    "objectID": "learning/reproduce-bert/reproduce-bert.html",
    "href": "learning/reproduce-bert/reproduce-bert.html",
    "title": "Augustas Macijauskas",
    "section": "",
    "text": "::: {#cell-0 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2024-07-22T18:46:12.649999Z”,“iopub.status.busy”:“2024-07-22T18:46:12.649390Z”,“iopub.status.idle”:“2024-07-22T18:46:13.681721Z”,“shell.execute_reply”:“2024-07-22T18:46:13.680961Z”,“shell.execute_reply.started”:“2024-07-22T18:46:12.649968Z”}’ trusted=‘true’ execution_count=1}\nimport csv\nimport gzip\nimport json\nimport math\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Union, Tuple, List, Iterable, Dict\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom scipy.stats import pearsonr, spearmanr\nfrom torch import nn, Tensor\nfrom torch.nn.parameter import Parameter\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer\n\ntorch.manual_seed(0)\nnp.random.seed(0)\n:::\n\n@dataclass\nclass BertConfig:\n    context_size: int = 512\n    vocab_size: int = 30522\n    num_hidden_layers: int = 12\n    hidden_size: int = 768\n    num_attention_heads: int = 12\n    dropout_prob: float = 0.1\n\n\nmodel_id = \"prajjwal1/bert-tiny\"\n\nTODO\n\nCheck what the different code pieces are doing\n\nInspect the shapes\n\nTry on some actual inputs\nTry make it faster?\n\nMaybe later?\n\nCheck that other model sizes work\n\n\nclass BertEmbeddings(nn.Module):\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n\n        self.word_embeddings = nn.Embedding(self.config.vocab_size, self.config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(self.config.context_size, self.config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(2, self.config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(self.config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(self.config.dropout_prob)\n\n        self.register_buffer(\"position_ids\", torch.arange(config.context_size).expand((1, -1)), persistent=False)\n        self.register_buffer(\n            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n        )\n\n    def forward(self, input_ids, position_ids, token_type_ids):\n        B, T = input_ids.shape\n\n        if position_ids is None:\n            position_ids = self.position_ids[:, :T]\n\n        if token_type_ids is None:\n            buffered_token_type_ids = self.token_type_ids[:, :T]\n            buffered_token_type_ids_expanded = buffered_token_type_ids.expand(B, T)\n            token_type_ids = buffered_token_type_ids_expanded\n\n        embeddings = (\n            self.word_embeddings(input_ids)\n            + self.token_type_embeddings(token_type_ids)\n            + self.position_embeddings(position_ids)\n        )\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = config.hidden_size // config.num_attention_heads\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.dropout_prob)\n\n    def transpose_for_scores(self, x: Tensor) -&gt; Tensor:\n        new_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states: Tensor, attention_mask: Tensor | None = None):\n        B, T, _ = hidden_states.size()\n\n        query_layer = self.query(hidden_states)\n        key_layer = self.key(hidden_states)\n        value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(query_layer)\n        key_layer = self.transpose_for_scores(key_layer)\n        value_layer = self.transpose_for_scores(value_layer)\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_layer,\n            key_layer,\n            value_layer,\n            attn_mask=attention_mask,\n            dropout_p=self.dropout_prob if self.training else 0.0,\n            is_causal=False,\n        )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(B, T, self.all_head_size)\n        return attn_output\n\n\nclass BertSelfOutput(nn.Module):\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.dropout_prob)\n\n    def forward(self, hidden_states: Tensor, input_tensor: Tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def forward(self, hidden_states: Tensor, attention_mask: Tensor | None = None):\n        self_output = self.self(hidden_states, attention_mask)\n        attention_output = self.output(self_output, hidden_states)\n        return attention_output\n\n\nclass BertIntermediate(nn.Module):\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n\n        self.dense = nn.Linear(config.hidden_size, 4 * config.hidden_size)\n        self.intermediate_act_fn = nn.GELU()\n\n    def forward(self, hidden_states: Tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n\n        self.dense = nn.Linear(4 * config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.dropout_prob)\n\n    def forward(self, hidden_states: Tensor, input_tensor: Tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states: Tensor, attention_mask: Tensor | None = None):\n        attention_output = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\n\nclass BertPooler(nn.Module):\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states: Tensor):\n        pooled_output = self.dense(hidden_states[:, 0])\n        return self.activation(pooled_output)\n\n\nclass BertEncoder(nn.Module):\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n\n        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states: Tensor, attention_mask: Tensor | None = None):\n        for layer_module in self.layer:\n            hidden_states = layer_module(hidden_states, attention_mask)\n        return hidden_states\n\n\nclass Bert(nn.Module):\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n\n    def forward(self, input_ids: Tensor, attention_mask: Tensor | None = None, token_type_ids: Tensor | None = None):\n        B, T = input_ids.shape\n        assert T &lt;= self.config.context_size, f\"Input length {T} exceeds context size {self.config.context_size}\"\n\n        x = self.embeddings(input_ids, None, token_type_ids)\n\n        if attention_mask is None:\n            attention_mask = torch.ones((B, T), device=input_ids.device)\n\n        inverted_mask = 1.0 - attention_mask[:, None, None, :].expand(B, 1, T, T).to(x.dtype)\n\n        inverted_mask = inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(x.dtype).min)\n\n        x = self.encoder(x, inverted_mask)\n\n        return x, self.pooler(x)\n\n    @classmethod\n    def from_pretrained(cls, model_id: str):\n        config_args = {\n            \"hidden_size\": 128,\n            \"num_attention_heads\": 2,\n            \"num_hidden_layers\": 2,\n            \"vocab_size\": 30522,  # example vocab size\n            \"context_size\": 512,  # example context size\n            \"dropout_prob\": 0.1,  # example dropout probability\n        }\n        config = BertConfig(**config_args)\n\n        model = Bert(config)\n        model_hf = AutoModel.from_pretrained(model_id)\n\n        model_dict = model.state_dict()\n        model_dict_hf = model_hf.state_dict()\n\n        for name, param in model_dict_hf.items():\n            if name in model_dict:\n                model_dict[name].copy_(param)\n            else:\n                raise KeyError(f\"{name} not found in custom model state_dict\")\n\n        model.load_state_dict(model_dict)\n\n        return model\n\n\nmodel = Bert.from_pretrained(model_id).eval()\n\nmodel_hf = AutoModel.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n/Users/augustasm/miniconda3/envs/website/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\nsample_sentence = \"An example use of pretrained BERT with transformers library to encode a sentence\"\ntokenized_sample_sentence = tokenizer(sample_sentence, return_tensors=\"pt\", padding=\"max_length\", max_length=512)\n\nwith torch.no_grad():  # Added torch.no_grad() to avoid OOM errors\n    output, _ = model(\n        input_ids=tokenized_sample_sentence[\"input_ids\"],\n        attention_mask=tokenized_sample_sentence[\"attention_mask\"],\n    )\n\n    output_hf = model_hf(\n        input_ids=tokenized_sample_sentence[\"input_ids\"],\n        attention_mask=tokenized_sample_sentence[\"attention_mask\"],\n    ).last_hidden_state\n\noutput.shape, output_hf.shape\n\n(torch.Size([1, 512, 128]), torch.Size([1, 512, 128]))\n\n\n\nprint((output - output_hf).pow(2).mean().sqrt().item())\n\nassert torch.allclose(output, output_hf, atol=1e-5)\n\n0.0\n\n\n\ndef compare_layer_outputs(model, model_hf, tokenized_sample_sentence):\n    input_ids = tokenized_sample_sentence[\"input_ids\"]\n    attention_mask = tokenized_sample_sentence[\"attention_mask\"].bool()  # Ensure attention_mask is boolean\n    token_type_ids = tokenized_sample_sentence.get(\"token_type_ids\", torch.zeros_like(input_ids))\n\n    with torch.no_grad():\n        x = model.embeddings(\n            input_ids=input_ids,\n            position_ids=torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device),\n            token_type_ids=token_type_ids,\n        )\n        x_hf = model_hf.embeddings(input_ids=input_ids, token_type_ids=token_type_ids)\n\n        print(\"Embeddings: \", torch.allclose(x, x_hf, atol=1e-7))\n        print(\"Embeddings: \", (x - x_hf).pow(2).mean().sqrt().item())\n\n        for i, layer in enumerate(model.encoder.layer):\n            x = layer.attention(x, attention_mask)\n            x_hf = model_hf.encoder.layer[i].attention(hidden_states=x_hf, attention_mask=attention_mask)[0]\n            print(f\"Layer {i} attention output: \", torch.allclose(x, x_hf))\n            print(f\"Layer {i} attention output: \", (x - x_hf).pow(2).mean().sqrt().item())\n\n            intermediate_output = layer.intermediate(x)\n            intermediate_output_hf = model_hf.encoder.layer[i].intermediate(x_hf)\n            print(\n                f\"Layer {i} intermediate output: \",\n                torch.allclose(intermediate_output, intermediate_output_hf),\n            )\n            print(\n                f\"Layer {i} intermediate output: \",\n                (intermediate_output - intermediate_output_hf).pow(2).mean().sqrt().item(),\n            )\n\n            layer_output = layer.output(intermediate_output, x)\n            layer_output_hf = model_hf.encoder.layer[i].output(intermediate_output_hf, x_hf)\n            print(f\"Layer {i} output: \", torch.allclose(layer_output, layer_output_hf))\n            print(f\"Layer {i} output: \", (layer_output - layer_output_hf).pow(2).mean().sqrt().item())\n\n            x = layer_output\n            x_hf = layer_output_hf\n\n        pooled_output = model.pooler(x)\n        pooled_output_hf = model_hf.pooler(x_hf)\n        print(\"Pooled output: \", torch.allclose(pooled_output, pooled_output_hf))\n        print(\"Pooled output: \", (pooled_output - pooled_output_hf).pow(2).mean().sqrt().item())\n\n\n# Run the comparison\ncompare_layer_outputs(model, model_hf, tokenized_sample_sentence)\n\nEmbeddings:  True\nEmbeddings:  0.0\nLayer 0 attention output:  True\nLayer 0 attention output:  0.0\nLayer 0 intermediate output:  True\nLayer 0 intermediate output:  0.0\nLayer 0 output:  True\nLayer 0 output:  0.0\nLayer 1 attention output:  True\nLayer 1 attention output:  0.0\nLayer 1 intermediate output:  True\nLayer 1 intermediate output:  0.0\nLayer 1 output:  True\nLayer 1 output:  0.0\nPooled output:  True\nPooled output:  0.0"
  },
  {
    "objectID": "learning/quantization-in-depth/L5_unpacking_2bit_weights.html",
    "href": "learning/quantization-in-depth/L5_unpacking_2bit_weights.html",
    "title": "L5-C: Unpacking 2-Bit Weights",
    "section": "",
    "text": "In this lesson, you will learn how to “unpack” the stored low precision “packed” weights.\nimport torch"
  },
  {
    "objectID": "learning/quantization-in-depth/L5_unpacking_2bit_weights.html#unpacking",
    "href": "learning/quantization-in-depth/L5_unpacking_2bit_weights.html#unpacking",
    "title": "L5-C: Unpacking 2-Bit Weights",
    "section": "Unpacking",
    "text": "Unpacking\nNote: Younes will explain the below code, and walk through each iteration step. You can go through the comprehensive explanation in the markdown below after first watching Younes’s explanation.\n# Example Tensor: [10110001]\n    # Which was Originally: 1 0 3 2 - 01 00 11 10\n\n    # Starting point of unpacked Tensor\n    # [00000000 00000000 00000000 00000000]\n    \n    ##### First Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 01 from [101100 01]\n    # No right shifts in the First Iteration\n    # After bit-wise OR operation between 00000000 and 10110001:\n    # [10110001 00000000 00000000 00000000]\n    # unpacked Tensor state: [10110001 00000000 00000000 00000000]\n    ##### First Iteration End\n\n    ##### Second Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 00 from [1011 00 01]\n    # 2 right shifts:\n    # [10110001] (1 shift)-&gt; 01011000 (2 shift)-&gt; 00101100\n    # After bit-wise OR operation between 00000000 and 00101100:\n    # [10110001 00101100 00000000 00000000]\n    # unpacked Tensor state: [10110001 00101100 00000000 00000000]\n    ##### Second Iteration End\n\n    ##### Third Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 11 from [10 11 0001]\n    # 4 right shifts:\n    # [10110001] (1 shift)-&gt; 01011000 (2 shift)-&gt; 00101100\n    # 00101100 (3 shift)-&gt; 00010110 (4 shift)-&gt; 00001011\n    # After bit-wise OR operation between 00000000 and 00001011:\n    # [10110001 00101100 00001011 00000000]\n    # unpacked Tensor state: [10110001 00101100 00001011 00000000]\n    ##### Third Iteration End\n\n    ##### Fourth Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 10 from [10 110001]\n    # 6 right shifts:\n    # [10110001] (1 shift)-&gt; 01011000 (2 shift)-&gt; 00101100\n    # 00101100 (3 shift)-&gt; 00010110 (4 shift)-&gt; 00001011\n    # 00001011 (5 shift)-&gt; 00000101 (6 shift)-&gt; 00000010\n    # After bit-wise OR operation between 00000000 and 00000010:\n    # [10110001 00101100 00001011 00000010]\n    # unpacked Tensor state: [10110001 00101100 00001011 00000010]\n    ##### Fourth Iteration End\n    \n    # Last step: Perform masking (bit-wise AND operation)\n    # Mask: 00000011\n    # Bit-wise AND operation between \n    # unpacked Tensor and 00000011\n    # [10110001 00101100 00001011 00000010] &lt;- unpacked tensor\n    # [00000011 00000011 00000011 00000011] &lt;- Mask\n    # [00000001 00000000 00000011 00000010] &lt;- Result\n\n    # Final\n    # unpacked Tensor state: [00000001 00000000 00000011 00000010]\n\ndef unpack_weights(uint8tensor, bits):\n    num_values = uint8tensor.shape[0] * 8 // bits\n\n    num_steps = 8 // bits\n\n    unpacked_tensor = torch.zeros((num_values), dtype=torch.uint8)\n\n    unpacked_idx = 0\n\n    # 1 0 3 2 - 01 00 11 10\n\n    # [00000000 00000000 00000000 00000000]\n    # [10110001 00101100 00001011 00000010]\n    # [00000001 00000000 00000011 00000010]\n\n    # 10110001\n    # 00000011\n\n    # 00000001\n\n    # 1: [10110001]\n    # 2: [00101100]\n    # 3: [00001011]\n\n    mask = 2**bits - 1\n\n    for i in range(uint8tensor.shape[0]):\n        for j in range(num_steps):\n            unpacked_tensor[unpacked_idx] |= uint8tensor[i] &gt;&gt; (bits * j)\n            unpacked_idx += 1\n\n    unpacked_tensor &= mask\n    return unpacked_tensor\n\n\npacked_tensor = torch.tensor([177, 255], dtype=torch.uint8)\n\n\n# Answer should be: torch.tensor([1, 0, 3, 2, 3, 3, 3, 3]\nunpack_weights(packed_tensor, 2)\n\ntensor([1, 0, 3, 2, 3, 3, 3, 3], dtype=torch.uint8)\n\n\n\ndef pack_weights(intweights: torch.Tensor, bits: int) -&gt; torch.Tensor:\n    \"\"\"\n    Pack int4 / int2 weights in a uint8 tensor\n\n    What packing means? Assume we have 4 values that are in 2bit but encoded in 8bit\n    (because torch does not have native support for 2-bit datatypes)\n\n    &gt; 0000 0011 | 0000 0010 | 0000 0001 | 0000 0000\n\n    We can pack them in a single 8-bit uint value\n\n    &gt; 1110 0100\n\n    Therefore instead of saving 4 values in 8-bit precision we save a single value of 8-bit precision saving 24 bits in total.\n\n    Args:\n        intweights (`torch.Tensor`):\n            The un-packed `torch.uint8` tensor\n        bits (`int`):\n            The actual `bits` - can be 2, 4\n    \"\"\"\n    original_shape = intweights.shape\n    values_per_item = 8 // bits\n    row_dim = (original_shape[0] + values_per_item - 1) // values_per_item\n\n    if len(original_shape) == 1:\n        packed_tensor_shape = (row_dim,)\n    else:\n        packed_tensor_shape = (row_dim, *original_shape[1:])\n\n    packed = torch.zeros(packed_tensor_shape, device=intweights.device, dtype=torch.uint8)\n    unpacked = intweights.to(torch.uint8)\n\n    def lshift(t: torch.Tensor, bits: int):\n        if t.device.type == \"mps\":\n            # lshift is not supported on MPS device\n            return t * (2**bits)\n        return t &lt;&lt; bits\n\n    it = min(values_per_item, (original_shape[0] // row_dim) + 1)\n    for i in range(it):\n        start = i * row_dim\n        end = min(start + row_dim, original_shape[0])\n        packed[: (end - start)] |= lshift(unpacked[start:end], bits * i)\n\n    return packed\n\n\npack_weights(torch.tensor([1, 0, 3, 2, 3, 3, 3, 3], dtype=torch.uint8), 2)\n\ntensor([253, 248], dtype=torch.uint8)\n\n\n\ndef unpack(packed: torch.Tensor, bits: int) -&gt; torch.Tensor:\n    \"\"\"\n    Un-Pack int4 / int2 weights (packed in a uint8) into a torch.uint8 tensor\n    What un-packing means? Assume we have packed 4 2-bit values in 8-bit\n    (because torch does not have native support for 2-bit datatypes)\n\n    &gt; 1110 0100\n\n    Unpacking them means retrieving the original 4 2-bit values:\n\n    &gt; 0000 0011 | 0000 0010 | 0000 0001 | 0000 0000\n\n    Args:\n        packed (`torch.Tensor`):\n            The packed tensor in `torch.uint8` precision\n        bits (`int`):\n            The number of bits per encoded value. Can be 2 or 4.\n    \"\"\"\n    unpacked = []\n    values_per_item = 8 // bits\n\n    def rshift(t: torch.Tensor, bits: int):\n        if t.device.type == \"mps\":\n            # rshift is not supported on MPS device\n            return t // (2**bits)\n        return t &gt;&gt; bits\n\n    # Unpack each set of values independently\n    for i in range(values_per_item):\n        mask = 2 ** (bits * (i + 1)) - 1\n        unpacked.append(rshift(packed & mask, bits * i))\n    \n    # Return the concatenated unpacked tensors\n    return torch.cat(unpacked).to(torch.uint8)\n\nunpack(torch.tensor([253, 248], dtype=torch.uint8), 2)\n\ntensor([1, 0, 3, 2, 3, 3, 3, 3], dtype=torch.uint8)"
  },
  {
    "objectID": "learning/quantization-in-depth/L3_linear_II_quantizing_weights_and_activations.html",
    "href": "learning/quantization-in-depth/L3_linear_II_quantizing_weights_and_activations.html",
    "title": "L3-E - Linear Quantization II: Quantizing Weights & Activations for Inference",
    "section": "",
    "text": "In this lesson, you will continue to learn different ways of performing linear quantization.\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport torch\n\n\ndef quantization_error(tensor, dequantized_tensor):\n    return (dequantized_tensor - tensor).abs().square().mean()\n\n\ndef linear_q_with_scale_and_zero_point(r_tensor, scale, zero_point, dtype=torch.int8):\n    \"\"\"\n    Performs simple linear quantization given\n    the scale and zero-point.\n    \"\"\"\n\n    # scale tensor and add the zero point\n    scaled_and_shifted_tensor = r_tensor / scale + zero_point\n\n    # round the tensor\n    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n\n    # we need to clamp to the min/max value of the specified dtype\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    q_tensor = rounded_tensor.clamp(q_min, q_max).to(dtype)\n    return q_tensor\n\n\ndef linear_dequantization(quantized_tensor, scale, zero_point):\n    \"\"\"\n    Linear de-quantization\n    \"\"\"\n    dequantized_tensor = scale * (quantized_tensor.float() - zero_point)\n\n    return dequantized_tensor\n\n\ndef plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n    \"\"\"\n    Plot a heatmap of tensors using seaborn\n    \"\"\"\n    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n    ax.set_title(title)\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n\n\ndef plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype=torch.int8, n_bits=8):\n    \"\"\"\n    A method that plots 4 matrices, the original tensor, the quantized tensor\n    the de-quantized tensor and the error tensor.\n    \"\"\"\n    # Get a figure of 4 plots\n    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n\n    # Plot the first matrix\n    plot_matrix(original_tensor, axes[0], \"Original Tensor\", cmap=ListedColormap([\"white\"]))\n\n    # Get the quantization range and plot the quantized tensor\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    plot_matrix(\n        quantized_tensor, axes[1], f\"{n_bits}-bit Linear Quantized Tensor\", vmin=q_min, vmax=q_max, cmap=\"coolwarm\"\n    )\n\n    # Plot the de-quantized tensors\n    plot_matrix(dequantized_tensor, axes[2], \"Dequantized Tensor\", cmap=\"coolwarm\")\n\n    # Get the quantization errors\n    q_error_tensor = abs(original_tensor - dequantized_tensor)\n    plot_matrix(q_error_tensor, axes[3], \"Quantization Error Tensor\", cmap=ListedColormap([\"white\"]))\n\n    fig.tight_layout()\n    plt.show()\n\n\ndef get_q_scale_and_zero_point(r_tensor, dtype=torch.int8):\n    \"\"\"\n    Get quantization parameters (scale, zero point)\n    for a floating point tensor\n    \"\"\"\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    r_min, r_max = r_tensor.min().item(), r_tensor.max().item()\n\n    scale = (r_max - r_min) / (q_max - q_min)\n\n    zero_point = q_min - (r_min / scale)\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    if zero_point &lt; q_min or zero_point &gt; q_max:\n        zero_point = q_min\n    else:\n        # round and cast to int\n        zero_point = int(round(zero_point))\n    return scale, zero_point\n\n\ndef linear_quantization(r_tensor, n_bits, dtype=torch.int8):\n    \"\"\"\n    linear quantization\n    \"\"\"\n\n    scale, zero_point = get_q_scale_and_zero_point(r_tensor)\n\n    quantized_tensor = linear_q_with_scale_and_zero_point(r_tensor, scale=scale, zero_point=zero_point, dtype=dtype)\n\n    return quantized_tensor, scale, zero_point\n\n\n############# From the previous lesson(s) of \"Linear Quantization II\"\ndef get_q_scale_symmetric(tensor, dtype=torch.int8):\n    r_max = tensor.abs().max().item()\n    q_max = torch.iinfo(dtype).max\n\n    # return the scale\n    return r_max / q_max\n\n\ndef linear_q_symmetric(tensor, dtype=torch.int8):\n    scale = get_q_scale_symmetric(tensor)\n\n    quantized_tensor = linear_q_with_scale_and_zero_point(\n        tensor,\n        scale=scale,\n        # in symmetric quantization zero point is = 0\n        zero_point=0,\n        dtype=dtype,\n    )\n\n    return quantized_tensor, scale\n\n\n###################################\nimport torch"
  },
  {
    "objectID": "learning/quantization-in-depth/L3_linear_II_quantizing_weights_and_activations.html#linear-quantization-inference",
    "href": "learning/quantization-in-depth/L3_linear_II_quantizing_weights_and_activations.html#linear-quantization-inference",
    "title": "L3-E - Linear Quantization II: Quantizing Weights & Activations for Inference",
    "section": "Linear Quantization: Inference",
    "text": "Linear Quantization: Inference\n\nW8A32 means weights in 8-bits and activations in 32-bits.\nFor simplicity, the linear layer will be without bias.\n\n\ndef quantized_linear_W8A32_without_bias(input, q_w, s_w, z_w):\n    assert input.dtype == torch.float32\n    assert q_w.dtype == torch.int8\n\n    dequantized_weight = q_w.to(torch.float32) * s_w + z_w\n    output = torch.nn.functional.linear(input, dequantized_weight)\n\n    return output\n\n\ninput = torch.tensor([1, 2, 3], dtype=torch.float32)\ninput\n\ntensor([1., 2., 3.])\n\n\n\nweight = torch.tensor([[-2, -1.13, 0.42], [-1.51, 0.25, 1.62], [0.23, 1.35, 2.15]])\nweight\n\ntensor([[-2.0000, -1.1300,  0.4200],\n        [-1.5100,  0.2500,  1.6200],\n        [ 0.2300,  1.3500,  2.1500]])\n\n\n\nq_w, s_w  = linear_q_symmetric(weight)\n\n\nq_w\n\ntensor([[-118,  -67,   25],\n        [ -89,   15,   96],\n        [  14,   80,  127]], dtype=torch.int8)\n\n\n\ns_w\n\n0.016929134609192376\n\n\n\noutput = quantized_linear_W8A32_without_bias(input, q_w, s_w, 0)\n\n\nprint(f\"This is the W8A32 output: {output}\")\n\nThis is the W8A32 output: tensor([-2.9965,  3.8768,  9.3957])\n\n\n\nfp32_output = torch.nn.functional.linear(input, weight)\n\n\nprint(f\"This is the output if we don't quantize: {fp32_output}\")\n\nThis is the output if we don't quantize: tensor([-3.0000,  3.8500,  9.3800])\n\n\n\n(output - fp32_output).pow(2).mean().sqrt().item()\n\n0.01802617497742176"
  },
  {
    "objectID": "learning/quantization-in-depth/L4_building_quantizer_replace_layers.html",
    "href": "learning/quantization-in-depth/L4_building_quantizer_replace_layers.html",
    "title": "L4-B - Building your own Quantizer: Replace PyTorch layers with Quantized Layers",
    "section": "",
    "text": "In this lesson, you will learn about the quantization pipline using your own 8-bit quantizer.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n############# From the previous lesson(s) of \"Building your own Quantizer\"\ndef w8_a16_forward(weight, input, scales, bias=None):\n\n    casted_weights = weight.to(input.dtype)\n    output = F.linear(input, casted_weights) * scales\n\n    if bias is not None:\n        output = output + bias\n\n    return output\n\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n        super().__init__()\n\n        self.register_buffer(\"int8_weights\", torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n\n        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n\n        if bias:\n            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n\n        else:\n            self.bias = None\n\n    def quantize(self, weights):\n        w_fp32 = weights.clone().to(torch.float32)\n\n        scales = w_fp32.abs().max(dim=-1).values / 127\n        scales = scales.to(weights.dtype)\n\n        int8_weights = torch.round(weights / scales.unsqueeze(1)).to(torch.int8)\n\n        self.int8_weights = int8_weights\n        self.scales = scales\n\n    def forward(self, input):\n        return w8_a16_forward(self.int8_weights, input, self.scales, self.bias)\n\n\n###################################"
  },
  {
    "objectID": "learning/quantization-in-depth/L4_building_quantizer_replace_layers.html#step-2-quantization-pipeline",
    "href": "learning/quantization-in-depth/L4_building_quantizer_replace_layers.html#step-2-quantization-pipeline",
    "title": "L4-B - Building your own Quantizer: Replace PyTorch layers with Quantized Layers",
    "section": "Step 2: Quantization Pipeline",
    "text": "Step 2: Quantization Pipeline\n\nReplace all of the torch.nn.Linear layers with the W8A16LinearLayer layer.\nCall quantize on the linear layers using the original weights.\n\n\n2.1 - Model In-place Linear Layer Replacement\n\nImplement replace_linear_with_target\n\n\ndef replace_linear_with_target(module, target_class, module_name_to_exclude):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n            old_bias = child.bias\n\n            new_module = target_class(child.in_features, child.out_features, old_bias is not None, child.weight.dtype)\n            setattr(module, name, new_module)\n            if old_bias is not None:\n                getattr(module, name).bias = old_bias\n        else:\n            # Recursively call the function for nested modules\n            replace_linear_with_target(child, target_class, module_name_to_exclude)\n\n\nclass DummyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.emb = torch.nn.Embedding(1, 1)\n        # Try with bias\n        self.linear_1 = nn.Linear(1, 1)\n        # Try without bias\n        self.linear_2 = nn.Linear(1, 1, bias=False)\n        # Lm prediction head\n        self.lm_head = nn.Linear(1, 1, bias=False)\n    \n    def forward(self, x):\n        x = self.emb(x)\n        x = self.linear_1(x)\n        x = self.linear_2(x)\n        x = self.lm_head(x)\n        return x\n\n\nmodel_1 = DummyModel()\nmodel_2 = DummyModel()\n\n\nreplace_linear_with_target(model_1, W8A16LinearLayer, [\"lm_head\"])\nprint(model_1)\n\nDummyModel(\n  (emb): Embedding(1, 1)\n  (linear_1): W8A16LinearLayer()\n  (linear_2): W8A16LinearLayer()\n  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n)\n\n\n\nreplace_linear_with_target(model_2, W8A16LinearLayer, [])\nprint(model_2)\n\nDummyModel(\n  (emb): Embedding(1, 1)\n  (linear_1): W8A16LinearLayer()\n  (linear_2): W8A16LinearLayer()\n  (lm_head): W8A16LinearLayer()\n)\n\n\n\n\n2.2 - Linear Layer Replacement + Quantization\n\nModify the replace_linear_with_target function to also perform quantization.\nImplement replace_linear_with_target_and_quantize.\n\n\ndef replace_linear_with_target_and_quantize(module, target_class, module_name_to_exclude):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n            old_bias = child.bias\n            old_weight = child.weight\n\n            new_module = target_class(child.in_features, child.out_features, old_bias is not None, child.weight.dtype)\n            setattr(module, name, new_module)\n\n            getattr(module, name).quantize(old_weight)\n\n            if old_bias is not None:\n                getattr(module, name).bias = old_bias\n        else:\n            # Recursively call the function for nested modules\n            replace_linear_with_target_and_quantize(child, target_class, module_name_to_exclude)\n\n\nmodel_3 = DummyModel()\n\n\nreplace_linear_with_target_and_quantize(model_3, W8A16LinearLayer, [\"lm_head\"])\nprint(model_3)\n\nDummyModel(\n  (emb): Embedding(1, 1)\n  (linear_1): W8A16LinearLayer()\n  (linear_2): W8A16LinearLayer()\n  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n)\n\n\n\nmodel_3(torch.randint(0, 1, (1, 1), dtype=torch.int32))\n\ntensor([[[0.0072]]], grad_fn=&lt;UnsafeViewBackward0&gt;)"
  },
  {
    "objectID": "learning/quantization-in-depth/L2_linear_I_quantize_dequantize_tensor.html",
    "href": "learning/quantization-in-depth/L2_linear_I_quantize_dequantize_tensor.html",
    "title": "L2-A - Linear Quantization I: Quantize and De-quantize a Tensor",
    "section": "",
    "text": "In this lesson, you will learn the fundamentals of linear quantization.\nimport torch"
  },
  {
    "objectID": "learning/quantization-in-depth/L2_linear_I_quantize_dequantize_tensor.html#quantization-with-random-scale-and-zero-point",
    "href": "learning/quantization-in-depth/L2_linear_I_quantize_dequantize_tensor.html#quantization-with-random-scale-and-zero-point",
    "title": "L2-A - Linear Quantization I: Quantize and De-quantize a Tensor",
    "section": "Quantization with Random Scale and Zero Point",
    "text": "Quantization with Random Scale and Zero Point\n\nImplement Linear Quantization for when the “scale” and the “zero point” are known/randomly selected.\n\n\ndef linear_q_with_scale_and_zero_point(tensor, scale, zero_point, dtype=torch.int8):\n\n    scaled_and_shifted_tensor = tensor / scale + zero_point\n\n    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n\n    q_min = torch.iinfo(dtype).min\n    q_max = torch.iinfo(dtype).max\n\n    q_tensor = rounded_tensor.clamp(q_min, q_max).to(dtype)\n\n    return q_tensor\n\n\n### a dummy tensor to test the implementation\ntest_tensor = torch.tensor([[191.6, -13.5, 728.6], [92.14, 295.5, -184], [0, 684.6, 245.5]])\n\n\n### these are random values for \"scale\" and \"zero_point\"\n### to test the implementation\nscale = 3.5\nzero_point = -70\n\n\nquantized_tensor = linear_q_with_scale_and_zero_point(test_tensor, scale, zero_point)\n\n\nquantized_tensor\n\ntensor([[ -15,  -74,  127],\n        [ -44,   14, -123],\n        [ -70,  126,    0]], dtype=torch.int8)"
  },
  {
    "objectID": "learning/quantization-in-depth/L2_linear_I_quantize_dequantize_tensor.html#dequantization-with-random-scale-and-zero-point",
    "href": "learning/quantization-in-depth/L2_linear_I_quantize_dequantize_tensor.html#dequantization-with-random-scale-and-zero-point",
    "title": "L2-A - Linear Quantization I: Quantize and De-quantize a Tensor",
    "section": "Dequantization with Random Scale and Zero Point",
    "text": "Dequantization with Random Scale and Zero Point\n\nNow, Dequantize the tensor to see how precise the quantization is.\n\n\ndequantized_tensor = scale * (quantized_tensor.float() - zero_point)\n\n\n# the original tensor\nprint(test_tensor.numpy())\n\n[[ 191.6   -13.5   728.6 ]\n [  92.14  295.5  -184.  ]\n [   0.    684.6   245.5 ]]\n\n\n\ndequantized_tensor\n\ntensor([[ 192.5000,  -14.0000,  689.5000],\n        [  91.0000,  294.0000, -185.5000],\n        [   0.0000,  686.0000,  245.0000]])\n\n\n\n### without casting to float ==&gt; need to cast\nscale * (quantized_tensor - zero_point)\n\ntensor([[ 192.5000,  -14.0000, -206.5000],\n        [  91.0000,  294.0000, -185.5000],\n        [   0.0000, -210.0000,  245.0000]])\n\n\n\ndef linear_dequantization(quantized_tensor, scale, zero_point):\n    return scale * (quantized_tensor.float() - zero_point)\n\n\nCalculate dequantized_tensor using the function linear_dequantization.\n\n\ndequantized_tensor = linear_dequantization(quantized_tensor, scale, zero_point)\n\n\nPrint the results of the dequantized_tensor.\n\n\ndequantized_tensor\n\ntensor([[ 192.5000,  -14.0000,  689.5000],\n        [  91.0000,  294.0000, -185.5000],\n        [   0.0000,  686.0000,  245.0000]])\n\n\n\nQuantization Error\n\nLoad the plot_quantization_errors from the helper file.\nTo access the helper.py file, you can click File --&gt; Open..., on the top left.\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom matplotlib.colors import ListedColormap\n\n\ndef plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n    \"\"\"\n    Plot a heatmap of tensors using seaborn\n    \"\"\"\n    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n    ax.set_title(title)\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n\n\ndef plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype=torch.int8, n_bits=8):\n    \"\"\"\n    A method that plots 4 matrices, the original tensor, the quantized tensor\n    the de-quantized tensor and the error tensor.\n    \"\"\"\n    # Get a figure of 4 plots\n    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n\n    # Plot the first matrix\n    plot_matrix(original_tensor, axes[0], \"Original Tensor\", cmap=ListedColormap([\"white\"]))\n\n    # Get the quantization range and plot the quantized tensor\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    plot_matrix(\n        quantized_tensor, axes[1], f\"{n_bits}-bit Linear Quantized Tensor\", vmin=q_min, vmax=q_max, cmap=\"coolwarm\"\n    )\n\n    # Plot the de-quantized tensors\n    plot_matrix(dequantized_tensor, axes[2], \"Dequantized Tensor\", cmap=\"coolwarm\")\n\n    # Get the quantization errors\n    q_error_tensor = abs(original_tensor - dequantized_tensor)\n    plot_matrix(q_error_tensor, axes[3], \"Quantization Error Tensor\", cmap=ListedColormap([\"white\"]))\n\n    fig.tight_layout()\n    plt.show()\n\n\nPlot the quantization results.\n\n\nplot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)\n\n\n\n\n\n\n\n\nNote: For the plot above, Quantization Error Tensor = abs(Original Tensor - Dequantized Tensor)\n\nCalculate an “overall” quantization error by using Mean Squared Error technique.\n\n\ndequantized_tensor - test_tensor\n\ntensor([[  0.9000,  -0.5000, -39.1000],\n        [ -1.1400,  -1.5000,  -1.5000],\n        [  0.0000,   1.4000,  -0.5000]])\n\n\n\n(dequantized_tensor - test_tensor).square()\n\ntensor([[8.0999e-01, 2.5000e-01, 1.5288e+03],\n        [1.2996e+00, 2.2500e+00, 2.2500e+00],\n        [0.0000e+00, 1.9601e+00, 2.5000e-01]])\n\n\n\n(dequantized_tensor - test_tensor).square().mean()\n\ntensor(170.8753)"
  },
  {
    "objectID": "learning/quantization-in-depth/L5_packing_2bit_weights.html",
    "href": "learning/quantization-in-depth/L5_packing_2bit_weights.html",
    "title": "L5-B: Packing 2-bit Weights",
    "section": "",
    "text": "In this lesson, you will learn how to store low precision weights through a technique called “packing”."
  },
  {
    "objectID": "learning/quantization-in-depth/L5_packing_2bit_weights.html#packing",
    "href": "learning/quantization-in-depth/L5_packing_2bit_weights.html#packing",
    "title": "L5-B: Packing 2-bit Weights",
    "section": "Packing",
    "text": "Packing\n\nimport torch\n\nNote: Younes will explain the below code, and walk through each iteration step. You can go through the comprehensive explanation in the markdown below after first watching Younes’s explanation.\n# Example Tensor: [1, 0, 3, 2]\n    # 1 0 3 2 - 01 00 11 10\n\n    # Starting point of packed int8 Tensor\n    # [0000 0000]\n    \n    ##### First Iteration Start:\n    # packed int8 Tensor State: [0000 0000]\n    # 1 = 0000 0001\n    # 0000 0001\n    # No left shifts in the First Iteration\n    # After bit-wise OR operation between 0000 0000 and 0000 0001:\n    # packed int8 Tensor State: 0000 0001\n    ##### First Iteration End\n\n    ##### Second Iteration Start:\n    # packed int8 Tensor State: [0000 0001]\n    # 0 = 0000 0000\n    # 0000 0000\n    # 2 left shifts:\n    # [0000 0000] (1 shift)-&gt; 0000 0000 (2 shift)-&gt; 0000 0000\n    # After bit-wise OR operation between 0000 0001 and 0000 0000:\n    # packed int8 Tensor State: 0000 0001\n    ##### Second Iteration End\n\n    ##### Third Iteration Start:\n    # packed int8 Tensor State: [0000 0001]\n    # 3 = 0000 0011\n    # 0000 0011\n    # 4 left shifts:\n    # [0000 0011] (1 shift)-&gt; 0000 0110 (2 shift)-&gt; 0000 1100\n    # 0000 1100 (3 shift)-&gt; 0001 1000 (4 shift)-&gt; 0011 0000\n    # After bit-wise OR operation between 0000 0001 and 0011 0000:\n    # packed int8 Tensor State: 0011 0001\n    ##### Third Iteration End\n\n    ##### Fourth Iteration Start:\n    # packed int8 Tensor State: [0011 0001]\n    # 2 = 0000 0010\n    # 0000 0010\n    # 6 left shifts:\n    # [0000 0010] (1 shift)-&gt; 0000 0100 (2 shift)-&gt; 0000 1000\n    # 0000 1000 (3 shift)-&gt; 0001 0000 (4 shift)-&gt; 0010 0000\n    # 0010 0000 (5 shift)-&gt; 0100 0000 (6 shift)-&gt; 1000 0000\n    # After bit-wise OR operation between 0011 0001 and 1000 0000:\n    # packed int8 Tensor State: 1011 0001\n    ##### Fourth Iteration End\n    \n    # Final packed int8 Tensor State: [1011 0001]\n\ndef pack_weights(uint8tensor, bits):\n    if uint8tensor.shape[0] * bits % 8 != 0:\n        raise ValueError(f\"The input shape needs to be a mutiple of {8 / bits} - got {uint8tensor.shape[0]}\")\n\n    num_values = uint8tensor.shape[0] * bits // 8\n\n    num_steps = 8 // bits\n\n    unpacked_idx = 0\n\n    packed_tensor = torch.zeros((num_values), dtype=torch.uint8)\n\n    # 1 0 3 2 - 01 00 11 10\n\n    # [0000 0000] -&gt; 0000 0001\n\n    # 0000 0001\n\n    # 0000 0000 - 0000 0000\n\n    # 0000 0011 - 0011 0000 - 0011 0001\n\n    # 1011 0001\n\n    for i in range(num_values):\n        for j in range(num_steps):\n            packed_tensor[i] |= uint8tensor[unpacked_idx] &lt;&lt; (bits * j)\n            unpacked_idx += 1\n    return packed_tensor\n\n\nunpacked_tensor = torch.tensor([1, 0, 3, 2], dtype=torch.uint8)\n\n\npack_weights(unpacked_tensor, 2)\n\ntensor([177], dtype=torch.uint8)\n\n\n\nunpacked_tensor = torch.tensor([1, 0, 3, 2, 3, 3, 3, 3], dtype=torch.uint8)\n\n\npack_weights(unpacked_tensor, 2)\n\ntensor([177, 255], dtype=torch.uint8)\n\n\n\ndef pack_weights_2(uint8tensor, bits):\n    if uint8tensor.shape[0] * bits % 8 != 0:\n        raise ValueError(f\"The input shape needs to be a mutiple of {8 / bits} - got {uint8tensor.shape[0]}\")\n\n    num_values = uint8tensor.shape[0] * bits // 8\n\n    num_steps = 8 // bits\n\n    unpacked_idx = 0\n\n    packed_tensor = torch.zeros((num_values), dtype=torch.uint8)\n\n    for i in range(num_values):\n        for j in range(num_steps):\n            packed_tensor[i] = (packed_tensor[i] &lt;&lt; bits) + uint8tensor[unpacked_idx]\n            unpacked_idx += 1\n\n    return packed_tensor\n\n\ndef unpack_weights_2(uint8tensor, bits):\n    print(f\"{uint8tensor.shape[0]=}\")\n\n    num_steps = 8 // bits\n\n    print(f\"{num_steps=}\")\n\n    unpacked_tensor = torch.zeros((uint8tensor.shape[0] * num_steps), dtype=torch.uint8)\n    print(f\"{unpacked_tensor.shape=}\")\n\n    unpacked_idx = 0\n\n    for i in range(uint8tensor.shape[0]):\n        for j in range(num_steps):\n            print(f\"{unpacked_idx=}\")\n            unpacked_tensor[unpacked_idx] = (uint8tensor[i] &gt;&gt; (bits * j))\n            unpacked_idx += 1\n\n    mask = (1 &lt;&lt; bits) - 1\n    unpacked_tensor &= mask\n    return unpacked_tensor\n\n\nprint(unpacked_tensor)\n\npacked_tensor = pack_weights_2(unpacked_tensor, 2)\nprint(packed_tensor)\n\nunpacked_packed_tensor = unpack_weights_2(packed_tensor, 2)\nprint(unpacked_packed_tensor)\n\nassert torch.all(unpacked_packed_tensor == unpacked_tensor)\n\ntensor([1, 0, 3, 2, 3, 3, 3, 3], dtype=torch.uint8)\ntensor([ 78, 255], dtype=torch.uint8)\nuint8tensor.shape[0]=2\nnum_steps=4\nunpacked_tensor.shape=torch.Size([8])\nunpacked_idx=0\nunpacked_idx=1\nunpacked_idx=2\nunpacked_idx=3\nunpacked_idx=4\nunpacked_idx=5\nunpacked_idx=6\nunpacked_idx=7\ntensor([2, 3, 0, 1, 3, 3, 3, 3], dtype=torch.uint8)\n\n\nAssertionError: \n\n\n\ndef pack_weights_3(intweights: torch.Tensor, bits: int) -&gt; torch.Tensor:\n    \"\"\"\n    Pack int4 / int2 weights in a uint8 tensor\n\n    What packing means? Assume we have 4 values that are in 2bit but encoded in 8bit\n    (because torch does not have native support for 2-bit datatypes)\n\n    &gt; 0000 0011 | 0000 0010 | 0000 0001 | 0000 0000\n\n    We can pack them in a single 8-bit uint value\n\n    &gt; 1110 0100\n\n    Therefore instead of saving 4 values in 8-bit precision we save a single value of 8-bit precision saving 24 bits in total.\n\n    Args:\n        intweights (`torch.Tensor`):\n            The un-packed `torch.uint8` tensor\n        bits (`int`):\n            The actual `bits` - can be 2, 4\n    \"\"\"\n    original_shape = intweights.shape\n    values_per_item = 8 // bits\n    row_dim = (original_shape[0] + values_per_item - 1) // values_per_item\n    print(f\"{original_shape[0]=}\")\n    print(f\"{values_per_item=}\")\n    print(f\"{row_dim=}\")\n\n    if len(original_shape) == 1:\n        packed_tensor_shape = (row_dim,)\n    else:\n        packed_tensor_shape = (row_dim, *original_shape[1:])\n\n    packed = torch.zeros(packed_tensor_shape, device=intweights.device, dtype=torch.uint8)\n    unpacked = intweights.to(torch.uint8)\n\n    def lshift(t: torch.Tensor, bits: int):\n        if t.device.type == \"mps\":\n            # lshift is not supported on MPS device\n            return t * (2**bits)\n        return t &lt;&lt; bits\n\n    print(\"---\")\n    it = min(values_per_item, (original_shape[0] // row_dim) + 1)\n    for i in range(it):\n        start = i * row_dim\n        end = min(start + row_dim, original_shape[0])\n        print(f\"{start=}, {end=}\")\n        print(f\"{unpacked[start:end]=}\")\n        print(bits * i)\n        print(f\"{lshift(unpacked[start:end], bits * i)=}\")\n        print(f\"{packed[: (end - start)]=}\")\n        packed[: (end - start)] |= lshift(unpacked[start:end], bits * i)\n        print(\"---\")\n\n    return packed\n\n\nprint(unpacked_tensor)\npack_weights_3(unpacked_tensor, 2)\n\ntensor([1, 0, 3, 2, 3, 3, 3, 3], dtype=torch.uint8)\noriginal_shape[0]=8\nvalues_per_item=4\nrow_dim=2\n---\nstart=0, end=2\nunpacked[start:end]=tensor([1, 0], dtype=torch.uint8)\n0\nlshift(unpacked[start:end], bits * i)=tensor([1, 0], dtype=torch.uint8)\npacked[: (end - start)]=tensor([0, 0], dtype=torch.uint8)\n---\nstart=2, end=4\nunpacked[start:end]=tensor([3, 2], dtype=torch.uint8)\n2\nlshift(unpacked[start:end], bits * i)=tensor([12,  8], dtype=torch.uint8)\npacked[: (end - start)]=tensor([1, 0], dtype=torch.uint8)\n---\nstart=4, end=6\nunpacked[start:end]=tensor([3, 3], dtype=torch.uint8)\n4\nlshift(unpacked[start:end], bits * i)=tensor([48, 48], dtype=torch.uint8)\npacked[: (end - start)]=tensor([13,  8], dtype=torch.uint8)\n---\nstart=6, end=8\nunpacked[start:end]=tensor([3, 3], dtype=torch.uint8)\n6\nlshift(unpacked[start:end], bits * i)=tensor([192, 192], dtype=torch.uint8)\npacked[: (end - start)]=tensor([61, 56], dtype=torch.uint8)\n---\n\n\ntensor([253, 248], dtype=torch.uint8)\n\n\n\npack_weights_3(torch.randint(0, 3, (8, 8)), 2)\n\noriginal_shape[0]=8\nvalues_per_item=4\nrow_dim=2\nstart=0, end=2\nunpacked[start:end]=tensor([[1, 1, 0, 0, 0, 0, 0, 1],\n        [1, 0, 0, 0, 1, 1, 2, 0]], dtype=torch.uint8)\nstart=2, end=4\nunpacked[start:end]=tensor([[2, 2, 1, 2, 0, 1, 0, 2],\n        [0, 0, 0, 2, 0, 0, 2, 1]], dtype=torch.uint8)\nstart=4, end=6\nunpacked[start:end]=tensor([[2, 2, 2, 1, 0, 2, 1, 0],\n        [0, 0, 2, 2, 0, 0, 1, 2]], dtype=torch.uint8)\nstart=6, end=8\nunpacked[start:end]=tensor([[1, 2, 2, 0, 0, 0, 1, 0],\n        [0, 2, 1, 1, 2, 0, 0, 1]], dtype=torch.uint8)\n\n\ntensor([[105, 169, 164,  24,   0,  36,  80,   9],\n        [  1, 128,  96, 104, 129,   1,  26, 100]], dtype=torch.uint8)\n\n\n\nassert (pack_weights(unpacked_tensor, 2) == pack_weights_2(unpacked_tensor, 2)).all()\n\nAssertionError: \n\n\n\nlarge_unpacked_tensor = torch.randint(0, 3, (1024, ), dtype=torch.uint8)\nlarge_unpacked_tensor\n\ntensor([2, 1, 0,  ..., 1, 0, 0], dtype=torch.uint8)\n\n\n\nassert (pack_weights(unpacked_tensor, 2) == pack_weights_2(unpacked_tensor, 2)).all()\n\npack_weights_2(large_unpacked_tensor, 2)\n\nAssertionError: \n\n\n\n%timeit pack_weights(large_unpacked_tensor, 2)\n%timeit pack_weights_2(large_unpacked_tensor, 2)\n\n5.16 ms ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n5.13 ms ± 27.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "learning/quantization-in-depth/L2_linear_I_get_scale_and_zero_point.html",
    "href": "learning/quantization-in-depth/L2_linear_I_get_scale_and_zero_point.html",
    "title": "L2-B - Linear Quantization I: Get the Scale and Zero Point",
    "section": "",
    "text": "In this lesson, continue to learn about fundamentals of linear quantization, and implement your own Linear Quantizer.\nimport torch\n\n### a dummy tensor to test the implementation\ntest_tensor = torch.tensor([[191.6, -13.5, 728.6], [92.14, 295.5, -184], [0, 684.6, 245.5]])\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport torch\n\n\ndef plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n    \"\"\"\n    Plot a heatmap of tensors using seaborn\n    \"\"\"\n    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n    ax.set_title(title)\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n\n\ndef plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype=torch.int8, n_bits=8):\n    \"\"\"\n    A method that plots 4 matrices, the original tensor, the quantized tensor\n    the de-quantized tensor and the error tensor.\n    \"\"\"\n    # Get a figure of 4 plots\n    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n\n    # Plot the first matrix\n    plot_matrix(original_tensor, axes[0], \"Original Tensor\", cmap=ListedColormap([\"white\"]))\n\n    # Get the quantization range and plot the quantized tensor\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    plot_matrix(\n        quantized_tensor, axes[1], f\"{n_bits}-bit Linear Quantized Tensor\", vmin=q_min, vmax=q_max, cmap=\"coolwarm\"\n    )\n\n    # Plot the de-quantized tensors\n    plot_matrix(dequantized_tensor, axes[2], \"Dequantized Tensor\", cmap=\"coolwarm\")\n\n    # Get the quantization errors\n    q_error_tensor = abs(original_tensor - dequantized_tensor)\n    plot_matrix(q_error_tensor, axes[3], \"Quantization Error Tensor\", cmap=ListedColormap([\"white\"]))\n\n    fig.tight_layout()\n    plt.show()\n\n\n########## Functions from Linear Quantization I (Part 1)\ndef linear_q_with_scale_and_zero_point(tensor, scale, zero_point, dtype=torch.int8):\n    scaled_and_shifted_tensor = tensor / scale + zero_point\n    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n    q_min = torch.iinfo(dtype).min\n    q_max = torch.iinfo(dtype).max\n    q_tensor = rounded_tensor.clamp(q_min, q_max).to(dtype)\n\n    return q_tensor\n\n\ndef linear_dequantization(quantized_tensor, scale, zero_point):\n    return scale * (quantized_tensor.float() - zero_point)\n\n\n#############"
  },
  {
    "objectID": "learning/quantization-in-depth/L2_linear_I_get_scale_and_zero_point.html#finding-scale-and-zero-point-for-quantization",
    "href": "learning/quantization-in-depth/L2_linear_I_get_scale_and_zero_point.html#finding-scale-and-zero-point-for-quantization",
    "title": "L2-B - Linear Quantization I: Get the Scale and Zero Point",
    "section": "Finding Scale and Zero Point for Quantization",
    "text": "Finding Scale and Zero Point for Quantization\n\nq_min = torch.iinfo(torch.int8).min\nq_max = torch.iinfo(torch.int8).max\n\nq_min, q_max\n\n(-128, 127)\n\n\n\nr_min = test_tensor.min().item()\nr_max = test_tensor.max().item()\n\nr_min, r_max\n\n(-184.0, 728.5999755859375)\n\n\n\nscale = (r_max - r_min) / (q_max - q_min)\nscale\n\n3.578823433670343\n\n\n\nzero_point = q_min - (r_min / scale)\nzero_point\n\n-76.58645490333825\n\n\n\nzero_point = int(round(zero_point))\nzero_point\n\n-77\n\n\n\nNow, put all of this in a function.\n\n\ndef get_q_scale_and_zero_point(tensor, dtype=torch.int8):\n\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    r_min, r_max = tensor.min().item(), tensor.max().item()\n\n    scale = (r_max - r_min) / (q_max - q_min)\n\n    zero_point = q_min - (r_min / scale)\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    zero_point = int(round(max(q_min, min(q_max, zero_point))))\n\n    return scale, zero_point\n\n\nTest the implementation using the test_tensor defined earlier.\n\n[[191.6, -13.5, 728.6],\n [92.14, 295.5,  -184],\n [0,     684.6, 245.5]]\n\nnew_scale, new_zero_point = get_q_scale_and_zero_point(test_tensor)\nnew_scale, new_zero_point\n\n(3.578823433670343, -77)"
  },
  {
    "objectID": "learning/quantization-in-depth/L2_linear_I_get_scale_and_zero_point.html#quantization-and-dequantization-with-calculated-scale-and-zero-point",
    "href": "learning/quantization-in-depth/L2_linear_I_get_scale_and_zero_point.html#quantization-and-dequantization-with-calculated-scale-and-zero-point",
    "title": "L2-B - Linear Quantization I: Get the Scale and Zero Point",
    "section": "Quantization and Dequantization with Calculated Scale and Zero Point",
    "text": "Quantization and Dequantization with Calculated Scale and Zero Point\n\nUse the calculated scale and zero_point with the functions linear_q_with_scale_and_zero_point and linear_dequantization.\n\n\nquantized_tensor = linear_q_with_scale_and_zero_point(test_tensor, new_scale, new_zero_point)\n\n\ndequantized_tensor = linear_dequantization(quantized_tensor, new_scale, new_zero_point)\n\n\nPlot to see how the Quantization Error looks like after using calculated scale and zero_point.\n\n\nplot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)\n\n\n\n\n\n\n\n\n\n(dequantized_tensor - test_tensor).square().mean()\n\ntensor(1.5730)\n\n\n\nPut Everything Together: Your Own Linear Quantizer\n\nNow, put everything togther to make your own Linear Quantizer.\n\n\ndef linear_quantization(tensor, dtype=torch.int8):\n    scale, zero_point = get_q_scale_and_zero_point(tensor, dtype=dtype)\n\n    quantized_tensor = linear_q_with_scale_and_zero_point(tensor, scale, zero_point, dtype=dtype)\n\n    return quantized_tensor, scale, zero_point\n\n\nTest your implementation on a random matrix.\n\n\nr_tensor = torch.randn((4, 4))\nprint(r_tensor.numpy())\n\n[[ 1.0743723   1.9920547  -0.9499424  -2.4288838 ]\n [ 1.3814387  -1.7934494   0.6764703   0.25291258]\n [-0.31475222  0.20515239  0.549644   -0.5739533 ]\n [ 0.461127   -0.2757374  -0.3414375   0.07443906]]\n\n\nNote: Since the values are random, what you see in the video might be different than what you will get.\n\nquantized_tensor, scale, zero_point = linear_quantization(r_tensor)\nprint(quantized_tensor.numpy())\n\n[[  74  127  -43 -128]\n [  92  -91   51   27]\n [  -6   24   44  -21]\n [  39   -4   -8   16]]\n\n\n\nscale, zero_point\n\n(0.01733701369341682, 12)\n\n\n\ndequantized_tensor = linear_dequantization(quantized_tensor, scale, zero_point)\n\n\nplot_quantization_errors(r_tensor, quantized_tensor, dequantized_tensor)\n\n\n\n\n\n\n\n\n\n(dequantized_tensor - r_tensor).square().mean()\n\ntensor(1.9450e-05)"
  },
  {
    "objectID": "posts/optimization-algorithms/optimisation-algorithms.html",
    "href": "posts/optimization-algorithms/optimisation-algorithms.html",
    "title": "Implementing optimisation algorithms to optimise parameters of linear functions",
    "section": "",
    "text": "Imports\nToggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nData\nLet’s create 100 x values uniformly distributed between 0 and 100. Note the usage of a random seed for reproducibility:\n\nnp.random.seed(0)\nx = np.random.uniform(0, 100, size=100)\n\nCalculate y values for the given x values. I chose a slope of -2 and an intercept of 10 and decided not to add noise to make the task easier:\n\nslope = -2\nintercept = 10\ny = slope * x + intercept\n\nLet’s plot the points to check that everything looks as it should be:\n\nplt.scatter(x, y)\nplt.plot(x, slope * x + intercept, color=\"red\");\n\n\n\n\n\n\n\n\n\n\nPrepare for the experiments\nLet’s create some classes and functions that will help us to quickly experiment with different optimization algorithms.\nWe start by creating our simple LinearModel whose parameters we’ll try to optimize. Here we choose to initialize both the slope and the intercept with the value 1, but this should not matter too much for this problem.\nThe forward method performs the forward pass of the model to get a prediction for a given x, i.e. just return ax+b.\nThe backward method is used to calculate gradients of the parameters given x, y and y_pred. These are usually calculated using automatic differentiation in deep learning libraries, such as PyTorch, but here we do everything by hand:\n\nclass LinearModel:\n    def __init__(self, initial_slope=1, initial_intercept=1):\n        self.slope = initial_slope\n        self.intercept = initial_intercept\n    \n    def forward(self, x):\n        return self.slope * x + self.intercept\n    \n    def backward(self, x, y, y_pred):\n        return x * (y_pred - y), (y_pred - y)\n\nThe Tracker utility class will be used to track different values while experimenting. It can be considered an overkill for such a simple task (and I later ended up tracking only one value with it), but it can be very convenient for larger projects, so I included it here too.\nThe class stores a dictionary of string-list pairs in its state and provides add_record and get_values methods to easily access and update that dictionary.\n\nfrom collections import defaultdict\n\nclass Tracker:\n    def __init__(self):\n        self.tracker = defaultdict(list)\n    \n    def add_record(self, key, value):\n        self.tracker[key].append(value)\n    \n    def get_values(self, key):\n        if not key in self.tracker: raise KeyError(f\"Key {key} not in tracker\")\n\n        return self.tracker[key]\n\ntrain_one_epoch is a utility function that trains the model for one epoch (one pass through the data). It performs a forward pass, calculates and tracks the error, gets the gradients of parameters by performing a backward pass and passes those along with the params parameter to the optimizer that updates the weights of the model. The params parameter is a dictionary that is used to hyperparameter constants, such as the learning rate, to the optimizer.\n\ndef train_one_epoch(model, data, optimizer, tracker, params):\n    total_error = 0.0\n    \n    for x, y in data:\n        # Forward pass\n        y_pred = model.forward(x)\n        \n        # Calculate error\n        error = 0.5 * (y_pred - y) ** 2\n        total_error += error\n\n        # Backward pass\n        grad_slope, grad_intercept = model.backward(x, y, y_pred)\n        \n        # Step the optimizer\n        params[\"step\"] += 1\n        optimizer.step(model, grad_slope, grad_intercept, params)\n\n    tracker.add_record(\"error\", total_error / len(data))\n\nFinally, a utility function that brings everything together to execute an experiment. Our goal is to train the model for as little as possible, stopping when error drops below some desired threshold.\n\ndef perform_experiment(optimizer, params, desired_error=1e-8):\n    # Initialization\n    model = LinearModel()\n    data = list(zip(x, y))\n    tracker = Tracker()\n    \n    params[\"step\"] = 0\n    while True:\n        # Train for an epoch\n        train_one_epoch(model, data, optimizer, tracker, params)\n        \n        # Check if we reached desired accuracy\n        if tracker.get_values(\"error\")[-1] &lt;= desired_error:\n            break\n        \n        # Improper hyperparameter values might cause the models to diverge,\n        # so we include this here to stop execution if this happens\n        if np.isnan(tracker.get_values(\"error\")[-1]):\n            break\n    \n    # Print results and plot error over time\n    steps = params[\"step\"]\n    print(f\"Epochs: {steps // len(data)}\")\n    print(f\"Batches (of size 1): {steps}\")\n    print(f\"Slope: {model.slope}\")\n    print(f\"Intercept: {model.intercept}\")\n    plt.plot(tracker.get_values(\"error\"), linestyle=\"dotted\")\n    \n    return params[\"step\"] // len(data), params[\"step\"], model.slope, model.intercept\n\n\n\nVanilla SGD\n\nclass SGDOptimizer:\n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        model.slope -= lr * grad_slope\n        model.intercept -= lr * grad_intercept\n\n\noptimizer = SGDOptimizer()\nparams = { \"lr\": 1e-4 }\nperform_experiment(optimizer, params);\n\nEpochs: 3893\nBatches (of size 1): 389300\nSlope: -1.9999963788900519\nIntercept: 9.999738575620166\n\n\n\n\n\n\n\n\n\nBreaks with lr=1e-3\n\n\nSGD with momentum\n\nclass SGDWithMomentumOptimizer(SGDOptimizer):\n    def __init__(self):\n        self.slope_momentum = self.intercept_momentum = 0\n    \n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        miu = params[\"miu\"]\n        self.slope_momentum = miu * self.slope_momentum + (1 - miu) * grad_slope\n        model.slope -= lr * self.slope_momentum\n\n        self.intercept_momentum = miu * self.intercept_momentum + (1 - miu) * grad_intercept\n        model.intercept -= lr * self.intercept_momentum\n\n\noptimizer = SGDWithMomentumOptimizer()\nparams = { \"lr\": 1e-4, \"miu\": 0.9 }\nperform_experiment(optimizer, params);\n\nEpochs: 3844\nBatches (of size 1): 384400\nSlope: -1.9999951526911044\nIntercept: 9.999742134759243\n\n\n\n\n\n\n\n\n\n\noptimizer = SGDWithMomentumOptimizer()\nparams = { \"lr\": 1e-3, \"miu\": 0.9 }\nperform_experiment(optimizer, params);\n\nEpochs: 365\nBatches (of size 1): 36500\nSlope: -1.9999980020599053\nIntercept: 9.999929876584854\n\n\n\n\n\n\n\n\n\n\n\nRMSprop\n\nclass RMSpropOptimizer(SGDOptimizer):\n    def __init__(self, eps=1e-8):\n        self.eps = eps\n        self.slope_squared_grads = self.intercept_squared_grads = 0\n    \n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        decay_rate = params[\"decay_rate\"]\n        self.slope_squared_grads = decay_rate * self.slope_squared_grads + (1 - decay_rate) * grad_slope ** 2\n        model.slope -= lr * grad_slope / (np.sqrt(self.slope_squared_grads) + self.eps)\n\n        self.intercept_squared_grads = decay_rate * self.intercept_squared_grads + (1 - decay_rate) * grad_intercept ** 2\n        model.intercept -= lr * grad_intercept / (np.sqrt(self.intercept_squared_grads) + self.eps)\n\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 1e-4, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 2765\nBatches (of size 1): 276500\nSlope: -1.9999999858447193\nIntercept: 9.999996406289089\n\n\n\n\n\n\n\n\n\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 1e-3, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 302\nBatches (of size 1): 30200\nSlope: -1.9999998782649515\nIntercept: 9.999976783670757\n\n\n\n\n\n\n\n\n\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 3e-3, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 113\nBatches (of size 1): 11300\nSlope: -2.0000001115561656\nIntercept: 10.000020816756175\n\n\n\n\n\n\n\n\n\nWorks best with lr=3e-3 and decay_rate=0.99.\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 1e1, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 83323\nBatches (of size 1): 8332300\nSlope: -2.000000169655216\nIntercept: 10.000030570050711\n\n\n\n\n\n\n\n\n\n\n\nAdam\n\nclass Adam(SGDOptimizer):\n    def __init__(self, eps=1e-8):\n        self.eps = eps\n        \n        self.slope_momentum = self.intercept_momentum = 0\n        self.slope_squared_grads = self.intercept_squared_grads = 0\n    \n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        beta1, beta2 = params[\"betas\"]\n        step = params[\"step\"]\n        \n        self.slope_momentum = beta1 * self.slope_momentum + (1 - beta1) * grad_slope\n        slope_corrected_momentum = self.slope_momentum / (1 - beta1 ** step)\n        self.slope_squared_grads = beta2 * self.slope_squared_grads + (1 - beta2) * grad_slope ** 2\n        slope_corrected_squared_grads = self.slope_squared_grads / (1 - beta2 ** step)\n        model.slope -= lr * slope_corrected_momentum / (np.sqrt(slope_corrected_squared_grads) + self.eps)\n\n        self.intercept_momentum = beta1 * self.intercept_momentum + (1 - beta1) * grad_intercept\n        intercept_corrected_momentum = self.intercept_momentum / (1 - beta1 ** step)\n        self.intercept_squared_grads = beta2 * self.intercept_squared_grads + (1 - beta2) * grad_intercept ** 2\n        intercept_corrected_squared_grads = self.intercept_squared_grads / (1 - beta2 ** step)\n        model.intercept -= lr * intercept_corrected_momentum / (np.sqrt(intercept_corrected_squared_grads) + self.eps)\n\n\noptimizer = Adam()\nparams = { \"lr\": 1e-4, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params);\n\nEpochs: 2848\nBatches (of size 1): 284800\nSlope: -1.9999980335219065\nIntercept: 9.999814585095613\n\n\n\n\n\n\n\n\n\n\noptimizer = Adam()\nparams = { \"lr\": 1e0, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params);\n\nEpochs: 10\nBatches (of size 1): 1000\nSlope: -2.00000001786048\nIntercept: 10.000001361061178\n\n\n\n\n\n\n\n\n\nAdam seems to be of the order of 10 times faster than other optimization methods and works with much greater learning rates\n\noptimizer = Adam()\nparams = { \"lr\": 1e8, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params);\n\nEpochs: 43\nBatches (of size 1): 4300\nSlope: -2.000001151784137\nIntercept: 9.999987336010909\n\n\n\n\n\n\n\n\n\nIn this very easy case of parameters of a linear curve estimation it works with learning rates as large as 1e8, whereas the largest learning rate that still works among other methods is 1e1 for RMSprop. However, the graphs there showed that oscillations in the error were all over the place, while Adam barely oscillated at all.\n\noptimizer = Adam()\nparams = { \"lr\": 6e-2, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params, 1e-32);\n\nEpochs: 72\nBatches (of size 1): 7200\nSlope: -2.0\nIntercept: 10.0\n\n\n\n\n\n\n\n\n\nAlso, Adam is the only optimizer that was able to achieve an error as little as 1e-32. Even though out of the box the number of steps (in epochs) was of the same order as for other optimization methods, some hyperparameter tinkering allowed to top them by reducing the number of steps to 72."
  },
  {
    "objectID": "posts/tokenizers-deep-dive/tokenizers-deep-dive.html",
    "href": "posts/tokenizers-deep-dive/tokenizers-deep-dive.html",
    "title": "Tokenizers deep dive",
    "section": "",
    "text": "In the conclusion of my recent blog post I argued that I disagree with Andrej Karpathy’s claims about the current state of tokenizer availability and tooling. In particular:\nThis prompted me to do a deep dive into tokenizers and how one would go about building one from scratch. Inspired by the top-down approach from Jeremy Howard, whose courses I enjoy a lot, this blog post will start with the very basics of tokenizers and then focus on how to train a Llama 3-like tokenizer on your own data with as little code as possible. Finally, we will explore what influence different proportions of English/non-English/code data have on the final vocabulary learnt by the tokenizer, as well as discuss a paper on how tokenizer design choices impact the downstream performance of the LLM.\nIn the second part, we will explore what influence different proportions of English/non-English/code data have on the final vocabulary learnt by the tokenizer, as well as discuss a paper on how tokenizer design choices impact the downstream performance of the LLM, so stay tuned! Now, let’s jump right in!."
  },
  {
    "objectID": "posts/tokenizers-deep-dive/tokenizers-deep-dive.html#assembling-a-llama-3-tokenizer-ourselves",
    "href": "posts/tokenizers-deep-dive/tokenizers-deep-dive.html#assembling-a-llama-3-tokenizer-ourselves",
    "title": "Tokenizers deep dive",
    "section": "Assembling a Llama 3 tokenizer ourselves",
    "text": "Assembling a Llama 3 tokenizer ourselves\nLet’s now dive a bit deeper and see how we could construct the tokenizer ourselves. This is where the different design decisions come into play.\nEssentially, there are up to five components (most of them being optional) that have to be specified to have a working tokenizer. We will cover them briefly here, but I encourage reading this great piece of documentation to learn more about the various components available in the tokenizers library that can be used to assemble a tokenizer.\nThe 5 components are:\n\nNormalizer (optional): pre-process the input string for a given use case, e.g. strip accents (é -&gt; e) or make the strings lowercase.\nPre-tokenizer (optional): performs some initial splitting with the main intent being to prevent the existence of tokens that are too long, e.g. ones that connect multiple common words. We could avoid this by having a pre-tokenizer that splits across spaces.\nModel: an algorithm that performs the tokenization (i.e. takes strings, splits them an converts into tokens).\nPost-processor (optional): used for post-processing the tokenized string, e.g. we could add special tokens or apply some other template.\nDecoder (optional): helps using the tokenizer in the opposite directiom. i.e. mapping a list of token ids into readable text. In the simplest case, just the model’s vocabulary is enough to perform the reversal, but certain normalizers or pre-tokenizers add special characters which are removed by the decoder.\n\nWith this knowledge, we can now build our own Llama 3 tokenizer! We can check this file to learn what the hyperparamaters are and just copy them over. It turns out that Llama 3 does not use a normalizer (this is because normalization usually makes tokenization an irreversible process, e.g. if you remove accents, you cannot recover them during decoding), while the pre-tokenizer consists of a regex splitter and a byte-level pre-tokenizer. To understand the former, see this Andrej Karpathy’s video on tokenizers from 57:36 to 01:14:59, while for the latter, BPE tokenizers are trained by starting with an initial vocabulary and then iteratively merging the most common byte pairs until the desired vocabulary size is reached. Back when GPT-2 was created, instead of using the bytes corresponding to the first 256 Unicode characters, researchers decided to remap them to some other set of bytes. The reasons for this are not enrirely clear, but it is likely that using the set of the leading 256 bytes caused errors under their implementation.\n\n\n\n\n\n\nNote\n\n\n\nThis byte-level normalizer is actually the reason why the Ġ characters occur, since the space character \" \" is mapped to the Ġ character during normalization.\n\n\nBefore jumping further into the model and decoder used by Llama 3, let’s quickly see these concepts in action.\n\nimport regex as re\ngpt_4_pat = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n\nsome_text = \"Hello world, we're live 🤗. The number is 1234. How's it going!!!?\"\nprint(re.findall(gpt_4_pat, some_text))\n\n['Hello', ' world', ',', ' we', \"'re\", ' live', ' 🤗.', ' The', ' number', ' is', ' ', '123', '4', '.', ' How', \"'s\", ' it', ' going', '!!!?']\n\n\nWe can see that the regex splits by whitespace, numbers are no longer than 3 digits, and apostrophe abbreviations and punctuation are also separated out (the list is non-exhaustive). As the for ByteLevel pre-tokenizer, we can see that ordinary English characters remain unchanged, spaces are indeed replaced by the Ġ Unicode character, and the emoji is completely replace by a combination of multiple bytes:\n\npre_tokenizer = pre_tokenizers.ByteLevel(\n    add_prefix_space=False, trim_offsets=True, use_regex=False\n)\npre_tokenization_result = pre_tokenizer.pre_tokenize_str(some_text)[0][0]\npre_tokenization_result\n\n\"HelloĠworld,Ġwe'reĠliveĠðŁ¤Ĺ.ĠTheĠnumberĠisĠ1234.ĠHow'sĠitĠgoing!!!?\"\n\n\nReturning to the components utilized by the Llama 3 tokenizer, the model employs the Byte Pair Encoding (BPE) algorithm. To avoid duplicating the excellent explanations already available, I encourage you to view the relevant sections of Andrej Karpathy’s video or consult the corresponding Wikipedia entry. The ByteLevel post-processor simply reverses the byte shifting that took place in the the pre-tokenization phase, as well as prepends the output string with the &lt;|begin_of_text|&gt; special token (this can be turned off by setting add_special_tokens=False when calling a tokenizer). Finally, the ByteLevel decoder is responsible for mapping token ids back to human-readable strings.\n\n\n\n\n\n\nNote\n\n\n\nHow these design choices are made when building a completely new tokenizer from scratch is out of scope of this article, but it turns out that, in practice, it is enough to simply use a BPE model and only tune dataset used for training, the vocabulary size and the regular expression used to split strings during pre-tokenization.\n\n\nAssembling our own Llama 3 tokenizer\nThat was a lot to take in but we can finally assemble our own Llama 3 tokenizer:\n\n# Define the most important part - the model\ntokenizer_custom = Tokenizer(models.BPE(ignore_merges=True))\n\n# Add a pre-tokenizer\ntokenizer_custom.pre_tokenizer = pre_tokenizers.Sequence([\n    pre_tokenizers.Split(\n        pattern=Regex(\n            \"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\n        ),\n        behavior=\"isolated\",\n        invert=False,\n    ),\n    pre_tokenizers.ByteLevel(\n        add_prefix_space=False, trim_offsets=True, use_regex=False\n    )\n])\n\n# Add the post-processor\ntokenizer_custom.post_processor = processors.Sequence([\n    processors.ByteLevel(\n        add_prefix_space=True, trim_offsets=False, use_regex=True\n    ),\n    processors.TemplateProcessing(\n        single=\"&lt;|begin_of_text|&gt; $A\",\n        pair=\"&lt;|begin_of_text|&gt; $A &lt;|begin_of_text|&gt;:1 $B:1\",\n        special_tokens=[(\"&lt;|begin_of_text|&gt;\", 128000)],\n    ),\n])\n\n# And finally, the decoder\ntokenizer_custom.decoder = decoders.ByteLevel()\n\nLet’s train and save it:\n\nfrom tokenizers import trainers\n\ntrainer = trainers.BpeTrainer(\n    # the `vocab_size` here actually means the number of merges\n    vocab_size=len(tokenizer.get_vocab()) - len(pre_tokenizers.ByteLevel.alphabet()),\n    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n)\n\ntokenizer_custom.train_from_iterator(\n    batch_iterator(dataset, verbose=False), trainer=trainer\n)\n\n\ntokenizer_custom.add_tokens(list(tokenizer.get_added_vocab().keys()))\n\nprint(f\"Vocab length={len(tokenizer_custom.get_vocab())}\")\n\n# Save the new tokenizer\ntokenizer_custom.save(\"new-llama-tokenizer-custom-english-only/tokenizer.json\")\n\nVocab length=128256\n\n\nWe can check that the resulting tokenizer is indeed the same as the one that we have previously trained:\n\nwith open(\"new-llama-tokenizer-english-only/tokenizer.json\", \"r\") as f:\n    new_tokenizer_config = json.load(f)\n\nwith open(\"new-llama-tokenizer-custom-english-only/tokenizer.json\", \"r\") as f:\n    custom_tokenizer_config = json.load(f)\n\ncustom_tokenizer_config[\"model\"][\"merges\"] == new_tokenizer_config[\"model\"][\"merges\"]\n\nTrue\n\n\n\n\n\n\n\n\nNote\n\n\n\nUpon inspecting the corresponding tokenizer configuration files (file 1 and file 2) you might notice that the two vocabularies are not exactly the same which is because a tokenizer trained from an old one has all the added tokens prepended at the start of the vocabulary. This is why above we check that the merges (i.e. which tokens where merged and in what sequence) are the same for the two tokenizers, which they are."
  },
  {
    "objectID": "posts/thesis/thesis.html",
    "href": "posts/thesis/thesis.html",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "",
    "text": "Hello, World!\nThis blog post discusses the main ideas behind my thesis for the MPhil in Machine Learning and Machine Intelligence degree at the University of Cambridge. You can read the full thesis here, or check the associated GitHub repository.\nThe main idea behind the project is trying to build a reward models that reward “truthfulness” in a scalable fashion, which current state-of-the-art methods, such as reinforcement learning from human feedback (RLHF), are not capable of (note that we use quatations because we defined “truthfulness” in a narrow sense and mean only the performance on binary question-aswering tasks, see the thesis pdf for more details). Specifically, methods that discover latent knowledge, such as CCS, are used to determine whether a piece of input text is truthful or not. Such linear probes are then combined with pre-trained language models to make up reward models, which are used in reinforcement learning RL fine-tuning to improve the “truthfulness” of large language models (LLMs).\nThese reward models can be trained by using transformed versions of existing datasets, thus relaxing the requirement to collect large numbers of human preference data, as is usual in RLHF. We find that using our reward models along with a few regularization techniques (discussed below) can already be used to improve the “truthfulness” of pre-trained LLMs by up to 1.6%, as measured on the TruthfulQA benchmark. Importantly, such an improvement is achieved without sacrificing the models’ performance on more general NLP tasks (we evaluate on the Open LLM Leaderboard tasks).\nAlthough our method serves as a proof of concept on how hallucinations in LLMs could be tackled in the future, it still has many limitations. For one, the current best DLK methods still have a long way to go in terms of robustness. Moreover, our method only tackles the narrow definition of “truthfulness”, and even though the accuracy on TruthfulQA improves too, many would argue that it is still not a very good proxy for actually reducing levels of hallucination in LLMs. Finally, we found that the pre-trained models that we would fine-tune using RL had to be already quite capable, otherwise our method would not work."
  },
  {
    "objectID": "posts/thesis/thesis.html#main-steps",
    "href": "posts/thesis/thesis.html#main-steps",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "Main steps",
    "text": "Main steps\nThere are four main steps to run the method on new data: 1. Split the dataset and prepare it for reward model training and RL fine-tuning. 1. Train a reward model. 1. Performing RL fine-tuning on some pre-trained LLM. 1. Evaluate the fine-tuned LLM on both target and general NLP tasks.\nSteps 1 and 4 are mostly boring and you can find more details about them in the README of the GitHub repository, so we are going to focus on the theory and main code bits for steps 2 and 3."
  },
  {
    "objectID": "posts/thesis/thesis.html#reward-model-training",
    "href": "posts/thesis/thesis.html#reward-model-training",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "Reward model training",
    "text": "Reward model training\nAs discussed in more detail in chapter 3 of the thesis, the reward model is made up of a pre-trained language model with a probe attached at the end.\n\n\n\nThe architecture of the reward model\n\n\nThe reward model takes as an input a question \\(q_i\\) with a binary answer (e.g. “Yes”/“No”), creates a contrastive pair from it and then this contrastive pair \\((x_i^+, x_i^-)\\) is used to compute a reward (a number between 0 and 1). The reward is computed by recording activations of the last token in a layer of a language model, denoted \\(\\mathrm{\\textbf{emb}}(x_i^+)\\) and \\(\\mathrm{\\textbf{emb}}(x_i^-)\\). We would try all layers of a language model and pick the one that worked the best. Finally, the embeddings are passed to a logistic classificer which is of the form: \\[p(q_i) = \\sigma(\\textbf{w}^\\mathrm{T}(\\mathrm{\\textbf{emb}}(x_i^+) - \\mathrm{\\textbf{emb}}(x_i^-)))\\] which is the only module with trainable parameters, the vector \\(\\textbf{w}\\). Here, \\(\\sigma\\) is the sigmoid activation function. This output probability denotes the probability that the question \\(q_i\\) is “truthful” which is what we use as the reward.\nThere are a few other intricacies, such as how to prompt for “truthfulness” (custom prompts are needed), or how to actually find the optimal parameters vector \\(\\textbf{w}\\), but I will sugeest interested readers to refer to the thesis pdf."
  },
  {
    "objectID": "posts/thesis/thesis.html#rl-fine-tuning",
    "href": "posts/thesis/thesis.html#rl-fine-tuning",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "RL fine-tuning",
    "text": "RL fine-tuning\nOnce we have a reward model, we can plug into an RL algorithm to perform fine-tuning. We used the proximal policy optimization algorithm, as implement in the Transformer Reinforcement Learning (TRL) library from Hugging Face. We found that a few pieces of regularization had to be applied to stabilize the training process. The tricks are:\n\nPrompting - we found that a specialized prompt had to be devised for each model for the method to work (we mostly focused on the 7B Vicuna models).\nMaximum number of new tokens - we found that setting the number of new tokens to two was enough in our case since answers to our binary questions were short. Additionally, we applied output post-processing to strip any undesirable tokens (see the code below).\nEncouraging the models to only output in the desired format - we want the models to only respond with “Yes”/“No”, but even with specialized prompts the models would still sometimes generate different responses. To tackle this, we tweaked the reward to be -1 if the model does not respond in the desired format, and we would give the usual score from the reward model if the output was what the model was asked for. This encouraged the model to converge to only responding with the required format over time.\n\nTo illustrate these concepts, the finel RL training loop looked roughly like the following:\n\nimport torch\nimport string\n\n\nCHARACTERS_TO_FILTER = string.punctuation + \" \\n\"\n\n\ndef is_answer_yes_no(answer):\n    return answer in [\"Yes\", \"No\"]\n\n\ndef postprocess_response(response):\n    while response and response[-1] in CHARACTERS_TO_FILTER:\n        response = response[:-1]\n    return response\n\n\ndef train(\n    ppo_trainer,\n    tokenizer,\n    generation_kwargs,\n    get_rewards,\n    script_args, config,\n):\n    n_epochs = config.steps // len(ppo_trainer.dataloader)\n\n    for epoch in range(1, n_epochs + 1):\n        loop = tqdm(\n            enumerate(ppo_trainer.dataloader, 1),\n            total=len(ppo_trainer.dataloader), leave=False\n        )\n        for batch_idx, batch in loop:\n            # Get the input tensors\n            question_tensors = batch[\"input_ids\"]\n\n            # Get the generations\n            response_tensors = ppo_trainer.generate(\n                question_tensors,\n                return_prompt=False,\n                batch_size=script_args.generator_batch_size,\n                **generation_kwargs,\n            )\n            responses = tokenizer.batch_decode(\n                response_tensors, skip_special_tokens=True,\n                spaces_between_special_tokens=False\n            )\n\n            # Postprocess the responses\n            if script_args.postprocess_responses:\n                responses = [postprocess_response(x) for x in responses]\n            batch[\"response\"] = responses\n\n            # Compute the rewards (scores)\n            texts = [q + \" \" + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n            rewards = get_rewards(texts)\n\n            # Replace reward for undesired answers to -1\n            mask = [not is_answer_yes_no(x) for x in batch[\"response\"]]\n            mask = torch.tensor(mask, dtype=torch.bool) # cast to tensor\n            rewards[mask] = -1\n\n            # Make the rewards a list of tensors\n            rewards = [x for x in rewards]\n\n            # Run PPO step\n            stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n            ppo_trainer.log_stats(stats, batch, rewards)\n\nAnd the generation_kwargs look like the following:\n\ngeneration_kwargs = {\n    \"top_k\": 0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.pad_token_id,\n    \"eos_token_id\": 100_000,\n    \"pad_to_multiple_of\": 8,\n    \"max_new_tokens\": 2,\n}"
  },
  {
    "objectID": "posts/thesis/thesis.html#conclusion",
    "href": "posts/thesis/thesis.html#conclusion",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "Conclusion",
    "text": "Conclusion\nI hope you found this blog as interesting as it was for me to work on this project. I feel like I have learnt a lot during it, for example, I joined multiple ML communities and got involved in discussions with very smart and ambitious people. Perhaps my proudest achievement is making my first open-source contribution to the elk library (link), as well as reported multiple bugs to the big-refactor branch of Language Model Evaluation Harness (link).\nI am excited to dive deeper into LLMs-related topics in the future. Feel free to reach out if you have any opportunities on offer!"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "",
    "text": "This article is adapted from fast.ai’s Machine Learning for Coders course, specifically, lesson 10. I would highly recommend checking this and other courses from fast.ai, it has numerous tips on how to do practical machine learning and deep learning.\nWe will be building a naive Bayes classifier in just 10 lines of code that will get over 98% accuracy on a spam message filtering task.\nWe will do this in the top-bottom approach, where we will first build the model and then dig deeper into the theory of how it works."
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#the-countvectorizer",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#the-countvectorizer",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "1. The CountVectorizer",
    "text": "1. The CountVectorizer\nNaive Bayes classifier uses what is called a bag of words approach. It simply means that we disregard any relationships between words and just look at how often they appear in the text that we want to classify.\n\nNote: I am not saying that bag of words is the best approach to do NLP, it is usually quite the opposite as nowadays we have tools like RNNs and Transformers that perform much better on NLP tasks. However, we use it here because it is a really simple approach that sometimes still gives reasonable results, as it did in this case!\n\nThis is exactly what we use a CountVectorizer for: it produces a term document matrix with frequencies of each word for each message.\nLet’s look at an example of what sklearn’s CountVectorizer is doing. Suppose that our messages are:\n\n\n\nmessage\nlabel\n\n\n\n\nThis is a good message\n0\n\n\nA good message\n0\n\n\nThis message is bad\n1\n\n\nThe message is bad\n1\n\n\n\nThen, what a CountVectorizer is going to do for us is produce the following matrix:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\n\n\nImportant: CountVectorizer just finds the vocabulary (list of all the unique words in our data) and then counts how often each word from the vocabulary is found in each message.\n\nNotice that for a larger dataset, our vocabulary might become very large which would mean that most of the cells in our term document matrix would be 0. For this reason, the sklearn implementation actually produces a sparse matrix which, instead of storing all the entries, stores only the location and values of non-zero entries, and since there are only so many of them, saves huge amounts of memory. How neat!\nYou must have noticed that I used a max_df=0.1 parameter for my CountVectorizer. This tells the vectorizer to ignore any words that appear in more than 10% on the documents as we can safely say that they are too common. I came with this number by trying different values and looking at vectorizer.stop_words_ to check how many and which words were ignored until I was satisfied. When you build your own model, make sure to play around with this and other parameters, such as min_df (opposite of max_df, used for very rare words), to find what works best for you! You can find more information on what parameters for CountVectorizer can be tinkered on the official docs.\nA trick that we want to do before moving on is to note that later we will want to calculate probabilities of each word appearing in spam or non-spam messages. But it might happen that certain words do not appear in a particular class at all and we might run into trouble because we will get feature probabilities of zero, and we don’t want that. To counter that, we will add a row of ones, like so:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\nrow of ones\n\n1\n1\n1\n1\n1\n1\n1\n\n\n\nIf we think more about, adding a row of ones is not that counter-intuitive at all, since the messages that we have so far only carry information up to this point in time, but if a word has not appeared in any of the messages so far, it is not at all impossible that it will not appear in the future, so adding the extra one takes care of that for us.\nSo that our feature probabilities will be:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\nrow of ones\n\n1\n1\n1\n1\n1\n1\n1\n\n\nP(feature|0)\n\n0.67\n0.67\n1\n1\n1\n0.33\n0.33\n\n\nP(feature|1)\n\n0.67\n1\n0.33\n0.33\n1\n0.67\n1\n\n\n\nIn code:\n\n\n# Create a vectorizer object that will ignore any\n# words that appear in more than 10% of messages.\nvectorizer = CountVectorizer(max_df=0.1)\n\n# Use the fit_transform method of the vectorizer to get\n# the term document matrix for the training set.\ntrain_term_doc = vectorizer.fit_transform(X_train)\n\n# Use the transform method of the vectorizer to get\n# the term document matrix for the validation set.\n# We do it this way so that train and validation sets\n# are have the same vocabularies so that we could make predictions.\nval_term_doc = vectorizer.transform(X_val)"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#bayes-theorem",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#bayes-theorem",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "2. Bayes’ theorem",
    "text": "2. Bayes’ theorem\nNow, we can get to the essence of the model which is to apply Bayes’ theorem. I am not going to give the usual form of the formula, but instead one that will illustrate how we will be using it. Our goal is, given a particular message, figure out whether it is spam or not. Hence, the formula for us is going to take the form:\n\\(P(\\text{spam} \\mid \\text{message}) = \\frac{P(\\text{message} \\mid \\text{spam}) \\cdot P(\\text{spam})}{P(\\text{message})}\\)\nBut we can use a trick: instead of trying to predict whether it is spam or not, let’s look at which class is a message more likely, i.e. \\(\\frac{P(\\text{spam} \\mid \\text{message})}{P(\\text{non-spam} \\mid \\text{message})}\\). In that case, the formula will become:\n\\(\\text{ratio} = \\frac{P(\\text{spam} \\mid \\text{message})}{P(\\text{non-spam} \\mid \\text{message})} = \\frac{P(\\text{message} \\mid \\text{spam}) \\cdot P(\\text{spam})}{P(\\text{message} \\mid \\text{non-spam}) \\cdot P(\\text{non-spam})}\\)\nReferring to our previous example, the ratios would then be:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\nrow of ones\n\n1\n1\n1\n1\n1\n1\n1\n\n\nP(feature|0)\n\n0.67\n0.67\n1\n1\n1\n0.33\n0.33\n\n\nP(feature|1)\n\n0.67\n1\n0.33\n0.33\n1\n0.67\n1\n\n\nratio\n\n1\n1.5\n0.33\n0.33\n1\n2\n3\n\n\n\n\nImportant: As you can see, ratios are greater than 1 for features that are more likely to be in spam messages and lower than one otherwise.\n\nThen, to further simplify things, we make the naive Bayes assumption, which says that probability of any word appearing in a message is independent of probabilities of other words appearing in that same message.\n\nNote: Obviously, this is a very naive assumption and that is most certainly not the case, but it turns out to work quite well.\n\nUnder the naive assumption, the probabilities like \\(P(\\text{message} \\mid \\text{spam})\\) can be factorized into a product of probabilities of individual features appearing in a message, so that:\n\\(P(\\text{message} \\mid \\text{spam}) = \\prod_{i=1}^{n}{P(\\text{features[i]} \\mid \\text{spam})}\\)\nand similarly for \\(P(\\text{message} \\mid \\text{non-spam})\\). Then, our big formula becomes:\n\\(\\text{ratio} = \\frac{P(\\text{spam} \\mid \\text{message})}{P(\\text{non-spam} \\mid \\text{message})} = \\frac{\\prod_{i=1}^{n}{P(\\text{features[i]} \\mid \\text{spam})}}{\\prod_{i=1}^{n}{P(\\text{features[i]} \\mid \\text{non-spam})}} \\cdot \\frac{P(\\text{spam})}{P(\\text{non-spam})}\\)"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#a-few-final-tricks",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#a-few-final-tricks",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "3. A few final tricks",
    "text": "3. A few final tricks\nWe are almost done, now we just want to apply a few tricks to make our calculations easier. First, notice that multiplying lots of probabilities together is going to result into a very small number and we might run out of floating point precision, so we can take the natural logarithm instead to handle this. Note that in this case we compare ratios not with 1, but with 0 (because log(1)=0) and that by the properties of logarithms all the products are going to turn into sums, which makes everything even simpler!\nFinally, we notice that to make predictions, we can just perform matrix multiplication on the validation term document matrix and our derived vector of ratios and add (remember, we are in log space!) the ratio of priors.\nPutting it all together:\n\n\n# Calculate P(feature|1) and P(feature|0).\n# This plus ones are there to constitute the\n# row of ones discussed above\np = train_term_doc[y_train == 1].sum(0) + 1\nq = train_term_doc[y_train == 0].sum(0) + 1\n\n# Calculate the ratios according to our derived formulae\nratio = np.log((p / p.sum()) / (q / q.sum()))\n\n# Calculate the log of ratio of priors\nb = np.log((y_train == 1).sum() / (y_train == 0).sum())\n\n# Make some predictions on the validation set\npre_preds = val_term_doc @ ratio.T + b\npreds = pre_preds.T &gt; 0 # Greater than 0 because we are working in log space\n(preds == y_val).mean() # Accuracy\n\n0.9856502242152466"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#try-n-grams",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#try-n-grams",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "Try n-grams",
    "text": "Try n-grams\n\nvectorizer = CountVectorizer(ngram_range=(1, 3), max_df=0.1)\ntrain_term_doc = vectorizer.fit_transform(X_train)\nval_term_doc = vectorizer.transform(X_val)\n\n\np = train_term_doc[y_train == 1].sum(0) + 1\nq = train_term_doc[y_train == 0].sum(0) + 1\nratio = np.log((p / p.sum()) / (q / q.sum()))\nb = np.log((y_train == 1).sum() / (y_train == 0).sum())\n\n\npre_preds = val_term_doc @ ratio.T + b\npreds = pre_preds.T &gt; 0\n(preds == y_val).mean()\n\n0.9874439461883409\n\n\nTurns out the model gives even better accuracy with bigrams and trigrams included. But watch out! Checking the confusion matrices, we see that the model is now perfect on non-spam messages, but the error on spam messages has increased. This might not be what we want, so we have to be careful with interpreting our models!\n\nconfusion_matrix(y_val, preds.T, normalize=None)\n\narray([[968,   2],\n       [ 12, 133]])\n\n\n\nconfusion_matrix(y_val, preds.T, normalize=\"true\")\n\narray([[0.99793814, 0.00206186],\n       [0.08275862, 0.91724138]])"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#binarized-version",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#binarized-version",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "Binarized version",
    "text": "Binarized version\nYou can also try the binarized version of the term document matrix (i.e. instead of frequencies we look at whether a word is present or not)\n\npre_preds = val_term_doc.sign() @ ratio.T + b\npreds = pre_preds.T &gt; 0\n(preds == y_val).mean()\n\n0.9874439461883409"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#learning-the-parameters-with-logistic-regression",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#learning-the-parameters-with-logistic-regression",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "Learning the parameters with logistic regression",
    "text": "Learning the parameters with logistic regression\nFinally, you might try taking it to the next level and learning the parameters with a logistic regression instead of using the theoretical ones. Check the parameters C for regularization and dual=True for when you term document matrix is much wider than it is tall.\n\nm = LogisticRegression(C=1e1, dual=False)\nm.fit(train_term_doc, y_train)\npreds = m.predict(val_term_doc)\n(preds == y_val).mean()\n\n0.9811659192825112\n\n\n\n\n# Binarized version\nm = LogisticRegression(C=1e1, dual=False)\nm.fit(train_term_doc.sign(), y_train)\npreds = m.predict(val_term_doc.sign())\n(preds == y_val).mean()\n\n0.9802690582959641"
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html",
    "href": "posts/tokenizer-problems/tokenizer-problems.html",
    "title": "Good tokenizers is all you need",
    "section": "",
    "text": "Although it might seem like the title is just me giving in to the popular trends in naming things in the ML space, it is not actually that misleading. Tokenization is a crucial part of the whole language modelling pipeline. Yet, we will see in this blog post that there all sorts of problems that tokenization can cause that one might not be aware of.\nThis post is inspired by the amazing video on LLM tokenizers by Andrej Karpathy.\nKarpathy’s list of problems (taken from the following notes):\nI will use the trailtoken tool that I have recently built with a collaborator to inspect why these problems occur. I encourage you to play around with it, especially if you build tokenizers from scratch yourself. You might be surprised with how easily problems can occur if one is not careful enough!\nLet’s dive in!"
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#spelling",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#spelling",
    "title": "Good tokenizers is all you need",
    "section": "Spelling",
    "text": "Spelling\nTokenization makes it hard for the LLMs to spell. For example, one of the best open-source LLMs, Mistral-7B-Instruct, has a very hard time spelling the word antidisestablishmentarianism:\n\n\n\nThe mistralai/Mistral-7B-Instruct-v0.1 LLM misspelling the word antidisestablishmentarianism\n\n\nHowever, if we use trailtoken to inspect how the said word is tokenized by the Mistral-7B tokenizer, we see that it is actually split into 8 seemingly random tokens:\n\n\n\nVisualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes the word antidisestablishmentarianism\n\n\nIt is unlikely that the model has seen them occurring together during training, so it no surprise that it finds it hard separating out the letters constituting each token.\nOn the other hand, the OpenAI models seem to handle the task with ease (see these following links for gpt-3.5-turbo and gpt-4-turbo). Even then, this is mostly a result of the capabilities of these models, as the word is still tokenized into 6 tokens which are again more or less arbitrary:\n\n\n\nVisualisation of how the tokenizer of gpt-3.5-turbo tokenizes the word antidisestablishmentarianism\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere and below I use the Xenova/gpt-3.5-turbo tokenizer on trailtoken because it is an open-source implementation of the gpt-3.5-turbo tokenizer."
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#string-operations",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#string-operations",
    "title": "Good tokenizers is all you need",
    "section": "String operations",
    "text": "String operations\nSimilarly to spelling words, LLMs find it hard to to simple string operations, such as reversing words. A well-known example is asking LLMs to reverse the word lollipop. Mistral-7B fails miserably on the task:\n\n\n\nmistralai/Mistral-7B-Instruct-v0.1 LLM failing to reverse the word lollipop\n\n\nIf we look at how the word is tokenized, we see that it ends up being only 4 tokens, so it is no wonder that the LLMs find it hard solving this task:\n\n\n\nVisualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes the word lollipop\n\n\nThe screenshot above hints at a “hack” that can be used to help LLMs: separate out the letters so that they end up as separate tokens after tokenization. Even then, few-shot examples are required to make it work (and it also did not work when I separated the letters using dashes '-' or spaces ' '):\n\n\n\nmistralai/Mistral-7B-Instruct-v0.1 LLM succeeding at reversing the word lollipop\n\n\n\n\n\n\n\n\nCaution\n\n\n\nInterestingly, gpt-4 fails reversing the string too, so it is truly a problem caused by tokenization!\n\n\nFor the curious reader, there are many other seemingly “simple” operations that LLMs struggle with. One such example is try to count the number of letter in a word:\n\n\n\nGPT-4 LLM failing to count the number of letters in words\n\n\nCan you think of any other tasks where LLMs might struggle?"
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#non-english-languages-and-code",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#non-english-languages-and-code",
    "title": "Good tokenizers is all you need",
    "section": "Non-English languages and code",
    "text": "Non-English languages and code\nIt turns out that not all tokenizers are equally good at tokenizing non-English text, be it materials in foreign languages or code. I put these two together because they are very similar in nature, which is that training tokenizers on unbalanced text corpuses makes them worse at tokenizing the under-represented pieces of text (I might write a separate post on this, so stay tuned!).\nLet’s see this in action. The following piece of text is taken from Karpathy’s video referenced above. The text states something like “Nice to meet you, I’m ChatGPT, a large-scale language model developed by OpenAI. If you have any questions, feel free to ask.” in Korean.\n\nstr = \"만나서 반가워요. 저는 OpenAI에서 개발한 대규모 언어 모델인 ChatGPT입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\"\nlen(str)\n\n72\n\n\nWe can see that the string is made up of 72 characters. Let’s see how the older GPT-3 tokenizer would handle this:\n\n\n\nVisualisation of how the tokenizer of gpt-3 tokenizes Korean\n\n\nWe find that the resulting number of tokens is 132, much more than the original length of the string! This is because the tokenizer probably did not see much Korean during training, so it did not learn to represent the Hanguls (Korean “letters”) in the Korean alphabet efficiently (we see that the first Hangul is represented by three tokens).\nOn the other hand, the GPT-4 tokenizer is much better at Korean, the resulting number of tokens is 61 (more 2x less!), meaning that the Korean Hanguls are better represented in the larger tokenizer vocabulary:\n\n\n\nVisualisation of how the tokenizer of gpt-4 tokenizes Korean\n\n\nHowever, the problem still remains. If we inspect how the English version of the same phrase is tokenizer, we find another 2x reduction in the number of tokens:\n\n\n\nVisualisation of how the tokenizer of gpt-4 tokenizes English\n\n\nTherefore, poor tokenization means that the “information density” of non-English tokens is much lower, so LLMs have to attend to longer sequences to process the same amount of information, though the fact that they have probably seen much less non-English data during training also significantly contributes to the problem.\nSame thing goes for code. We find that the original code string is 197 characters:\n\ncode_str = '''for i in range(1, 101):\n    if i % 3 == 0 and i % 5 == 0:\n        print(\"FizzBuzz\")\n    elif i % 3 == 0:\n        print(\"Fizz\")\n    elif i % 5 == 0:\n        print(\"Buzz\")\n    else:\n        print(i)\n'''\nlen(code_str)\n\n197\n\n\nThe GPT-3 tokenizer compresses the string reasonably well, but there are a few areas for improvement. For instance, we see that every space of indentation is a separate token, so we waste a lof of tokens. Similarly, keywords like elif are broken into two tokens which is not ideal.\n\n\n\nVisualisation of how the tokenizer of gpt-3 tokenizes code\n\n\nJust as before, the GPT-4 tokenizer is much better at this, as we see in the image below. The indentation spaces are now grouped into a single token, and the elif keyword is a single token as well. The resulting tokenization saves about 30 tokens for the given piece of code. Imagine what the savings would be for a very large codebase!\n\n\n\nVisualisation of how the tokenizer of gpt-4 tokenizes code\n\n\nThere is no doubt that these improvements in the tokenizer have contributed to the much improved coding and multilingual performance of the GPT-4 model."
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#arithmetic",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#arithmetic",
    "title": "Good tokenizers is all you need",
    "section": "Arithmetic",
    "text": "Arithmetic\nThis is going to be a brief one, but LLM tokenizers can do odd things when tokenizing numbers which can hinder their performance on simple arithmetic. For example, here is the GPT-2 tokenizer behaves on the following numbers:\n\n\n\nVisualisation of how the tokenizer of gpt2 tokenizes numbers\n\n\nWe find that in one case the decimal is one token, but becomes two tokens if an extra 0 is appended at the end. This would not cause any trouble to us humans, but could potentially confuse the LLM.\nHowever, most of the modern LLM tokenizers are quite robust at handling numbers."
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#special-data-formats",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#special-data-formats",
    "title": "Good tokenizers is all you need",
    "section": "Special data formats",
    "text": "Special data formats\nHow structured data tokenized is another source of peculiar behaviours in LLMs. For example the images below show how the Mistral-7B tokenizer handles JSON and YAML formats:\n\n\n\nVisualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes JSON\n\n\n\n\n\nVisualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes YAML\n\n\nAs we can see, we can save up on tokens (about 2x!) and overall complexity if we use YAML over JSON. Also, notice how using underscores in key names adds extra tokens, so being smart about how structured data is passed to the tokenizer can help reduce the number of tokens used and, therefore, the compute costs!"
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#other-problems",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#other-problems",
    "title": "Good tokenizers is all you need",
    "section": "Other problems",
    "text": "Other problems\nFinally, I want to briefly touch upon the few remaining problems on Karpathy’s list which have either been solved completely or are very rare if one is careful enough.\nWhy did my LLM abruptly halt when it sees the string &lt;|endoftext|&gt;?\n&lt;|endoftext|&gt; is a special tokens which hints LLMs that they should stop generating text, so if you accidentally put that into the model, it might halt abruptly. Simple string processing techniques can help deal with such edge cases most of the time, though that sometimes leads to undesired behaviours, such as the one below which likely happens because OpenAI’s tokenizers remove special tokens from the input string during pre-processing:\n\n\n\ngpt-4 tokenizers remove special tokens from the input string during pre-processing\n\n\nWhat is this weird warning I get about a trailing whitespace?\nThe two images below will help us better understand what’s going on:\n\n\n\ngpt-4 tokenizer trailing whitespace issue\n\n\n\n\n\ngpt-4 tokenizer without trailing whitespace issue\n\n\nWe can see that in the first image, the trailing space becomes a separate token. However, the tokenizers often concatenate words the the spaces before them, just like the for word once in the second image. Since models are trained to predict the next token, during training the learn to predict the token once from the tokens that go before it up to the colon :, so the case where they have to predict the token once following a space ' ' is completely out of distrubution for them and can lead to all kinds of peculiar behaviours.\nWhy did the LLM break if I ask it about SolidGoldMagikarp?\nThis problem was quite hilarious, but the researchers at OpenAI trained their olders tokenizers on Reddit data and SolidGoldMagikarp is a Reddit user with lots of posts, so their username became a separate token in the LLMs vocabulary, but the same data did not make it into the training data, so the models behave more or less randomly if they ever see this token because they have never been trained to handle them."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Miscellaneous notes",
    "section": "",
    "text": "Are Llama 3 and GPT-4 tokenizers the same?\n\n\n\n\n\n\ndeep learning\n\n\nLLMs\n\n\ntokenization\n\n\n\nThere seems to be a lot overlap between the tokenizers of Llama 3 and GPT-4. How similar are they?\n\n\n\n\n\nMay 6, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nCubic spline interpolation exercise\n\n\n\n\n\n\nscientific computing\n\n\nnumerical methods\n\n\n\nLarge language models (predictably) learn to represent the semantic meaning of sentences.\n\n\n\n\n\nApr 13, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising contextualised large language model embeddings with context\n\n\n\n\n\n\ndeep learning\n\n\nLLMs\n\n\nvisualisation\n\n\n\nLarge language models (predictably) learn to represent the semantic meaning of sentences.\n\n\n\n\n\nApr 10, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/embedding-visualisation/embedding-visualisation.html",
    "href": "posts/embedding-visualisation/embedding-visualisation.html",
    "title": "Visualising large language model embeddings",
    "section": "",
    "text": "In a recent post, I shared a tool developed alongside a dear friend of mine, designed for visualizing the tokenizers of language models. Following that post, I received inquiries about my views on visualizing the embeddings learned by Large Language Models (LLMs) during their pre-training phase. This post aims to serve as a guide detailing one possible way to visualizing the high-dimensional embeddings of LLMs, while also delving into the interesting patters that become apparent through such visualizations. The findings are quite intriguing, so continue reading to discover more!\n\nImports\nToggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nimport re\n\nimport plotly.graph_objects as go\nfrom sklearn.manifold import TSNE\nfrom transformers import AutoModel, AutoTokenizer\n\n# Ensures we can render plotly plots with quarto\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n\nCode\n# Put default plotly colors into a variable\nimport plotly.express as px\nDEFAULT_PLOTLY_COLORS = px.colors.qualitative.Plotly\n\n# Put full month names into a constant\nimport calendar\nMONTHS = [month for month in calendar.month_name if month]\n\nCOUNTRY_TOKENS = [\n    \"state\", \"states\", \"international\", \"world\", \"united\", \"washington\",\n    \"california\", \"uk\", \"america\", \"american\", \"british\", \"australia\",\n    \"australian\", \"canada\", \"english\", \"french\", \"german\", \"russian\",\n    \"european\", \"europe\", \"france\", \"germany\", \"england\", \"london\",\n    \"york\", \"japanese\", \"chinese\", \"japan\", \"china\", \"indian\", \"india\"\n]\n\n\n\n\nDimensionality reduction\nAs you can see below, the code uses very standard libraries and functions. We begin by loading a model and extracting the tensor containing the embedding vectors:\n\nmodel_name = \"google-bert/bert-base-cased\"\nmodel = AutoModel.from_pretrained(model_name)\n\n\nembedding_vectors = model.embeddings.word_embeddings.weight.data\nembedding_vectors.shape\n\ntorch.Size([28996, 768])\n\n\nWe also extract the unique tokens in the tokenizer:\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nvocab = tokenizer.get_vocab()\n\ntokens = sorted(tokenizer.get_vocab().items(), key=lambda item: item[1])\ntokens = [item[0] for item in tokens]\ntokens[:5]\n\n['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]']\n\n\nThe essence of the code is below. We use the t-SNE dimensionality reduction technique to reduce the dimensionality of the embedding vectors to 2.\nAlternatively, one could use the UMAP8 library to compute the embeddings. The code runs much faster for large embedding matrices, but we are only going to use a subset of the embedding matrix in this post, so t-SNE will do just fine. However, I will leave the code for the interested reader to try out:\nimport umap\nreducer = umap.UMAP()\ndata_2d = reducer.fit_transform(embedding_vectors[NUM_TOKENS_TO_SKIP:NUM_TOKENS_TO_SKIP+NUM_TOKENS_TO_VISUALISE])\n\nNUM_TOKENS_TO_VISUALISE = 2000\nNUM_TOKENS_TO_SKIP = 100\n\n\n# Performing t-SNE to reduce the dataset to 2 dimensions\ntsne = TSNE(n_components=2, random_state=42)\ndata_2d = tsne.fit_transform(\n    embedding_vectors[NUM_TOKENS_TO_SKIP:NUM_TOKENS_TO_SKIP+NUM_TOKENS_TO_VISUALISE]\n)\ndata_2d.shape\n\n(2000, 2)\n\n\nThe code to visualise the embeddings is below. The first cell contains mundane code to color the various token groups, toggle the cell if you are interested to see it.\n\n\nCode\nrelevant_tokens = tokens[NUM_TOKENS_TO_SKIP:NUM_TOKENS_TO_SKIP + NUM_TOKENS_TO_VISUALISE]\n\n# Define default colors\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[0], token) for token in relevant_tokens\n]\n\n# Color numbers\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[1] if token.isdigit() else color, token) for color, token in colors\n]\n\n# Color tokens that start with ##\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[2] if token.startswith(\"##\") else color, token) for color, token in colors\n]\n\n# Color months\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[3] if token in MONTHS else color, token) for color, token in colors\n]\n\n# Color special tokens\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[4] if token in tokenizer.special_tokens_map.values() else color, token)\n    for color, token in colors\n]\n\n# Color single letters\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[5] if re.match(r\"^[a-zA-Z]$\", token) else color, token) for color, token in colors\n]\n\n# Color country tokens\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[6] if token.lower() in COUNTRY_TOKENS or token == \"US\" else color, token)\n    for color, token in colors\n]\n\n# Leave just the colors\ncolors = [color for color, _ in colors]\n\n# Define cluster names\ncluster_names = [\n    \"default\", \"numbers\", \"tokens that start with ##\", \"months\",\n    \"special tokens\", \"single letters\", \"countries\",\n]\nunique_colors = DEFAULT_PLOTLY_COLORS[: len(cluster_names)]\n\n\n\nfig = go.Figure(\n    data=go.Scatter(\n        x=data_2d[:, 0],\n        y=data_2d[:, 1],\n        mode=\"markers\",\n        marker=dict(color=colors),\n        hovertext=relevant_tokens,\n        hoverinfo=\"text\",\n        showlegend=False\n    )\n)\n\n# Create dummy traces to have a nice legend\nfor name, color in zip(cluster_names, unique_colors):\n    fig.add_trace(go.Scatter(\n        x=[None], y=[None],\n        mode=\"markers\", marker=dict(color=color), name=name\n    ))\n\nfig.update_layout(title=\"Token embeddings\", legend=dict(title=\"Clusters\"))\n\nfig.show()\n\n                                                \n\n\nWe can see that there is a lot of semantic structure in the learnt embeddings! For instance, I have highlighted the clusters for names of the months, countries/nationalities, numbers, etc. Of course, one could have expected this to happen given all the theory behind methods like word2vec, but it is still exciting that the semantic structure is still present in the embedding layer even though the model much deeper and the LLM pre-training objective is completely different from word2vec!\n\n\nConclusion\nI was very pleased to see that, as expected, the learnt patters have a nice semantic structure. I encourage the interested reader to explore the ideas in this post in more depth, e.g. try a different pre-trained LLM, try the UMAP dimensionality reduction method instead of t-SNE, or just play around with the number of embedded vectors and try to find more semantic clusters.\nThank you for reading! I hope you enjoyed the post and am looking forward to hearing your feedback.\n\n\nNext steps\nI would like to explore the idea of clustering these embeddings in the future and trying to look for what clusters emerge and what tokens get grouped into the same cluster. Again, please be encouraged to try it yourself and let me know about the results!"
  },
  {
    "objectID": "posts/captcha-fastai/captcha-fastai.html",
    "href": "posts/captcha-fastai/captcha-fastai.html",
    "title": "Multi-output classification for captcha recognition using fastai",
    "section": "",
    "text": "Special thanks to my dear friends Julius and Laurynas who joined me to work on this project.\nWe worked on and off for more than a month on this project, and I must say that it was not an easy, but definitely an extremely rewarding experience considering how much I’ve learnt. It was also a good chance to hone my problem solving skills and endurance, as we switched approaches (and even libraries!) at least three times, spent substantial amounts of time reading through various resources and codebases and trying to adapt the codes to our purpose through trial and error.\nThe key takeaways are:\n\nDo not ever use old captchas and encourage others not to, as there are too many ways to break them nowadays.\nFastai has a great mid-level API that allows for customization for almost any use, but getting the hang of it might be tricky. Here are some resources that tremendously helped us:\n\nLooking at fastai docs and tutorials and reading through source code\nA walk with fastai2 playlist that has excellent material on using the fastai mid-level API\nfastai v2 walk-thru playlist on YouTube where Jeremy Howard talks how and why the library is built the way it is\n\nPerseverance is key - great ideas and solutions usually do not come overnight, but breaking the problem into smaller pieces and continously improving on each of those is a good way to go."
  },
  {
    "objectID": "posts/captcha-fastai/captcha-fastai.html#check-validation-set-results",
    "href": "posts/captcha-fastai/captcha-fastai.html#check-validation-set-results",
    "title": "Multi-output classification for captcha recognition using fastai",
    "section": "Check validation set results",
    "text": "Check validation set results\nCheck the model’s performance on a batch of data:\n\nlearn.show_results()"
  },
  {
    "objectID": "posts/captcha-fastai/captcha-fastai.html#get-failed-captchas",
    "href": "posts/captcha-fastai/captcha-fastai.html#get-failed-captchas",
    "title": "Multi-output classification for captcha recognition using fastai",
    "section": "Get failed captchas",
    "text": "Get failed captchas\nGet predictions:\n\ninputs, _, targets, decoded_preds = learn.get_preds(with_input=True, with_decoded=True)\ninputs.size(), targets.size(), decoded_preds.size()\n\n\n\n\n(torch.Size([1991, 3, 64, 64]), torch.Size([1991, 4]), torch.Size([1991, 4]))\n\n\nIndices of failed captchas are:\n\nfailed_idxs = (~(decoded_preds == targets).all(dim=1)).nonzero().view(-1)\n\nShow those captchas to check why the model failed:\n\ndls.show_results(b=(inputs[failed_idxs], targets[failed_idxs]), out=decoded_preds[failed_idxs])\n\n\n\n\n\n\n\n\nWe can see that the model does some slight errors, but it only fails on 4 captchas out of a validation set of 1991 images which is an amazing result. I hope that it is now obvious that using captchas to protect a website from bots is not a good idea.\nCheck the model’s confidence when it makes errors:\n\nfor idx in failed_idxs.detach().cpu().numpy().flatten():\n    decoded_pred, pred, model_output = learn.predict(dls.valid_ds[idx][0])\n    probs = torch.softmax(model_output, dim=0)[pred].diag()\n    print(f\"Actual: {''.join([i2l[v.item()] for v in dls.valid_ds[idx][1]])}, predicted: {decoded_pred}, probs: {probs}\")\n\n\n\n\nActual: W4KN, predicted: WAKN, probs: tensor([0.0805, 0.0574, 0.0800, 0.0804])\nActual: B56M, predicted: B5GM, probs: tensor([0.0775, 0.0805, 0.0517, 0.0805])\nActual: M3AH, predicted: MBAH, probs: tensor([0.0805, 0.0492, 0.0803, 0.0805])\nActual: 3WZP, predicted: 3W2P, probs: tensor([0.0805, 0.0795, 0.0597, 0.0804])\n\n\n\n\n\n\n\n\n\n\n\n\nNote: In each case the probabilities for the failed letters were lower."
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html",
    "href": "posts/basic-panorama/basic-panorama.html",
    "title": "Basic panorama stitching",
    "section": "",
    "text": "In this notebook we will find that we can use knowledge of computer vision to create our own panoramas! We only need to take a few photos with a smartphone or camera, but we have to make sure that we only change the angle of the phone/camera, and not the position! This notebook then takes care of everything else.\nThis notebook is based on the Computer Vision course by Andreas Geiger."
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#preliminaries",
    "href": "posts/basic-panorama/basic-panorama.html#preliminaries",
    "title": "Basic panorama stitching",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom pathlib import Path\n\n\nLet’s first import the some images that we want to stich together:\n\nPATH_TO_YOUR_IMAGES = Path(\"path/to/your/images\")\n\n\n# Load images\nimages = sorted([f for f in PATH_TO_YOUR_IMAGES.iterdir()])\nimages = map(str, images)\nimages = map(cv2.imread, images)\nimages = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images]\n\nlen(images)\n\n10\n\n\n\nimages = images[2:4] + images[5:]\n\nLet’s have a look at the images:\n\nfig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n\nfor img, ax in zip(images, axes.flatten()):\n    ax.imshow(img)\n    ax.axis(\"off\")\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#finding-keypoints",
    "href": "posts/basic-panorama/basic-panorama.html#finding-keypoints",
    "title": "Basic panorama stitching",
    "section": "Finding keypoints",
    "text": "Finding keypoints\nTo estimate the homography matrix, we need correspondence pairs between images. The following is a function for this based on feature matching:\n\ndef get_keypoints(img1, img2):\n    orb = cv2.ORB_create(nfeatures=2000)\n\n    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n    bf = cv2.BFMatcher_create(cv2.NORM_HAMMING)\n\n    # Find matching points\n    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n    good = []\n    for m, n in matches:\n        if m.distance &lt; 0.5 * n.distance:\n            good.append(m)\n    \n    p_source = np.float32([keypoints1[good_match.queryIdx].pt for good_match in good]).reshape(-1, 2)\n    p_target = np.float32([keypoints2[good_match.trainIdx].pt for good_match in good]).reshape(-1, 2)\n    \n    N = p_source.shape[0]\n    p_source = np.concatenate([p_source, np.ones((N, 1))], axis=-1)\n    p_target = np.concatenate([p_target, np.ones((N, 1))], axis=-1)\n    \n    return p_source, p_target\n\nLet’s now look at some correspondence pairs. For this, the we use the draw_matches function:\n\ndef draw_matches(img1, points_source, img2, points_target):\n    ''' Returns an image with matches drawn onto the images.\n    '''\n    r, c = img1.shape[:2]\n    r1, c1 = img2.shape[:2]\n\n    output_img = np.zeros((max([r, r1]), c + c1, 3), dtype='uint8')\n    output_img[:r, :c, :] = np.dstack([img1])\n    output_img[:r1, c:c + c1, :] = np.dstack([img2])\n\n    for p1, p2 in zip(points_source, points_target):\n        (x1, y1) = p1[:2]\n        (x2, y2) = p2[:2]\n\n        cv2.circle(output_img, (int(x1), int(y1)), 10, (0, 255, 255), 10)\n        cv2.circle(output_img, (int(x2) + c, int(y2)), 10, (0, 255, 255), 10)\n\n        cv2.line(output_img, (int(x1), int(y1)), (int(x2) + c, int(y2)), (0, 255, 255), 5)\n\n    return output_img\n\nWe calculate the keypoints:\n\nkeypoint_pairs = [get_keypoints(img1, img2) for img1, img2 in zip(images[:-1], images[1:])]\nsource_points = [pair[0] for pair in keypoint_pairs]\ntarget_points = [pair[1] for pair in keypoint_pairs]\n\nlen(source_points), len(target_points)\n\n(6, 6)\n\n\nCheck how many keypoints we have:\n\nfor i in range(len(source_points)):\n    print(source_points[i].shape, target_points[i].shape)\n\n(305, 3) (305, 3)\n(188, 3) (188, 3)\n(500, 3) (500, 3)\n(482, 3) (482, 3)\n(388, 3) (388, 3)\n(476, 3) (476, 3)\n\n\nVisualise the keypoints:\n\nmatches_to_show = 200\n\nfor img1, p_source, img2, p_target in zip(images[:-1], source_points, images[1:], target_points):\n    f = plt.figure(figsize=(20, 10))\n    vis = draw_matches(img1, p_source[:matches_to_show], img2, p_target[:matches_to_show])\n    plt.axis(\"off\")\n    plt.imshow(vis)"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#estimating-the-homography-matrix",
    "href": "posts/basic-panorama/basic-panorama.html#estimating-the-homography-matrix",
    "title": "Basic panorama stitching",
    "section": "Estimating the homography matrix",
    "text": "Estimating the homography matrix\nAfter looking at the correspondences, let’s stitch the images together! In order to stitch together the images, we need a function to return the 2x9 homography matrix A_i matrix for a given 2D correspondence pair xi_vector and xi_prime_vector (which are 3D homogeneous vectors).\n\ndef get_Ai(xi_vector, xi_prime_vector):\n    ''' Returns the A_i matrix discussed in the lecture for input vectors.\n    \n    Args:\n        xi_vector (array): the x_i vector in homogeneous coordinates\n        xi_vector_prime (array): the x_i_prime vector in homogeneous coordinates\n    '''\n    assert xi_vector.shape == (3, ) and xi_prime_vector.shape == (3, )\n\n    Ai = np.zeros((2, 9))\n    Ai[0, 3:6] = -xi_prime_vector[2] * xi_vector\n    Ai[0, 6:9] = xi_prime_vector[1] * xi_vector\n    Ai[1, 0:3] = xi_prime_vector[2] * xi_vector\n    Ai[1, 6:9] = -xi_prime_vector[0] * xi_vector\n\n    assert(Ai.shape == (2, 9))\n    \n    return Ai\n\nUsing get_Ai, write a function get_A which returns the A matrix of size 2Nx9:\n\ndef get_A(points_source, points_target):\n    ''' Returns the A matrix discussed in the lecture.\n    \n    Args:\n        points_source (array): 3D homogeneous points from source image\n        points_target (array): 3D homogeneous points from target image\n    '''\n    N = points_source.shape[0]\n\n    # Insert your code here\n    A = np.vstack([\n        get_Ai(src, target) for src, target in zip(points_source, points_target)\n    ])\n    \n    assert(A.shape == (2*N, 9))\n    return A\n\nNext, implement the function get_homography which returns the homography H for point correspondence pairs. We obtain H by performing the Direct Linear Transformation (DLT) algorithm:\n\ndef get_homography(points_source, points_target):\n    ''' Returns the homography H.\n    \n    Args:\n        points_source (array): 3D homogeneous points from source image\n        points_target (array): 3D homogeneous points from target image        \n    '''\n\n    # Insert your code here\n    A = get_A(points_source, points_target)\n    _, _, V_T = np.linalg.svd(A)\n    H = V_T.T[:, -1].reshape((3, 3))\n\n    assert H.shape == (3, 3)\n    \n    return H\n\nWe need a function which takes in the two images and the calculated homography and it returns the stiched image in a format which we can display easy with matplotlib. This function is provided in the following.\n\ndef stitch_images(img1, img2, H):\n    ''' Stitches together the images via given homography H.\n\n    Args:\n        img1 (array): image 1\n        img2 (array): image 2\n        H (array): homography\n    '''\n\n    rows1, cols1 = img1.shape[:2]\n    rows2, cols2 = img2.shape[:2]\n\n    list_of_points_1 = np.float32([[0,0], [0, rows1],[cols1, rows1], [cols1, 0]]).reshape(-1, 1, 2)\n    temp_points = np.float32([[0,0], [0,rows2], [cols2,rows2], [cols2,0]]).reshape(-1,1,2)\n\n    list_of_points_2 = cv2.perspectiveTransform(temp_points, H)\n    list_of_points = np.concatenate((list_of_points_1,list_of_points_2), axis=0)\n\n    [x_min, y_min] = np.int32(list_of_points.min(axis=0).ravel() - 0.5)\n    [x_max, y_max] = np.int32(list_of_points.max(axis=0).ravel() + 0.5)\n\n    translation_dist = [-x_min,-y_min]\n\n    H_translation = np.array([[1, 0, translation_dist[0]], [0, 1, translation_dist[1]], [0, 0, 1]])\n    H_final = H_translation.dot(H)\n\n    output_img = cv2.warpPerspective(img2, H_final, (x_max-x_min, y_max-y_min))\n    \n    output_img[translation_dist[1]:rows1+translation_dist[1], translation_dist[0]:cols1+translation_dist[0]] = img1\n    output_img = output_img[translation_dist[1]:rows1+translation_dist[1]:, :]\n\n    # Find top and bottom rows\n    top_row = output_img.nonzero()[0].min()\n    bottom_row = output_img.nonzero()[0].max()\n\n    top_max = output_img[top_row, :].nonzero()[0].max()\n    bottom_max = output_img[bottom_row, :].nonzero()[0].max()\n   \n    # Cut width\n    output_img = output_img[:, :min(top_max, bottom_max)]\n\n    return output_img\n\nWith this, we can stitch two images together and see how it looks:\n\nfor img1, p_source, img2, p_target in zip(images[:-1], source_points, images[1:], target_points):\n    H = get_homography(p_target, p_source)\n    stitched_image = stitch_images(img1, img2, H)\n\n    fig = plt.figure(figsize=(15, 10))\n    plt.axis(\"off\")\n    plt.imshow(stitched_image)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we repeat this process iteratively until all images are stitched together:\n\ndef get_panorama(images):\n    n_repeats = len(images) - 1\n\n    current_images = images\n\n    for _ in range(n_repeats):\n        new_images = []\n\n        keypoint_pairs = [get_keypoints(img1, img2) for img1, img2 in zip(current_images[:-1], current_images[1:])]\n        source_points = [pair[0] for pair in keypoint_pairs]\n        target_points = [pair[1] for pair in keypoint_pairs]\n\n        for img1, p_source, img2, p_target in zip(current_images[:-1], source_points, current_images[1:], target_points):\n            H = get_homography(p_target, p_source)\n            stitched_image = stitch_images(img1, img2, H)\n\n            new_images.append(stitched_image)\n\n        current_images = new_images\n    \n    assert len(current_images) == 1\n\n    return current_images[0]\n\n\npanorama = get_panorama(images)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(panorama)\nplt.axis(\"off\");"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#conclusion",
    "href": "posts/basic-panorama/basic-panorama.html#conclusion",
    "title": "Basic panorama stitching",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s it! We now have a very basic panorama stitcher.\nThis approach is quite naive and I am sure that real-world algorithms to produce panoramas are much more sophisticated. Therefore, I am open to hear any feedback or suggestions that you may have!"
  },
  {
    "objectID": "posts/uom-project/uom-project.html",
    "href": "posts/uom-project/uom-project.html",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "",
    "text": "Hello, World!\nThis blog post presents my undergraduate project on the numerical solutions to the Navier-Stokes equations. You can find more information by reading the project report here, or check the associated GitHub repository."
  },
  {
    "objectID": "posts/uom-project/uom-project.html#main-ideas",
    "href": "posts/uom-project/uom-project.html#main-ideas",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "Main ideas",
    "text": "Main ideas\nThe idea is simple - using numerical methods to approximate solutions to the Navier-Stokes equations. The particular setup that I used was not too interesting and was chosen because it is a good toy example, you can find all of the details are in the PDF linked above. Here I will present only the main ideas:\n\nCompare different approaches to solve the problem: time-stepping methods versus finding the steady-state solution by using Newton-Raphson iteration.\nCompare different implementations of the methods and their running speeds and memory usage.\nLearn more about optimization and scientific computing and, most important of all, have fun!"
  },
  {
    "objectID": "posts/uom-project/uom-project.html#opportunities-for-growth",
    "href": "posts/uom-project/uom-project.html#opportunities-for-growth",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "Opportunities for growth",
    "text": "Opportunities for growth\nAs mentioned above, I had many opportunities to strengthen my understanding of optimization and scientific computing. In particular, I:\n\nLearnt about various (non)linear optimization methods: least- quares, Newton-Raphson iteration, Gauss-Newton method, LBFGS, etc.\nLearnt how these methods can be used through various Python packages (SciPy, PyTorch, but also JAX which I found to be amazing, expect more projects built on JAX in the future!).\nCompared the performance of the various methods using the different implementations.\nAlso learnt the basic of nbdev and really enjoyed using it, for example, it made testing and publishing the code extremely easy (just for fun, I put up the code on PyPI)."
  },
  {
    "objectID": "posts/uom-project/uom-project.html#findings",
    "href": "posts/uom-project/uom-project.html#findings",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "Findings",
    "text": "Findings\nYou can find many more findings in the linked PDF, but the most interesting findings to me seem to be:\n\nNewton-Raphson iteration is much faster than time-stepping methods (although that is only true in this particular case - it is not always possible to apply this method).\nAs can be seen in this notebook, PyTorch and JAX perform faster both on CPU and GPU, but that’s probably expected.\nOn a GPU, PyTorch and JAX perform mostly the same. I expected JAX to be faster due to the fact that it uses jitting, but maybe the scale of the problem was not large enough to see a real difference (might be an interesting direction for future work)."
  },
  {
    "objectID": "posts/uom-project/uom-project.html#limitations",
    "href": "posts/uom-project/uom-project.html#limitations",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "Limitations",
    "text": "Limitations\nI feel content with the project for now, but here is a (inexhaustive) list of things that could be improved:\n\nThe current solvers are specific (the specifics of the problem are hard-coded into them). It might be worth abstracting the parts that define the problem into a separate entity and make the solvers operate on that entity.\nThere is a lot of code that was written in a rush, so it is not always as elegant as it could be.\nMore tests could be added."
  },
  {
    "objectID": "posts/uom-project/uom-project.html#conclusion",
    "href": "posts/uom-project/uom-project.html#conclusion",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "Conclusion",
    "text": "Conclusion\nI found the project exciting and I hope you found it interesting reading this blog post. It was fun learning JAX and nbdev and highly recommend everyone to try them. I Do not hesitate to contact me if you have any questions. Until next time!"
  },
  {
    "objectID": "learning/quantization-in-depth/L4_building_quantizer_quantize_models.html",
    "href": "learning/quantization-in-depth/L4_building_quantizer_quantize_models.html",
    "title": "L4-C - Building your own Quantizer: Quantize any Open Source PyTorch Model",
    "section": "",
    "text": "In this lesson, you will look at the results of open source models compressed using the custom quantizer you built.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n# colors for visualization\nCOLORS = [\n    [0.000, 0.447, 0.741],\n    [0.850, 0.325, 0.098],\n    [0.929, 0.694, 0.125],\n    [0.494, 0.184, 0.556],\n    [0.466, 0.674, 0.188],\n    [0.301, 0.745, 0.933],\n]\n\n\ndef plot_results(model, pil_img, results):\n    plt.figure(figsize=(16, 10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    scores, labels, boxes = results[\"scores\"], results[\"labels\"], results[\"boxes\"]\n    colors = COLORS * 100\n    for score, label, (xmin, ymin, xmax, ymax), c in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n        text = f\"{model.config.id2label[label]}: {score:0.2f}\"\n        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor=\"yellow\", alpha=0.5))\n    plt.axis(\"off\")\n    plt.show()\n\n\n############# From the previous lesson(s) of \"Building your own Quantizer\"\ndef w8_a16_forward(weight, input, scales, bias=None):\n\n    casted_weights = weight.to(input.dtype)\n    output = F.linear(input, casted_weights) * scales\n\n    if bias is not None:\n        output = output + bias\n\n    return output\n\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n        super().__init__()\n\n        self.register_buffer(\"int8_weights\", torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n\n        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n\n        if bias:\n            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n\n        else:\n            self.bias = None\n\n    def quantize(self, weights):\n        w_fp32 = weights.clone().to(torch.float32)\n\n        scales = w_fp32.abs().max(dim=-1).values / 127\n        scales = scales.to(weights.dtype)\n\n        int8_weights = torch.round(weights / scales.unsqueeze(1)).to(torch.int8)\n\n        self.int8_weights = int8_weights\n        self.scales = scales\n\n    def forward(self, input):\n        return w8_a16_forward(self.int8_weights, input, self.scales, self.bias)\n\n\ndef replace_linear_with_target_and_quantize(module, target_class, module_name_to_exclude):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n            old_bias = child.bias\n            old_weight = child.weight\n\n            new_module = target_class(child.in_features, child.out_features, old_bias is not None, child.weight.dtype)\n            setattr(module, name, new_module)\n\n            getattr(module, name).quantize(old_weight)\n\n            if old_bias is not None:\n                getattr(module, name).bias = old_bias\n        else:\n            # Recursively call the function for nested modules\n            replace_linear_with_target_and_quantize(child, target_class, module_name_to_exclude)\n\n\n###################################"
  },
  {
    "objectID": "learning/quantization-in-depth/L4_building_quantizer_quantize_models.html#step-3-test-the-implementation-on-various-llms",
    "href": "learning/quantization-in-depth/L4_building_quantizer_quantize_models.html#step-3-test-the-implementation-on-various-llms",
    "title": "L4-C - Building your own Quantizer: Quantize any Open Source PyTorch Model",
    "section": "Step 3: Test the Implementation on Various LLMs",
    "text": "Step 3: Test the Implementation on Various LLMs\n\n3.1 - Salesforce/codegen-350M-mono\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_id = \"Salesforce/codegen-350M-mono\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nprint(pipe(\"def hello_world():\", max_new_tokens=20, do_sample=False)[0][\"generated_text\"])\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\ndef hello_world():\n    print(\"Hello World\")\n\nhello_world()\n\n# 파\n\n\n\nprint(\"Model before:\\n\\n\", model)\n\nModel before:\n\n CodeGenForCausalLM(\n  (transformer): CodeGenModel(\n    (wte): Embedding(51200, 1024)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-19): 20 x CodeGenBlock(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): CodeGenAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n        )\n        (mlp): CodeGenMLP(\n          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n)\n\n\n\nreplace_linear_with_target_and_quantize(model, W8A16LinearLayer, [\"lm_head\"])\n\n\npipe.model\n\nCodeGenForCausalLM(\n  (transformer): CodeGenModel(\n    (wte): Embedding(51200, 1024)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-19): 20 x CodeGenBlock(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): CodeGenAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (qkv_proj): W8A16LinearLayer()\n          (out_proj): W8A16LinearLayer()\n        )\n        (mlp): CodeGenMLP(\n          (fc_in): W8A16LinearLayer()\n          (fc_out): W8A16LinearLayer()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n)\n\n\n\nprint(pipe(\"def hello_world():\", max_new_tokens=20, do_sample=False)[0][\"generated_text\"])\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\ndef hello_world():\n    print(\"Hello World\")\n\n# hello_world()\n\n# def hello_\n\n\n\n\n3.2 - facebook/detr-resnet-50\n\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\nfrom PIL import Image\nimport requests\n\n# you can specify the revision tag if you don't want the timm dependency\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n\n\nprevious_memory_footprint = model.get_memory_footprint()\n\n\nprint(\"Footprint of the model in MBs: \", previous_memory_footprint / 1e6)\n\nFootprint of the model in MBs:  166.524032\n\n\n\nimg_path = \"data/dinner_with_friends.png\"\nimage = Image.open(img_path).convert(\"RGB\")\nimage\n\n\n\n\n\n\n\n\n\ninputs = processor(images=image, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score &gt; 0.9\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n\n\nplot_results(model, image, results)\n\n\n\n\n\n\n\n\n\nmodel\n\nDetrForObjectDetection(\n  (model): DetrModel(\n    (backbone): DetrConvModel(\n      (conv_encoder): DetrConvEncoder(\n        (model): ResNetBackbone(\n          (embedder): ResNetEmbeddings(\n            (embedder): ResNetConvLayer(\n              (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n              (normalization): DetrFrozenBatchNorm2d()\n              (activation): ReLU()\n            )\n            (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n          )\n          (encoder): ResNetEncoder(\n            (stages): ModuleList(\n              (0): ResNetStage(\n                (layers): Sequential(\n                  (0): ResNetBottleNeckLayer(\n                    (shortcut): ResNetShortCut(\n                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (normalization): DetrFrozenBatchNorm2d()\n                    )\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (1): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (2): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                )\n              )\n              (1): ResNetStage(\n                (layers): Sequential(\n                  (0): ResNetBottleNeckLayer(\n                    (shortcut): ResNetShortCut(\n                      (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                      (normalization): DetrFrozenBatchNorm2d()\n                    )\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (1): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (2): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (3): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                )\n              )\n              (2): ResNetStage(\n                (layers): Sequential(\n                  (0): ResNetBottleNeckLayer(\n                    (shortcut): ResNetShortCut(\n                      (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                      (normalization): DetrFrozenBatchNorm2d()\n                    )\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (1): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (2): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (3): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (4): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (5): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                )\n              )\n              (3): ResNetStage(\n                (layers): Sequential(\n                  (0): ResNetBottleNeckLayer(\n                    (shortcut): ResNetShortCut(\n                      (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                      (normalization): DetrFrozenBatchNorm2d()\n                    )\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (1): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (2): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                )\n              )\n            )\n          )\n        )\n      )\n      (position_embedding): DetrSinePositionEmbedding()\n    )\n    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (query_position_embeddings): Embedding(100, 256)\n    (encoder): DetrEncoder(\n      (layers): ModuleList(\n        (0-5): 6 x DetrEncoderLayer(\n          (self_attn): DetrAttention(\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): DetrDecoder(\n      (layers): ModuleList(\n        (0-5): 6 x DetrDecoderLayer(\n          (self_attn): DetrAttention(\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): DetrAttention(\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (class_labels_classifier): Linear(in_features=256, out_features=92, bias=True)\n  (bbox_predictor): DetrMLPPredictionHead(\n    (layers): ModuleList(\n      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n      (2): Linear(in_features=256, out_features=4, bias=True)\n    )\n  )\n)\n\n\n\nreplace_linear_with_target_and_quantize(model, W8A16LinearLayer, [\"0\", \"1\", \"2\", \"class_labels_classifier\"])\n\n\n### Model after quantization\nmodel\n\nDetrForObjectDetection(\n  (model): DetrModel(\n    (backbone): DetrConvModel(\n      (conv_encoder): DetrConvEncoder(\n        (model): ResNetBackbone(\n          (embedder): ResNetEmbeddings(\n            (embedder): ResNetConvLayer(\n              (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n              (normalization): DetrFrozenBatchNorm2d()\n              (activation): ReLU()\n            )\n            (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n          )\n          (encoder): ResNetEncoder(\n            (stages): ModuleList(\n              (0): ResNetStage(\n                (layers): Sequential(\n                  (0): ResNetBottleNeckLayer(\n                    (shortcut): ResNetShortCut(\n                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (normalization): DetrFrozenBatchNorm2d()\n                    )\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (1): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (2): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                )\n              )\n              (1): ResNetStage(\n                (layers): Sequential(\n                  (0): ResNetBottleNeckLayer(\n                    (shortcut): ResNetShortCut(\n                      (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                      (normalization): DetrFrozenBatchNorm2d()\n                    )\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (1): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (2): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (3): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                )\n              )\n              (2): ResNetStage(\n                (layers): Sequential(\n                  (0): ResNetBottleNeckLayer(\n                    (shortcut): ResNetShortCut(\n                      (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                      (normalization): DetrFrozenBatchNorm2d()\n                    )\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (1): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (2): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (3): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (4): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (5): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                )\n              )\n              (3): ResNetStage(\n                (layers): Sequential(\n                  (0): ResNetBottleNeckLayer(\n                    (shortcut): ResNetShortCut(\n                      (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                      (normalization): DetrFrozenBatchNorm2d()\n                    )\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (1): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                  (2): ResNetBottleNeckLayer(\n                    (shortcut): Identity()\n                    (layer): Sequential(\n                      (0): ResNetConvLayer(\n                        (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (1): ResNetConvLayer(\n                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): ReLU()\n                      )\n                      (2): ResNetConvLayer(\n                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                        (normalization): DetrFrozenBatchNorm2d()\n                        (activation): Identity()\n                      )\n                    )\n                    (activation): ReLU()\n                  )\n                )\n              )\n            )\n          )\n        )\n      )\n      (position_embedding): DetrSinePositionEmbedding()\n    )\n    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (query_position_embeddings): Embedding(100, 256)\n    (encoder): DetrEncoder(\n      (layers): ModuleList(\n        (0-5): 6 x DetrEncoderLayer(\n          (self_attn): DetrAttention(\n            (k_proj): W8A16LinearLayer()\n            (v_proj): W8A16LinearLayer()\n            (q_proj): W8A16LinearLayer()\n            (out_proj): W8A16LinearLayer()\n          )\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): W8A16LinearLayer()\n          (fc2): W8A16LinearLayer()\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): DetrDecoder(\n      (layers): ModuleList(\n        (0-5): 6 x DetrDecoderLayer(\n          (self_attn): DetrAttention(\n            (k_proj): W8A16LinearLayer()\n            (v_proj): W8A16LinearLayer()\n            (q_proj): W8A16LinearLayer()\n            (out_proj): W8A16LinearLayer()\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): DetrAttention(\n            (k_proj): W8A16LinearLayer()\n            (v_proj): W8A16LinearLayer()\n            (q_proj): W8A16LinearLayer()\n            (out_proj): W8A16LinearLayer()\n          )\n          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (fc1): W8A16LinearLayer()\n          (fc2): W8A16LinearLayer()\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (class_labels_classifier): Linear(in_features=256, out_features=92, bias=True)\n  (bbox_predictor): DetrMLPPredictionHead(\n    (layers): ModuleList(\n      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n      (2): Linear(in_features=256, out_features=4, bias=True)\n    )\n  )\n)\n\n\n\nVisualize results after quantization.\n\n\ninputs = processor(images=image, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score &gt; 0.9\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n\n\nplot_results(model, image, results)\n\n\n\n\n\n\n\n\n\nnew_footprint = model.get_memory_footprint()\n\n\nprint(\"Footprint of the model in MBs: \", new_footprint / 1e6)\n\nFootprint of the model in MBs:  114.80384\n\n\n\n### Memory saved\nprint(\"Memory saved in MBs: \", (previous_memory_footprint - new_footprint) / 1e6)\n\nMemory saved in MBs:  51.720192"
  },
  {
    "objectID": "learning/quantization-in-depth/L4_building_quantizer_custom_quantizer.html",
    "href": "learning/quantization-in-depth/L4_building_quantizer_custom_quantizer.html",
    "title": "L4-A - Building your own Quantizer: Custom Build an 8-Bit Quantizer",
    "section": "",
    "text": "In this lesson, you will learn how to compress any model in 8-bit precision."
  },
  {
    "objectID": "learning/quantization-in-depth/L4_building_quantizer_custom_quantizer.html#step-1-class-w8a16linearlayer",
    "href": "learning/quantization-in-depth/L4_building_quantizer_custom_quantizer.html#step-1-class-w8a16linearlayer",
    "title": "L4-A - Building your own Quantizer: Custom Build an 8-Bit Quantizer",
    "section": "Step 1: class W8A16LinearLayer",
    "text": "Step 1: class W8A16LinearLayer\n\nBuild the target class, W8A16LinearLayer(), that will be responsible for quantizing your model.\n\n\n1.1 - w8_a16_forward Function\n\n\n\nW8A16LinearLayer\n                    # 8-bit  # 16-bit         # optional\n* w8_a16_forward -&gt; weights, input,   scales, bias=None\n                    \n\nCast the 8-bit weights to the same data type as the input, “casted weights”,\nkeeping the “casted weights” in the same range as before, [-128, 127]\nNext, \\[(({inputs} \\cdot \\text{``casted weights''}) * {scale}) + {bias}\\]\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nrandom_int8 = torch.randint(-128, 127, (32, 16)).to(torch.int8)\nrandom_hs = torch.randn((1, 16), dtype=torch.bfloat16)\nscales = torch.randn((1, 32), dtype=torch.bfloat16)\nbias = torch.randn((1, 32), dtype=torch.bfloat16)\n\nNote: Since the values are random, what you see in the video might be different than what you will get.\n\nF.linear(random_hs, random_int8.to(random_hs.dtype))\n\ntensor([[-362.0000,  334.0000,  302.0000,  245.0000,  408.0000,  352.0000,\n          -19.1250,  194.0000, -312.0000,  276.0000,  108.5000, -176.0000,\n           74.0000, -201.0000,   77.0000, -584.0000,  434.0000, -149.0000,\n          168.0000, -132.0000,  190.0000, -384.0000,  -49.7500, -382.0000,\n          266.0000, -468.0000,  -78.0000, -406.0000,  288.0000,  -41.2500,\n          260.0000,   98.5000]], dtype=torch.bfloat16)\n\n\n\nF.linear(random_hs, random_int8.to(random_hs.dtype)) * scales\n\ntensor([[-450.0000, -213.0000, -162.0000,   57.0000, -239.0000,  135.0000,\n           22.7500, -392.0000, -428.0000,  111.5000,   14.8125,   -6.0625,\n           73.0000,  140.0000,  -87.0000,  792.0000,  -14.5000, -152.0000,\n           39.7500,   30.7500, -290.0000,  384.0000,   56.7500,  736.0000,\n          191.0000,  172.0000,  -29.2500, -199.0000,   35.0000,  -21.0000,\n         -168.0000,   -5.7500]], dtype=torch.bfloat16)\n\n\n\n(F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales) + bias\n\ntensor([[-450.0000, -213.0000, -163.0000,   57.0000, -236.0000,  136.0000,\n           23.5000, -392.0000, -428.0000,  112.0000,   14.5000,   -7.9688,\n           73.5000,  139.0000,  -86.5000,  792.0000,  -12.5000, -150.0000,\n           38.7500,   31.5000, -288.0000,  382.0000,   57.5000,  736.0000,\n          191.0000,  172.0000,  -28.2500, -199.0000,   35.0000,  -20.7500,\n         -169.0000,   -6.9062]], dtype=torch.bfloat16)\n\n\n\nImplement all this as a function, w8_a16_forward\n\n\ndef w8_a16_forward(weight, input, scales, bias=None):\n\n    casted_weights = weight.to(input.dtype)\n    output = F.linear(input, casted_weights) * scales\n\n    if bias is not None:\n        output = output + bias\n\n    return output\n\n\nprint(\"With bias:\\n\\n\", w8_a16_forward(random_int8, random_hs, scales, bias))\n\nprint(\"\\nWithout bias:\\n\\n\", w8_a16_forward(random_int8, random_hs, scales))\n\nWith bias:\n\n tensor([[-450.0000, -213.0000, -163.0000,   57.0000, -236.0000,  136.0000,\n           23.5000, -392.0000, -428.0000,  112.0000,   14.5000,   -7.9688,\n           73.5000,  139.0000,  -86.5000,  792.0000,  -12.5000, -150.0000,\n           38.7500,   31.5000, -288.0000,  382.0000,   57.5000,  736.0000,\n          191.0000,  172.0000,  -28.2500, -199.0000,   35.0000,  -20.7500,\n         -169.0000,   -6.9062]], dtype=torch.bfloat16)\n\nWithout bias:\n\n tensor([[-450.0000, -213.0000, -162.0000,   57.0000, -239.0000,  135.0000,\n           22.7500, -392.0000, -428.0000,  111.5000,   14.8125,   -6.0625,\n           73.0000,  140.0000,  -87.0000,  792.0000,  -14.5000, -152.0000,\n           39.7500,   30.7500, -290.0000,  384.0000,   56.7500,  736.0000,\n          191.0000,  172.0000,  -29.2500, -199.0000,   35.0000,  -21.0000,\n         -168.0000,   -5.7500]], dtype=torch.bfloat16)\n\n\n\n\n1.2 - init Function of class W8A16LinearLayer\n\nThis is how the init is of PyTorch Linear layer:\n\ndef __init__(self, in_features, out_features, bias=True,\n             device=None, dtype=None)\n\n### running this will result in an error\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n        super().__init__()\n\n        self.int8_weights = nn.Parameter(torch.Tensor([0, 1]).to(dtype=torch.int8))\n \n\ntry:\n\n    W8A16LinearLayer(1, 1)\n\nexcept Exception as error:\n    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")\n\n RuntimeError :  Only Tensors of floating point and complex dtype can require gradients \n\n\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n        super().__init__()\n\n        self.register_buffer(\"int8_weights\", torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n\n        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n\n        if bias:\n            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n\n        else:\n            self.bias = None\n\n\nTest your implementation.\n\n\ndummy_instance = W8A16LinearLayer(16, 32)\n\n\nprint(dummy_instance.int8_weights.shape)\nprint(dummy_instance.scales.shape)\n\ntorch.Size([32, 16])\ntorch.Size([32])\n\n\n\n\n1.3 - forward Function of class W8A16LinearLayer\n\nUse the w8_a16_forward defined earlier (Step 1.1) to define the forward function.\n\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n        super().__init__()\n\n        self.register_buffer(\"int8_weights\", torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n\n        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n\n        if bias:\n            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n\n        else:\n            self.bias = None\n\n    def forward(self, input):\n        return w8_a16_forward(self.int8_weights, input, self.scales, self.bias)\n\n\nmodule = W8A16LinearLayer(16, 32)\ndummy_hidden_states = torch.randn(1, 6, 16)\n\n\nmodule(dummy_hidden_states).shape\n\ntorch.Size([1, 6, 32])\n\n\n\nmodule(dummy_hidden_states).dtype\n\ntorch.float32\n\n\n\n\n1.4 - quantize Function of class W8A16LinearLayer\n\nquantize function will dynamically quantize half-precision weights into torch.int8\n\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n        super().__init__()\n\n        self.register_buffer(\"int8_weights\", torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n\n        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n\n        if bias:\n            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n\n        else:\n            self.bias = None\n\n    def quantize(self, weights):\n        w_fp32 = weights.clone().to(torch.float32)\n\n        scales = w_fp32.abs().max(dim=-1).values / 127\n        scales = scales.to(weights.dtype)\n\n        int8_weights = torch.round(weights / scales.unsqueeze(1)).to(torch.int8)\n\n        self.int8_weights = int8_weights\n        self.scales = scales\n\n    def forward(self, input):\n        return w8_a16_forward(self.int8_weights, input, self.scales, self.bias)\n\n\nmodule = W8A16LinearLayer(4, 8)\n\n\nprint(\"Weights before:\\n\", module.int8_weights)\n\nWeights before:\n tensor([[ -97,   70, -104,   93],\n        [  87,   89,  -89,  105],\n        [  -5,   82,   48,  -62],\n        [ -10,  -93, -124,   34],\n        [ -99,  -71, -121,   38],\n        [   6,   -3,  -35, -127],\n        [   8,  -32,   84,  -20],\n        [ 116,  -18,  101,  -86]], dtype=torch.int8)\n\n\n\nrandom_matrix = torch.randn((4, 8), dtype=torch.bfloat16)\n\n\nmodule.quantize(random_matrix)\n\n\nprint(\"Weights After:\\n\", module.int8_weights)\n\nWeights After:\n tensor([[  85,   15,   46,  127,   52,  -23,  -27,  -75],\n        [  31,   42,  -55,   32,   60, -100,  -50, -127],\n        [-127,   97,   34,  -48,   94,    5,  116,  -70],\n        [ 127,  -94,   12,   -8,  -48, -122, -100,   94]], dtype=torch.int8)\n\n\n\nmodule.scales\n\ntensor([0.0095, 0.0146, 0.0076, 0.0088], dtype=torch.bfloat16)\n\n\n\nmodule.scales.shape\n\ntorch.Size([4])\n\n\n\nmodule.int8_weights.shape\n\ntorch.Size([4, 8])\n\n\n\n### dequantized weights\nmodule.int8_weights * module.scales.unsqueeze(1)\n\ntensor([[ 0.8086,  0.1426,  0.4375,  1.2109,  0.4961, -0.2188, -0.2578, -0.7148],\n        [ 0.4531,  0.6172, -0.8047,  0.4688,  0.8789, -1.4688, -0.7344, -1.8594],\n        [-0.9648,  0.7383,  0.2578, -0.3652,  0.7148,  0.0381,  0.8828, -0.5312],\n        [ 1.1172, -0.8281,  0.1055, -0.0703, -0.4219, -1.0703, -0.8789,  0.8281]],\n       dtype=torch.bfloat16)\n\n\n\n### original weights\nrandom_matrix\n\ntensor([[ 0.8086,  0.1387,  0.4434,  1.2109,  0.4980, -0.2217, -0.2559, -0.7148],\n        [ 0.4570,  0.6211, -0.8008,  0.4668,  0.8789, -1.4688, -0.7305, -1.8594],\n        [-0.9648,  0.7383,  0.2598, -0.3633,  0.7148,  0.0403,  0.8789, -0.5273],\n        [ 1.1172, -0.8320,  0.1074, -0.0747, -0.4219, -1.0703, -0.8789,  0.8242]],\n       dtype=torch.bfloat16)\n\n\n\n(random_matrix - module.int8_weights * module.scales.unsqueeze(1)).abs().mean().item()\n\n0.00194549560546875\n\n\n\n(random_matrix - module.int8_weights * module.scales.unsqueeze(1)).pow(2).mean().sqrt().item()\n\n0.0026702880859375"
  },
  {
    "objectID": "learning/quantization-in-depth/L3_linear_II_symmetric_vs_asymmetric.html",
    "href": "learning/quantization-in-depth/L3_linear_II_symmetric_vs_asymmetric.html",
    "title": "L3-A - Linear Quantization II: Symmetric vs. Asymmetric Mode",
    "section": "",
    "text": "In this lesson, you will learn a different way of performing linear quantization, Symmetric Mode.\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n\ndef quantization_error(tensor, dequantized_tensor):\n    return (dequantized_tensor - tensor).abs().square().mean()\n\n\ndef linear_q_with_scale_and_zero_point(r_tensor, scale, zero_point, dtype=torch.int8):\n    \"\"\"\n    Performs simple linear quantization given\n    the scale and zero-point.\n    \"\"\"\n\n    # scale tensor and add the zero point\n    scaled_and_shifted_tensor = r_tensor / scale + zero_point\n\n    # round the tensor\n    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n\n    # we need to clamp to the min/max value of the specified dtype\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    q_tensor = rounded_tensor.clamp(q_min, q_max).to(dtype)\n    return q_tensor\n\n\ndef linear_dequantization(quantized_tensor, scale, zero_point):\n    \"\"\"\n    Linear de-quantization\n    \"\"\"\n    dequantized_tensor = scale * (quantized_tensor.float() - zero_point)\n\n    return dequantized_tensor\n\n\ndef plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n    \"\"\"\n    Plot a heatmap of tensors using seaborn\n    \"\"\"\n    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n    ax.set_title(title)\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n\n\ndef plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype=torch.int8, n_bits=8):\n    \"\"\"\n    A method that plots 4 matrices, the original tensor, the quantized tensor\n    the de-quantized tensor and the error tensor.\n    \"\"\"\n    # Get a figure of 4 plots\n    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n\n    # Plot the first matrix\n    plot_matrix(original_tensor, axes[0], \"Original Tensor\", cmap=ListedColormap([\"white\"]))\n\n    # Get the quantization range and plot the quantized tensor\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    plot_matrix(\n        quantized_tensor, axes[1], f\"{n_bits}-bit Linear Quantized Tensor\", vmin=q_min, vmax=q_max, cmap=\"coolwarm\"\n    )\n\n    # Plot the de-quantized tensors\n    plot_matrix(dequantized_tensor, axes[2], \"Dequantized Tensor\", cmap=\"coolwarm\")\n\n    # Get the quantization errors\n    q_error_tensor = abs(original_tensor - dequantized_tensor)\n    plot_matrix(q_error_tensor, axes[3], \"Quantization Error Tensor\", cmap=ListedColormap([\"white\"]))\n\n    fig.tight_layout()\n    plt.show()\n\n\n# From the previous lab\ndef get_q_scale_and_zero_point(r_tensor, dtype=torch.int8):\n    \"\"\"\n    Get quantization parameters (scale, zero point)\n    for a floating point tensor\n    \"\"\"\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    r_min, r_max = r_tensor.min().item(), r_tensor.max().item()\n\n    scale = (r_max - r_min) / (q_max - q_min)\n\n    zero_point = q_min - (r_min / scale)\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    if zero_point &lt; q_min or zero_point &gt; q_max:\n        zero_point = q_min\n    else:\n        # round and cast to int\n        zero_point = int(round(zero_point))\n    return scale, zero_point\n\n\ndef linear_quantization(r_tensor, n_bits, dtype=torch.int8):\n    \"\"\"\n    linear quantization\n    \"\"\"\n\n    scale, zero_point = get_q_scale_and_zero_point(r_tensor)\n\n    quantized_tensor = linear_q_with_scale_and_zero_point(r_tensor, scale=scale, zero_point=zero_point, dtype=dtype)\n\n    return quantized_tensor, scale, zero_point\nimport torch"
  },
  {
    "objectID": "learning/quantization-in-depth/L3_linear_II_symmetric_vs_asymmetric.html#linear-quantization-symmetric-mode",
    "href": "learning/quantization-in-depth/L3_linear_II_symmetric_vs_asymmetric.html#linear-quantization-symmetric-mode",
    "title": "L3-A - Linear Quantization II: Symmetric vs. Asymmetric Mode",
    "section": "Linear Quantization: Symmetric Mode",
    "text": "Linear Quantization: Symmetric Mode\n\nImplement a function which returns the scale for Linear Quantization in Symmetric Mode.\n\n\ndef get_q_scale_symmetric(tensor, dtype=torch.int8):\n    r_max = tensor.abs().max().item()\n    q_max = torch.iinfo(dtype).max\n\n    # return the scale\n    return r_max / q_max\n\n\n### test the implementation on a 4x4 matrix\ntest_tensor = torch.randn((4, 4))\nprint(test_tensor.numpy())\n\n[[-2.82923788e-01  3.28040361e-01 -5.59608400e-01  7.00194418e-01]\n [ 2.13929844e+00  5.07355094e-01 -8.34084034e-01 -1.13720536e-01]\n [ 1.13845253e+00  6.10361099e-01  2.53565222e-01  3.08846235e-01]\n [-5.32116625e-04 -1.77835941e-01 -5.45356631e-01 -6.64512992e-01]]\n\n\nNote: Since the values are random, what you see in the video might be different than what you will get.\n\nget_q_scale_symmetric(test_tensor)\n\n0.016844869598628968\n\n\n\nPerform Linear Quantization in Symmetric Mode.\nlinear_q_with_scale_and_zero_point is the same function you implemented in the previous lesson.\n\n\ndef linear_q_symmetric(tensor, dtype=torch.int8):\n    scale = get_q_scale_symmetric(tensor)\n\n    quantized_tensor = linear_q_with_scale_and_zero_point(\n        tensor,\n        scale=scale,\n        # in symmetric quantization zero point is = 0\n        zero_point=0,\n        dtype=dtype,\n    )\n\n    return quantized_tensor, scale\n\n\nquantized_tensor, scale = linear_q_symmetric(test_tensor)\nquantized_tensor, scale\n\n(tensor([[-17,  19, -33,  42],\n         [127,  30, -50,  -7],\n         [ 68,  36,  15,  18],\n         [  0, -11, -32, -39]], dtype=torch.int8),\n 0.016844869598628968)\n\n\n\nDequantization\n\nPerform Dequantization\nPlot the Quantization error.\nlinear_dequantization is the same function you implemented in the previous lesson.\n\n\ndequantized_tensor = linear_dequantization(quantized_tensor, scale, 0)\n\n\nplot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)\n\n\n\n\n\n\n\n\n\nprint(f\"\"\"Quantization Error : \\\n{quantization_error(test_tensor, dequantized_tensor)}\"\"\")\n\nQuantization Error : 3.0066523322602734e-05"
  },
  {
    "objectID": "learning/quantization-in-depth/L3_linear_II_per_channel.html",
    "href": "learning/quantization-in-depth/L3_linear_II_per_channel.html",
    "title": "L3-C - Linear Quantization II: Per Channel Quantization",
    "section": "",
    "text": "In this lesson, you will continue to learn about different granularities of performing linear quantization. You will cover per channel in this notebook.\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport torch\n\n\ndef quantization_error(tensor, dequantized_tensor):\n    return (dequantized_tensor - tensor).abs().square().mean()\n\n\ndef linear_q_with_scale_and_zero_point(r_tensor, scale, zero_point, dtype=torch.int8):\n    \"\"\"\n    Performs simple linear quantization given\n    the scale and zero-point.\n    \"\"\"\n\n    # scale tensor and add the zero point\n    scaled_and_shifted_tensor = r_tensor / scale + zero_point\n\n    # round the tensor\n    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n\n    # we need to clamp to the min/max value of the specified dtype\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    q_tensor = rounded_tensor.clamp(q_min, q_max).to(dtype)\n    return q_tensor\n\n\ndef linear_dequantization(quantized_tensor, scale, zero_point):\n    \"\"\"\n    Linear de-quantization\n    \"\"\"\n    dequantized_tensor = scale * (quantized_tensor.float() - zero_point)\n\n    return dequantized_tensor\n\n\ndef plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n    \"\"\"\n    Plot a heatmap of tensors using seaborn\n    \"\"\"\n    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n    ax.set_title(title)\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n\n\ndef plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype=torch.int8, n_bits=8):\n    \"\"\"\n    A method that plots 4 matrices, the original tensor, the quantized tensor\n    the de-quantized tensor and the error tensor.\n    \"\"\"\n    # Get a figure of 4 plots\n    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n\n    # Plot the first matrix\n    plot_matrix(original_tensor, axes[0], \"Original Tensor\", cmap=ListedColormap([\"white\"]))\n\n    # Get the quantization range and plot the quantized tensor\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    plot_matrix(\n        quantized_tensor, axes[1], f\"{n_bits}-bit Linear Quantized Tensor\", vmin=q_min, vmax=q_max, cmap=\"coolwarm\"\n    )\n\n    # Plot the de-quantized tensors\n    plot_matrix(dequantized_tensor, axes[2], \"Dequantized Tensor\", cmap=\"coolwarm\")\n\n    # Get the quantization errors\n    q_error_tensor = abs(original_tensor - dequantized_tensor)\n    plot_matrix(q_error_tensor, axes[3], \"Quantization Error Tensor\", cmap=ListedColormap([\"white\"]))\n\n    fig.tight_layout()\n    plt.show()\n\n\ndef get_q_scale_and_zero_point(r_tensor, dtype=torch.int8):\n    \"\"\"\n    Get quantization parameters (scale, zero point)\n    for a floating point tensor\n    \"\"\"\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    r_min, r_max = r_tensor.min().item(), r_tensor.max().item()\n\n    scale = (r_max - r_min) / (q_max - q_min)\n\n    zero_point = q_min - (r_min / scale)\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    if zero_point &lt; q_min or zero_point &gt; q_max:\n        zero_point = q_min\n    else:\n        # round and cast to int\n        zero_point = int(round(zero_point))\n    return scale, zero_point\n\n\n############# From the previous lesson(s) of \"Linear Quantization II\"\ndef get_q_scale_symmetric(tensor, dtype=torch.int8):\n    r_max = tensor.abs().max().item()\n    q_max = torch.iinfo(dtype).max\n\n    # return the scale\n    return r_max / q_max\n\n\n###################################\nimport torch"
  },
  {
    "objectID": "learning/quantization-in-depth/L3_linear_II_per_channel.html#different-granularities-for-quantization",
    "href": "learning/quantization-in-depth/L3_linear_II_per_channel.html#different-granularities-for-quantization",
    "title": "L3-C - Linear Quantization II: Per Channel Quantization",
    "section": "Different Granularities for Quantization",
    "text": "Different Granularities for Quantization\n\nFor simplicity, you’ll perform these using Symmetric mode.\n\n\nPer Channel\n\nImplement Per Channel Symmetric Quantization\ndim parameter decides if it needs to be along the rows or columns\n\n\ndef linear_q_symmetric_per_channel(tensor, dim, dtype=torch.int8):\n\n    # 1 - dim means \"the other dimension\"\n    rs_max = tensor.abs().max(dim=1 - dim, keepdim=True).values\n    q_max = torch.iinfo(dtype).max\n\n    # return the scale\n    scales = rs_max / q_max\n\n    quantized_tensor = linear_q_with_scale_and_zero_point(tensor, scale=scales, zero_point=0, dtype=dtype)\n\n    return quantized_tensor, scales\n\n\ntest_tensor = torch.tensor([[191.6, -13.5, 728.6], [92.14, 295.5, -184], [0, 684.6, 245.5]])\n\n\ndim = 0, along the rows\ndim = 1, along the columns\n\n\ndim = 0\noutput_dim = test_tensor.shape[dim]\noutput_dim\n\n3\n\n\n\nIterate through each row to calculate its scale.\n\n\nscale = torch.zeros(output_dim)\n\nfor index in range(output_dim):\n    sub_tensor = test_tensor.select(dim, index)\n    # print(sub_tensor)\n    scale[index] = get_q_scale_symmetric(sub_tensor)\n\nscale = scale[:, None]\n\n# copy to be used later\ncopy_scale = scale\n\nscale\n\ntensor([[5.7370],\n        [2.3268],\n        [5.3906]])\n\n\n\nlinear_q_symmetric_per_channel(test_tensor, dim=0, dtype=torch.int8)[1]\n\ntensor([[5.7370],\n        [2.3268],\n        [5.3906]])\n\n\n\nUnderstanding tensor by tensor division using view function\n\nm = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nm\n\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\n\n\ns = torch.tensor([1, 5, 10])\nprint(s)\nprint(s.shape)\nprint(s.view(1, 3).shape)\n\n# alternate way\nprint(s.view(1, -1).shape)\nprint(s.view(-1, 1).shape)\n\ntensor([ 1,  5, 10])\ntorch.Size([3])\ntorch.Size([1, 3])\ntorch.Size([1, 3])\ntorch.Size([3, 1])\n\n\n\nAlong the row division\n\nscale = torch.tensor([[1], [5], [10]])\nscale.shape\n\ntorch.Size([3, 1])\n\n\n\nm / scale\n\ntensor([[1.0000, 2.0000, 3.0000],\n        [0.8000, 1.0000, 1.2000],\n        [0.7000, 0.8000, 0.9000]])\n\n\n\n\nAlong the column division\n\nscale = torch.tensor([[1, 5, 10]])\nscale.shape\n\ntorch.Size([1, 3])\n\n\n\nm / scale\n\ntensor([[1.0000, 0.4000, 0.3000],\n        [4.0000, 1.0000, 0.6000],\n        [7.0000, 1.6000, 0.9000]])\n\n\n\n\n\nComing back to quantizing the tensor\n\n# the scale you got earlier\nscale = copy_scale\n\nscale, scale.shape\n\n(tensor([[5.7370],\n         [2.3268],\n         [5.3906]]),\n torch.Size([3, 1]))\n\n\n\nquantized_tensor = linear_q_with_scale_and_zero_point(test_tensor, scale=scale, zero_point=0)\nquantized_tensor\n\ntensor([[ 33,  -2, 127],\n        [ 40, 127, -79],\n        [  0, 127,  46]], dtype=torch.int8)\n\n\n\nNow, put all this in linear_q_symmetric_per_channel function defined earlier.\n\n\ndef linear_q_symmetric_per_channel_2(r_tensor, dim, dtype=torch.int8):\n\n    output_dim = r_tensor.shape[dim]\n    # store the scales\n    scale = torch.zeros(output_dim)\n\n    for index in range(output_dim):\n        sub_tensor = r_tensor.select(dim, index)\n        scale[index] = get_q_scale_symmetric(sub_tensor, dtype=dtype)\n\n    # reshape the scale\n    scale_shape = [1] * r_tensor.dim()\n    scale_shape[dim] = -1\n    scale = scale.view(scale_shape)\n    quantized_tensor = linear_q_with_scale_and_zero_point(r_tensor, scale=scale, zero_point=0, dtype=dtype)\n\n    return quantized_tensor, scale\n\n\ntest_tensor = torch.tensor([[191.6, -13.5, 728.6], [92.14, 295.5, -184], [0, 684.6, 245.5]])\n\n\n### along the rows (dim = 0)\nquantized_tensor_0, scale_0 = linear_q_symmetric_per_channel_2(test_tensor, dim=0)\nquantized_tensor_0_2, scale_0_2 = linear_q_symmetric_per_channel(test_tensor, dim=0)\nassert torch.allclose(quantized_tensor_0, quantized_tensor_0_2)\nassert torch.allclose(scale_0, scale_0_2)\n\n### along the columns (dim = 1)\nquantized_tensor_1, scale_1 = linear_q_symmetric_per_channel_2(test_tensor, dim=1)\nquantized_tensor_1_2, scale_1_2 = linear_q_symmetric_per_channel(test_tensor, dim=1)\nassert torch.allclose(quantized_tensor_1, quantized_tensor_1_2)\nassert torch.allclose(scale_1, scale_1_2)\n\n\ntest_tensor_large = torch.randn(1000, 1000)\n\n### along the rows (dim = 0)\n%timeit linear_q_symmetric_per_channel_2(test_tensor_large, dim=0)\n%timeit linear_q_symmetric_per_channel(test_tensor_large, dim=0)\n\n### along the columns (dim = 1)\n%timeit linear_q_symmetric_per_channel_2(test_tensor_large, dim=1)\n%timeit linear_q_symmetric_per_channel(test_tensor_large, dim=1)\n\n5.96 ms ± 101 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n1.32 ms ± 58.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n6.29 ms ± 19.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n1.4 ms ± 74.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nPlot the quantization error for along the rows.\n\n\ndequantized_tensor_0 = linear_dequantization(quantized_tensor_0, scale_0, 0)\n\nplot_quantization_errors(test_tensor, quantized_tensor_0, dequantized_tensor_0)\n\n\n\n\n\n\n\n\n\nprint(f\"Quantization Error: {quantization_error(test_tensor, dequantized_tensor_0)}\")\n\nQuantization Error: 1.8084441423416138\n\n\n\nPlot the quantization error for along the columns.\n\n\ndequantized_tensor_1 = linear_dequantization(quantized_tensor_1, scale_1, 0)\n\nplot_quantization_errors(test_tensor, quantized_tensor_1, dequantized_tensor_1, n_bits=8)\n\nprint(f\"Quantization Error: {quantization_error(test_tensor, dequantized_tensor_1)}\")\n\n\n\n\n\n\n\n\nQuantization Error: 1.0781488418579102"
  },
  {
    "objectID": "learning/quantization-in-depth/L3_linear_II_per_tensor.html",
    "href": "learning/quantization-in-depth/L3_linear_II_per_tensor.html",
    "title": "L3-B - Linear Quantization II: Finer Granularity for more Precision",
    "section": "",
    "text": "In this lesson, you will learn about different granularities of performing linear quantization. You will cover per tensor in this notebook.\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport torch\n\n\ndef quantization_error(tensor, dequantized_tensor):\n    return (dequantized_tensor - tensor).abs().square().mean()\n\n\ndef linear_q_with_scale_and_zero_point(r_tensor, scale, zero_point, dtype=torch.int8):\n    \"\"\"\n    Performs simple linear quantization given\n    the scale and zero-point.\n    \"\"\"\n\n    # scale tensor and add the zero point\n    scaled_and_shifted_tensor = r_tensor / scale + zero_point\n\n    # round the tensor\n    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n\n    # we need to clamp to the min/max value of the specified dtype\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    q_tensor = rounded_tensor.clamp(q_min, q_max).to(dtype)\n    return q_tensor\n\n\ndef linear_dequantization(quantized_tensor, scale, zero_point):\n    \"\"\"\n    Linear de-quantization\n    \"\"\"\n    dequantized_tensor = scale * (quantized_tensor.float() - zero_point)\n\n    return dequantized_tensor\n\n\ndef plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n    \"\"\"\n    Plot a heatmap of tensors using seaborn\n    \"\"\"\n    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n    ax.set_title(title)\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n\n\ndef plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype=torch.int8, n_bits=8):\n    \"\"\"\n    A method that plots 4 matrices, the original tensor, the quantized tensor\n    the de-quantized tensor and the error tensor.\n    \"\"\"\n    # Get a figure of 4 plots\n    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n\n    # Plot the first matrix\n    plot_matrix(original_tensor, axes[0], \"Original Tensor\", cmap=ListedColormap([\"white\"]))\n\n    # Get the quantization range and plot the quantized tensor\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    plot_matrix(\n        quantized_tensor, axes[1], f\"{n_bits}-bit Linear Quantized Tensor\", vmin=q_min, vmax=q_max, cmap=\"coolwarm\"\n    )\n\n    # Plot the de-quantized tensors\n    plot_matrix(dequantized_tensor, axes[2], \"Dequantized Tensor\", cmap=\"coolwarm\")\n\n    # Get the quantization errors\n    q_error_tensor = abs(original_tensor - dequantized_tensor)\n    plot_matrix(q_error_tensor, axes[3], \"Quantization Error Tensor\", cmap=ListedColormap([\"white\"]))\n\n    fig.tight_layout()\n    plt.show()\n\n\ndef get_q_scale_and_zero_point(r_tensor, dtype=torch.int8):\n    \"\"\"\n    Get quantization parameters (scale, zero point)\n    for a floating point tensor\n    \"\"\"\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    r_min, r_max = r_tensor.min().item(), r_tensor.max().item()\n\n    scale = (r_max - r_min) / (q_max - q_min)\n\n    zero_point = q_min - (r_min / scale)\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    if zero_point &lt; q_min or zero_point &gt; q_max:\n        zero_point = q_min\n    else:\n        # round and cast to int\n        zero_point = int(round(zero_point))\n    return scale, zero_point\n\n\ndef linear_quantization(r_tensor, n_bits, dtype=torch.int8):\n    \"\"\"\n    linear quantization\n    \"\"\"\n\n    scale, zero_point = get_q_scale_and_zero_point(r_tensor)\n\n    quantized_tensor = linear_q_with_scale_and_zero_point(r_tensor, scale=scale, zero_point=zero_point, dtype=dtype)\n\n    return quantized_tensor, scale, zero_point\n\n\n############# From the previous lesson(s) of \"Linear Quantization II\"\ndef get_q_scale_symmetric(tensor, dtype=torch.int8):\n    r_max = tensor.abs().max().item()\n    q_max = torch.iinfo(dtype).max\n\n    # return the scale\n    return r_max / q_max\n\n\ndef linear_q_symmetric(tensor, dtype=torch.int8):\n    scale = get_q_scale_symmetric(tensor)\n\n    quantized_tensor = linear_q_with_scale_and_zero_point(\n        tensor,\n        scale=scale,\n        # in symmetric quantization zero point is = 0\n        zero_point=0,\n        dtype=dtype,\n    )\n\n    return quantized_tensor, scale\n\n\n###################################\nimport torch"
  },
  {
    "objectID": "learning/quantization-in-depth/L3_linear_II_per_tensor.html#different-granularities-for-quantization",
    "href": "learning/quantization-in-depth/L3_linear_II_per_tensor.html#different-granularities-for-quantization",
    "title": "L3-B - Linear Quantization II: Finer Granularity for more Precision",
    "section": "Different Granularities for Quantization",
    "text": "Different Granularities for Quantization\n\nFor simplicity, you’ll perform these using Symmetric mode.\n\n\nPer Tensor\n\nPerform Per Tensor Symmetric Quantization.\n\n\n# test tensor\ntest_tensor = torch.tensor([[191.6, -13.5, 728.6], [92.14, 295.5, -184], [0, 684.6, 245.5]])\n\n\nquantized_tensor, scale = linear_q_symmetric(test_tensor)\n\n\ndequantized_tensor = linear_dequantization(quantized_tensor, scale, 0)\n\n\nplot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)\n\n\n\n\n\n\n\n\n\nprint(f\"Quantization Error : {quantization_error(test_tensor, dequantized_tensor)}\")\n\nQuantization Error : 2.5091912746429443"
  },
  {
    "objectID": "learning/quantization-in-depth/L3_linear_II_per_group.html",
    "href": "learning/quantization-in-depth/L3_linear_II_per_group.html",
    "title": "L3-D - Linear Quantization II: Per Group Quantization",
    "section": "",
    "text": "In this lesson, you will continue to learn about different granularities of performing linear quantization. You will cover per group in this notebook.\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport torch\n\n\ndef quantization_error(tensor, dequantized_tensor):\n    return (dequantized_tensor - tensor).abs().square().mean()\n\n\ndef linear_q_with_scale_and_zero_point(r_tensor, scale, zero_point, dtype=torch.int8):\n    \"\"\"\n    Performs simple linear quantization given\n    the scale and zero-point.\n    \"\"\"\n\n    # scale tensor and add the zero point\n    scaled_and_shifted_tensor = r_tensor / scale + zero_point\n\n    # round the tensor\n    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n\n    # we need to clamp to the min/max value of the specified dtype\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    q_tensor = rounded_tensor.clamp(q_min, q_max).to(dtype)\n    return q_tensor\n\n\ndef linear_dequantization(quantized_tensor, scale, zero_point):\n    \"\"\"\n    Linear de-quantization\n    \"\"\"\n    dequantized_tensor = scale * (quantized_tensor.float() - zero_point)\n\n    return dequantized_tensor\n\n\ndef plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n    \"\"\"\n    Plot a heatmap of tensors using seaborn\n    \"\"\"\n    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n    ax.set_title(title)\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n\n\ndef plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype=torch.int8, n_bits=8):\n    \"\"\"\n    A method that plots 4 matrices, the original tensor, the quantized tensor\n    the de-quantized tensor and the error tensor.\n    \"\"\"\n    # Get a figure of 4 plots\n    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n\n    # Plot the first matrix\n    plot_matrix(original_tensor, axes[0], \"Original Tensor\", cmap=ListedColormap([\"white\"]))\n\n    # Get the quantization range and plot the quantized tensor\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    plot_matrix(\n        quantized_tensor, axes[1], f\"{n_bits}-bit Linear Quantized Tensor\", vmin=q_min, vmax=q_max, cmap=\"coolwarm\"\n    )\n\n    # Plot the de-quantized tensors\n    plot_matrix(dequantized_tensor, axes[2], \"Dequantized Tensor\", cmap=\"coolwarm\")\n\n    # Get the quantization errors\n    q_error_tensor = abs(original_tensor - dequantized_tensor)\n    plot_matrix(q_error_tensor, axes[3], \"Quantization Error Tensor\", cmap=ListedColormap([\"white\"]))\n\n    fig.tight_layout()\n    plt.show()\n\n\ndef get_q_scale_and_zero_point(r_tensor, dtype=torch.int8):\n    \"\"\"\n    Get quantization parameters (scale, zero point)\n    for a floating point tensor\n    \"\"\"\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    r_min, r_max = r_tensor.min().item(), r_tensor.max().item()\n\n    scale = (r_max - r_min) / (q_max - q_min)\n\n    zero_point = q_min - (r_min / scale)\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    if zero_point &lt; q_min or zero_point &gt; q_max:\n        zero_point = q_min\n    else:\n        # round and cast to int\n        zero_point = int(round(zero_point))\n    return scale, zero_point\n\n\ndef linear_quantization(r_tensor, dtype=torch.int8):\n    \"\"\"\n    linear quantization\n    \"\"\"\n\n    scale, zero_point = get_q_scale_and_zero_point(r_tensor)\n\n    quantized_tensor = linear_q_with_scale_and_zero_point(r_tensor, scale=scale, zero_point=zero_point, dtype=dtype)\n\n    return quantized_tensor, scale, zero_point\n\n\n############# From the previous lesson(s) of \"Linear Quantization II\"\ndef linear_q_symmetric_per_channel(r_tensor, dim, dtype=torch.int8):\n\n    output_dim = r_tensor.shape[dim]\n    # store the scales\n    scale = torch.zeros(output_dim)\n\n    for index in range(output_dim):\n        sub_tensor = r_tensor.select(dim, index)\n        scale[index] = get_q_scale_symmetric(sub_tensor, dtype=dtype)\n\n    # reshape the scale\n    scale_shape = [1] * r_tensor.dim()\n    scale_shape[dim] = -1\n    scale = scale.view(scale_shape)\n    quantized_tensor = linear_q_with_scale_and_zero_point(r_tensor, scale=scale, zero_point=0, dtype=dtype)\n\n    return quantized_tensor, scale\n\n\ndef get_q_scale_symmetric(tensor, dtype=torch.int8):\n    r_max = tensor.abs().max().item()\n    q_max = torch.iinfo(dtype).max\n\n    # return the scale\n    return r_max / q_max\n\n\n###################################\nimport torch"
  },
  {
    "objectID": "learning/quantization-in-depth/L3_linear_II_per_group.html#different-granularities-for-quantization",
    "href": "learning/quantization-in-depth/L3_linear_II_per_group.html#different-granularities-for-quantization",
    "title": "L3-D - Linear Quantization II: Per Group Quantization",
    "section": "Different Granularities for Quantization",
    "text": "Different Granularities for Quantization\n\nFor simplicity, you’ll perform these using Symmetric mode.\n\n\nPer Group\n\nFor simplicity, you’ll quantize a 2D tensor along the rows.\n\n\ndef linear_q_symmetric_per_group(tensor, group_size, dtype=torch.int8):\n\n    t_shape = tensor.shape\n    assert t_shape[1] % group_size == 0\n    assert tensor.dim() == 2\n\n    tensor = tensor.view(-1, group_size)\n\n    quantized_tensor, scale = linear_q_symmetric_per_channel(tensor, dim=0, dtype=dtype)\n\n    quantized_tensor = quantized_tensor.view(t_shape)\n\n    return quantized_tensor, scale\n\n\ndef linear_dequantization_per_group(quantized_tensor, scale, group_size):\n    q_shape = quantized_tensor.shape\n\n    quantized_tensor = quantized_tensor.view(-1, group_size)\n\n    dequantized_tensor = linear_dequantization(quantized_tensor, scale, 0)\n\n    dequantized_tensor = dequantized_tensor.view(q_shape)\n\n    return dequantized_tensor\n\n\ntest_tensor = torch.rand((6, 6))\n\nNote: Since the values are random, what you see in the video might be different than what you will get.\n\ngroup_size = 3\n\nquantized_tensor, scale = linear_q_symmetric_per_group(test_tensor, group_size=group_size)\n\ndequantized_tensor = linear_dequantization_per_group(quantized_tensor, scale, group_size=group_size)\n\nplot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)\n\n\n\n\n\n\n\n\n\nprint(f\"Quantization Error : {quantization_error(test_tensor, dequantized_tensor)}\")\n\nQuantization Error : 1.716572455734422e-06"
  },
  {
    "objectID": "learning/quantization-fundamentals/L4_quantization_theory.html",
    "href": "learning/quantization-fundamentals/L4_quantization_theory.html",
    "title": "Lesson 4: Quantization Theory",
    "section": "",
    "text": "In this lab, you will perform Linear Quantization."
  },
  {
    "objectID": "learning/quantization-fundamentals/L4_quantization_theory.html#t5-flan",
    "href": "learning/quantization-fundamentals/L4_quantization_theory.html#t5-flan",
    "title": "Lesson 4: Quantization Theory",
    "section": "T5-FLAN",
    "text": "T5-FLAN\n\nPlease note that due to hardware memory constraints, and in order to offer this course for free to everyone, the code you’ll run here is for the T5-FLAN model instead of the EleutherAI AI Pythia model.\n\nThank you for your understanding! 🤗\n\n\nWithout Quantization\n\nmodel_name = \"google/flan-t5-small\"\n# model_name = \"EleutherAI/pythia-410m\"\n\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n# print(model.gpt_neox)\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n\nprint(f\"{sum(p.numel() for p in model.parameters()):,}\")\n\n76,961,152\n\n\n\ninput_text = \"Hello, my name is \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\nannie scott\n\n\n/Users/augustasm/miniconda3/envs/website/lib/python3.12/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n\n\n\nfrom helper import compute_module_sizes\n\nmodule_sizes = compute_module_sizes(model)\nprint(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n\nThe model size is 0.307844608 GB"
  },
  {
    "objectID": "learning/quantization-fundamentals/L4_quantization_theory.html#quantize-the-model-8-bit-precision",
    "href": "learning/quantization-fundamentals/L4_quantization_theory.html#quantize-the-model-8-bit-precision",
    "title": "Lesson 4: Quantization Theory",
    "section": "Quantize the model (8-bit precision)",
    "text": "Quantize the model (8-bit precision)\n\nfrom quanto import quantize, freeze\nimport torch\n\n\nquantize(model, weights=torch.int8, activations=None)\n\n\nprint(model)\n\nT5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): QLinear(in_features=512, out_features=384, bias=False)\n              (k): QLinear(in_features=512, out_features=384, bias=False)\n              (v): QLinear(in_features=512, out_features=384, bias=False)\n              (o): QLinear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): QLinear(in_features=512, out_features=384, bias=False)\n              (k): QLinear(in_features=512, out_features=384, bias=False)\n              (v): QLinear(in_features=512, out_features=384, bias=False)\n              (o): QLinear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): QLinear(in_features=512, out_features=384, bias=False)\n              (k): QLinear(in_features=512, out_features=384, bias=False)\n              (v): QLinear(in_features=512, out_features=384, bias=False)\n              (o): QLinear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): QLinear(in_features=512, out_features=384, bias=False)\n              (k): QLinear(in_features=512, out_features=384, bias=False)\n              (v): QLinear(in_features=512, out_features=384, bias=False)\n              (o): QLinear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): QLinear(in_features=512, out_features=384, bias=False)\n              (k): QLinear(in_features=512, out_features=384, bias=False)\n              (v): QLinear(in_features=512, out_features=384, bias=False)\n              (o): QLinear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): QLinear(in_features=512, out_features=384, bias=False)\n              (k): QLinear(in_features=512, out_features=384, bias=False)\n              (v): QLinear(in_features=512, out_features=384, bias=False)\n              (o): QLinear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): QLinear(in_features=512, out_features=32128, bias=False)\n)\n\n\n\nFreeze the model\n\nThis step takes a bit of memory, and so for the Pythia model that is shown in the lecture video, it will not run in the classroom.\nThis will work fine with the smaller T5-Flan model.\n\n\nfreeze(model)\n\n\nmodule_sizes = compute_module_sizes(model)\nprint(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n\nThe model size is 0.12682868 GB\n\n\n\n# print(model.gpt_neox)\n# print(model.gpt_neox.layers[0].attention.dense.weight)\n\n\n\nTry running inference on the quantized model\n\ninput_text = \"Hello, my name is \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\nannie scott"
  },
  {
    "objectID": "learning/quantization-fundamentals/L4_quantization_theory.html#note-quantizing-the-model-used-in-the-lecture-video-will-not-work-due-to-classroom-hardware-limitations.",
    "href": "learning/quantization-fundamentals/L4_quantization_theory.html#note-quantizing-the-model-used-in-the-lecture-video-will-not-work-due-to-classroom-hardware-limitations.",
    "title": "Lesson 4: Quantization Theory",
    "section": "Note: Quantizing the model used in the lecture video will not work due to classroom hardware limitations.",
    "text": "Note: Quantizing the model used in the lecture video will not work due to classroom hardware limitations.\n\nHere is the code that Marc, the instructor is walking through.\n\nIt will likely run on your local computer if you have 8GB of memory, which is usually the minimum for personal computers.\n\nTo run locally, you can download the notebook and the helper.py file by clicking on the “Jupyter icon” at the top of the notebook and navigating the file directory of this classroom. Also download the requirements.txt to install all the required libraries.\n\n\n\nWithout Quantization\n\nLoad EleutherAI/pythia-410m model and tokenizer.\n\nfrom transformers import AutoModelForCausalLM\nmodel_name = \"EleutherAI/pythia-410m\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\nprint(model.gpt_neox)\n\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nWrite a start of a (text) sentence which you’d like the model to complete.\n\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=10)\noutputs\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\nCompute the model’s size using the helper function, compute_module_sizes.\n\nfrom helper import compute_module_sizes\nmodule_sizes = compute_module_sizes(model)\nprint(f\"The model size is {module_sizes[''] * 1e-9} GB\")\nprint(model.gpt_neox.layers[0].attention.dense.weight)\nNote: The weights are in fp32.\n\n\n8-bit Quantization\nfrom quanto import quantize, freeze\nimport torch\n\nquantize(model, weights=torch.int8, activations=None)\n# after performing quantization\nprint(model.gpt_neox)\nprint(model.gpt_neox.layers[0].attention.dense.weight)\n\nThe “freeze” function requires more memory than is available in this classroom.\nThis code will run on a machine that has 8GB of memory, and so it will likely work if you run this code on your local machine.\n\n# freeze the model\nfreeze(model)\nprint(model.gpt_neox.layers[0].attention.dense.weight)\n\n# get model size after quantization\nmodule_sizes = compute_module_sizes(model)\nprint(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n\n# run inference after quantizing the model\noutputs = model.generate(**inputs, max_new_tokens=10)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\nComparing “linear quantization” to “downcasting”\nTo recap the difference between the “linear quantization” method in this lesson with the “downcasting” method in the previous lesson:\n\nWhen downcasting a model, you convert the model’s parameters to a more compact data type (bfloat16). During inference, the model performs its calculations in this data type, and its activations are in this data type. Downcasting may work with the bfloat16 data type, but the model performance will likely degrade with any smaller data type, and won’t work if you convert to an integer data type (like the int8 in this lesson).\nIn this lesson, you used another quantization method, “linear quantization”, which enables the quantized model to maintain performance much closer to the original model by converting from the compressed data type back to the original FP32 data type during inference. So when the model makes a prediction, it is performing the matrix multiplications in FP32, and the activations are in FP32. This enables you to quantize the model in data types smaller than bfloat16, such as int8, in this example.\n\n\n\nThis is just the beginning…\n\nThis course is intended to be a beginner-friendly introduction to the field of quantization. 🐣\nIf you’d like to learn more about quantization, please stay tuned for another Hugging Face short course that goes into more depth on this topic (launching in a few weeks!) 🤗"
  },
  {
    "objectID": "learning/quantization-fundamentals/L4_quantization_theory.html#did-you-like-this-course",
    "href": "learning/quantization-fundamentals/L4_quantization_theory.html#did-you-like-this-course",
    "title": "Lesson 4: Quantization Theory",
    "section": "Did you like this course?",
    "text": "Did you like this course?\n\nIf you liked this course, could you consider giving a rating and share what you liked? 💕\nIf you did not like this course, could you also please share what you think could have made it better? 🙏\n\n\nA note about the “Course Review” page.\nThe rating options are from 0 to 10. - A score of 9 or 10 means you like the course.🤗 - A score of 7 or 8 means you feel neutral about the course (neither like nor dislike).🙄 - A score of 0,1,2,3,4,5 or 6 all mean that you do not like the course. 😭 - Whether you give a 0 or a 6, these are all defined as “detractors” according to the standard measurement called “Net Promoter Score”. 🧐"
  },
  {
    "objectID": "learning/quantization-fundamentals/L2_data_types.html",
    "href": "learning/quantization-fundamentals/L2_data_types.html",
    "title": "Lesson 2: Data Types and Sizes",
    "section": "",
    "text": "In this lab, you will learn about the common data types used to store the parameters of machine learning models.\nimport torch"
  },
  {
    "objectID": "learning/quantization-fundamentals/L2_data_types.html#my-questions",
    "href": "learning/quantization-fundamentals/L2_data_types.html#my-questions",
    "title": "Lesson 2: Data Types and Sizes",
    "section": "My questions",
    "text": "My questions\n\nWhat does “exponent” and “fraction” mean and how exactly do they represent ranges and precision?\nHow to read torch.finfo?\nMixed-precision training?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Tokenizers deep dive\n\n\n\n\n\n\ndeep learning\n\n\nLLMs\n\n\ntokenization\n\n\n\nDeep dive into understanding and building tokenizers with an end goal of replicating the LLaMA 3 tokenizer.\n\n\n\n\n\nMay 3, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nGood tokenizers is all you need\n\n\n\n\n\n\ndeep learning\n\n\nLLMs\n\n\ntokenization\n\n\n\nTokenization is (at least partially) the cause of a lot of the problems with large language models.\n\n\n\n\n\nApr 19, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising large language model embeddings\n\n\n\n\n\n\ndeep learning\n\n\nLLMs\n\n\nvisualisation\n\n\n\nDimensionality reduction techniques reveal interesting structure in the embeddings learnt by large language models.\n\n\n\n\n\nApr 4, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nEliciting latent knowledge from language reward models\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\nLLMs\n\n\ncambridge\n\n\n\nBlog post about my master’s research project at the University of Cambridge.\n\n\n\n\n\nOct 4, 2023\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical solutions to the Navier-Stokes equations\n\n\n\n\n\n\nscientific computing\n\n\n\nBlog post about my undergraduate project at the University of Manchester.\n\n\n\n\n\nOct 1, 2023\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nBasic panorama stitching\n\n\n\n\n\n\nmachine learning\n\n\ncomputer vision\n\n\n\nA look at homographies and how they can be used to stitch photos into a panorama.\n\n\n\n\n\nJul 9, 2022\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing optimisation algorithms to optimise parameters of linear functions\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\noptimization\n\n\n\nA sandbox notebook which I used to play around with optimizers.\n\n\n\n\n\nMay 1, 2021\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-output classification for captcha recognition using fastai\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\nclassification\n\n\nfastai\n\n\nvision\n\n\n\nImplementing a multi-output model to recognize easy captcha images using the fastai library.\n\n\n\n\n\nMar 31, 2021\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nElegant implementation of Naive Bayes in just 10 lines of code\n\n\n\n\n\n\nmachine learning\n\n\nnlp\n\n\n\nA tutorial on how to use a few tricks to implement a Naive Bayes model in just a few lines of code.\n\n\n\n\n\nFeb 1, 2021\n\n\nAugustas Macijauskas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html",
    "href": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html",
    "title": "Cubic spline interpolation exercise",
    "section": "",
    "text": "I somehow stumbled upon cubic spline interpolation quite accidentally, but it caught my interest with all of the neat linear algebra and scientific computing techniques behind it. This got me excited to revise materials from my undergrad classes and try to implement the method in Python from scratch.\nResources:\n\nThe scipy scipy implementation of the method.\nCubic Spline Interpolation on Wikiversity.\n\n\n\nToggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom scipy.interpolate import CubicSpline\n\n# Ensures we can render plotly plots with quarto\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n\n\n\n\n\n\nx_train = np.linspace(0, 10, 11)\ny_train = np.sin(x_train) + np.random.normal(0, 0.5, 11)\n\n\ncs = CubicSpline(x_train, y_train, bc_type=\"natural\", extrapolate=True)\n\nx_test = np.linspace(-1, 11, 121)\ny_test = cs(x_test)\n\n\n# Create a figure\nfig = go.Figure()\n\n# Add scatter plot\nfig.add_trace(go.Scatter(x=x_train, y=y_train, mode='markers', name='Original points'))\n\n# Add line plot\nfig.add_trace(go.Scatter(x=x_test, y=y_test, mode='lines', name='Fitted line'))\n\n# Add titles and labels\nfig.update_layout(title=\"Scipy interpolation\", xaxis_title=\"x\", yaxis_title=\"y\")\n\n# Show the plot\nfig.show()\n\n                                                \n\n\n\n\n\n\ndef tridiagonal_linear_system_solver(d_lower, d_main, d_upper, b):\n    \"\"\"\n    Solve a tridiagonal linear system given the main, lower, and upper diagonals, as well as the vector b. The Thomas\n    algorithm is used.\n    \"\"\"\n\n    n = len(d_main)\n\n    # Forward sweep\n    for i in range(1, n):\n        w = d_lower[i - 1] / d_main[i - 1]\n        d_main[i] -= w * d_upper[i - 1]\n        b[i] -= w * b[i - 1]\n\n    # Back substitution\n    x = np.zeros(n)\n    x[-1] = b[-1] / d_main[-1]\n    for i in range(n - 2, -1, -1):\n        x[i] = (b[i] - d_upper[i] * x[i + 1]) / d_main[i]\n\n    return x\n\n\nclass CubicSplineCustom:\n\n    def __init__(self, x, y):\n        if not np.all(np.diff(x) &gt; 0):\n            raise ValueError(\"x values must be in ascending order.\")\n\n        self.x = np.array(x)\n        self.y = np.array(y)\n        self._find_coeffs()\n\n    def _find_coeffs(self):\n        # Find the relevant diagonals and from self.x and self.y assuming natural boundary conditions\n        d_lower, d_main, d_upper, b, h = self._compute_diagonals_and_b()\n\n        # Compute the M_i's for i = 1, n-1 (since M_0 and M_n are assumed to be 0). This will require solving a\n        # tridiagonal linear system\n        ms = tridiagonal_linear_system_solver(d_lower, d_main, d_upper, b)\n\n        # Compute the coefficients of the cubic polynomials\n        self.c = self._compute_coefficients_from_second_derivatives(ms, h)\n\n    def _compute_diagonals_and_b(self):\n        x, y = self.x, self.y\n        h = np.diff(x)\n\n        # Compute the diagonals for the tridiagonal matrix\n        d_lower = h.copy()\n        d_lower[-1] = 0  # one of the naturals BCs\n        d_upper = h.copy()\n        d_upper[0] = 0\n\n        d_main = 2 * np.ones(len(x))\n        d_main[1:-1] *= h[:-1] + h[1:]\n\n        b = np.zeros(len(x))\n        y_diff = np.diff(y)\n        b[1:-1] = 6 * (y_diff[1:] / h[1:] - y_diff[:-1] / h[:-1])\n\n        return d_lower, d_main, d_upper, b, h\n\n    def _compute_coefficients_from_second_derivatives(self, ms, h):\n        coeffs = np.zeros((4, len(self.x) - 1))\n        coeffs[0, :] = (ms[1:] - ms[:-1]) / (6 * h)\n        coeffs[1, :] = ms[:-1] / 2\n        coeffs[2, :] = (self.y[1:] - self.y[:-1]) / h - (ms[1:] + 2 * ms[:-1]) * h / 6\n        coeffs[3, :] = self.y[:-1]\n\n        return coeffs\n\n    def _get_index(self, x):\n        \"\"\"Performs binary search\"\"\"\n        low, high = 0, len(self.x) - 1\n\n        while low &lt; high:\n            mid = (low + high) // 2\n            if self.x[mid] &lt;= x:\n                low = mid + 1\n            else:\n                high = mid\n\n        return min(max(low - 1, 0), len(self.x) - 2)\n\n    def interpolate(self, x: float):\n        # Find which polynomial is appropriate and evaluate at x\n        idx = self._get_index(x)\n        dx = x - self.x[idx]\n        return (self.c[0, idx] * dx + self.c[1, idx]) * dx**2 + self.c[2, idx] * dx + self.c[3, idx]\n\n\ncustom_spline = CubicSplineCustom(x_train, y_train)\n\nassert cs.c.shape == custom_spline.c.shape\nassert np.allclose(cs.c, custom_spline.c)\nnp.sqrt(((cs.c - custom_spline.c) ** 2).mean())\n\n2.4075003542614415e-16\n\n\n\nassert np.allclose(custom_spline.interpolate(-1), cs(-1))\nassert np.allclose(custom_spline.interpolate(86500), cs(86500))\n\n\n# Create a figure\nfig = go.Figure()\n\n# Add scatter plot\nfig.add_trace(go.Scatter(x=x_train, y=y_train, mode=\"markers\", name=\"Original data\"))\n\n# Add line plot\nfig.add_trace(go.Scatter(x=x_test, y=y_test, mode=\"lines\", name=\"Fitted line\"))\n\ny_test_custom = [custom_spline.interpolate(x) for x in x_test]\nassert np.allclose(y_test, y_test_custom)\nfig.add_trace(\n    go.Scatter(x=x_test, y=y_test_custom, mode=\"lines\", name=\"Fitted line (custom)\", line=dict(dash=\"longdash\"))\n)\n\n# Add titles and labels\nfig.update_layout(title=\"Scipy and custom interpolations\", xaxis_title=\"x\", yaxis_title=\"y\")\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#imports",
    "href": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#imports",
    "title": "Cubic spline interpolation exercise",
    "section": "",
    "text": "Toggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom scipy.interpolate import CubicSpline\n\n# Ensures we can render plotly plots with quarto\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\""
  },
  {
    "objectID": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#cubic-spline-interpolation-with-scipy",
    "href": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#cubic-spline-interpolation-with-scipy",
    "title": "Cubic spline interpolation exercise",
    "section": "",
    "text": "x_train = np.linspace(0, 10, 11)\ny_train = np.sin(x_train) + np.random.normal(0, 0.5, 11)\n\n\ncs = CubicSpline(x_train, y_train, bc_type=\"natural\", extrapolate=True)\n\nx_test = np.linspace(-1, 11, 121)\ny_test = cs(x_test)\n\n\n# Create a figure\nfig = go.Figure()\n\n# Add scatter plot\nfig.add_trace(go.Scatter(x=x_train, y=y_train, mode='markers', name='Original points'))\n\n# Add line plot\nfig.add_trace(go.Scatter(x=x_test, y=y_test, mode='lines', name='Fitted line'))\n\n# Add titles and labels\nfig.update_layout(title=\"Scipy interpolation\", xaxis_title=\"x\", yaxis_title=\"y\")\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#custom-implementation-of-cubic-spline-interpolation",
    "href": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#custom-implementation-of-cubic-spline-interpolation",
    "title": "Cubic spline interpolation exercise",
    "section": "",
    "text": "def tridiagonal_linear_system_solver(d_lower, d_main, d_upper, b):\n    \"\"\"\n    Solve a tridiagonal linear system given the main, lower, and upper diagonals, as well as the vector b. The Thomas\n    algorithm is used.\n    \"\"\"\n\n    n = len(d_main)\n\n    # Forward sweep\n    for i in range(1, n):\n        w = d_lower[i - 1] / d_main[i - 1]\n        d_main[i] -= w * d_upper[i - 1]\n        b[i] -= w * b[i - 1]\n\n    # Back substitution\n    x = np.zeros(n)\n    x[-1] = b[-1] / d_main[-1]\n    for i in range(n - 2, -1, -1):\n        x[i] = (b[i] - d_upper[i] * x[i + 1]) / d_main[i]\n\n    return x\n\n\nclass CubicSplineCustom:\n\n    def __init__(self, x, y):\n        if not np.all(np.diff(x) &gt; 0):\n            raise ValueError(\"x values must be in ascending order.\")\n\n        self.x = np.array(x)\n        self.y = np.array(y)\n        self._find_coeffs()\n\n    def _find_coeffs(self):\n        # Find the relevant diagonals and from self.x and self.y assuming natural boundary conditions\n        d_lower, d_main, d_upper, b, h = self._compute_diagonals_and_b()\n\n        # Compute the M_i's for i = 1, n-1 (since M_0 and M_n are assumed to be 0). This will require solving a\n        # tridiagonal linear system\n        ms = tridiagonal_linear_system_solver(d_lower, d_main, d_upper, b)\n\n        # Compute the coefficients of the cubic polynomials\n        self.c = self._compute_coefficients_from_second_derivatives(ms, h)\n\n    def _compute_diagonals_and_b(self):\n        x, y = self.x, self.y\n        h = np.diff(x)\n\n        # Compute the diagonals for the tridiagonal matrix\n        d_lower = h.copy()\n        d_lower[-1] = 0  # one of the naturals BCs\n        d_upper = h.copy()\n        d_upper[0] = 0\n\n        d_main = 2 * np.ones(len(x))\n        d_main[1:-1] *= h[:-1] + h[1:]\n\n        b = np.zeros(len(x))\n        y_diff = np.diff(y)\n        b[1:-1] = 6 * (y_diff[1:] / h[1:] - y_diff[:-1] / h[:-1])\n\n        return d_lower, d_main, d_upper, b, h\n\n    def _compute_coefficients_from_second_derivatives(self, ms, h):\n        coeffs = np.zeros((4, len(self.x) - 1))\n        coeffs[0, :] = (ms[1:] - ms[:-1]) / (6 * h)\n        coeffs[1, :] = ms[:-1] / 2\n        coeffs[2, :] = (self.y[1:] - self.y[:-1]) / h - (ms[1:] + 2 * ms[:-1]) * h / 6\n        coeffs[3, :] = self.y[:-1]\n\n        return coeffs\n\n    def _get_index(self, x):\n        \"\"\"Performs binary search\"\"\"\n        low, high = 0, len(self.x) - 1\n\n        while low &lt; high:\n            mid = (low + high) // 2\n            if self.x[mid] &lt;= x:\n                low = mid + 1\n            else:\n                high = mid\n\n        return min(max(low - 1, 0), len(self.x) - 2)\n\n    def interpolate(self, x: float):\n        # Find which polynomial is appropriate and evaluate at x\n        idx = self._get_index(x)\n        dx = x - self.x[idx]\n        return (self.c[0, idx] * dx + self.c[1, idx]) * dx**2 + self.c[2, idx] * dx + self.c[3, idx]\n\n\ncustom_spline = CubicSplineCustom(x_train, y_train)\n\nassert cs.c.shape == custom_spline.c.shape\nassert np.allclose(cs.c, custom_spline.c)\nnp.sqrt(((cs.c - custom_spline.c) ** 2).mean())\n\n2.4075003542614415e-16\n\n\n\nassert np.allclose(custom_spline.interpolate(-1), cs(-1))\nassert np.allclose(custom_spline.interpolate(86500), cs(86500))\n\n\n# Create a figure\nfig = go.Figure()\n\n# Add scatter plot\nfig.add_trace(go.Scatter(x=x_train, y=y_train, mode=\"markers\", name=\"Original data\"))\n\n# Add line plot\nfig.add_trace(go.Scatter(x=x_test, y=y_test, mode=\"lines\", name=\"Fitted line\"))\n\ny_test_custom = [custom_spline.interpolate(x) for x in x_test]\nassert np.allclose(y_test, y_test_custom)\nfig.add_trace(\n    go.Scatter(x=x_test, y=y_test_custom, mode=\"lines\", name=\"Fitted line (custom)\", line=dict(dash=\"longdash\"))\n)\n\n# Add titles and labels\nfig.update_layout(title=\"Scipy and custom interpolations\", xaxis_title=\"x\", yaxis_title=\"y\")\n\n# Show the plot\nfig.show()"
  }
]