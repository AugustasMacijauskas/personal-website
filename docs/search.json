[
  {
    "objectID": "posts/prompt-engineering-course/prompt-engineering-course-notes.html",
    "href": "posts/prompt-engineering-course/prompt-engineering-course-notes.html",
    "title": "A list of notes on the “ChatGPT Prompt Engineering for Developers” course",
    "section": "",
    "text": "List of notes on the “ChatGPT Prompt Engineering for Developers” course\nCourse link.\nHope you find these helpful (I certainly do)!"
  },
  {
    "objectID": "posts/optimization-algorithms/optimisation-algorithms.html",
    "href": "posts/optimization-algorithms/optimisation-algorithms.html",
    "title": "Implementing optimisation algorithms to optimise parameters of linear functions",
    "section": "",
    "text": "Imports\nToggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nData\nLet’s create 100 x values uniformly distributed between 0 and 100. Note the usage of a random seed for reproducibility:\n\nnp.random.seed(0)\nx = np.random.uniform(0, 100, size=100)\n\nCalculate y values for the given x values. I chose a slope of -2 and an intercept of 10 and decided not to add noise to make the task easier:\n\nslope = -2\nintercept = 10\ny = slope * x + intercept\n\nLet’s plot the points to check that everything looks as it should be:\n\nplt.scatter(x, y)\nplt.plot(x, slope * x + intercept, color=\"red\");\n\n\n\n\n\n\nPrepare for the experiments\nLet’s create some classes and functions that will help us to quickly experiment with different optimization algorithms.\nWe start by creating our simple LinearModel whose parameters we’ll try to optimize. Here we choose to initialize both the slope and the intercept with the value 1, but this should not matter too much for this problem.\nThe forward method performs the forward pass of the model to get a prediction for a given x, i.e. just return ax+b.\nThe backward method is used to calculate gradients of the parameters given x, y and y_pred. These are usually calculated using automatic differentiation in deep learning libraries, such as PyTorch, but here we do everything by hand:\n\nclass LinearModel:\n    def __init__(self, initial_slope=1, initial_intercept=1):\n        self.slope = initial_slope\n        self.intercept = initial_intercept\n    \n    def forward(self, x):\n        return self.slope * x + self.intercept\n    \n    def backward(self, x, y, y_pred):\n        return x * (y_pred - y), (y_pred - y)\n\nThe Tracker utility class will be used to track different values while experimenting. It can be considered an overkill for such a simple task (and I later ended up tracking only one value with it), but it can be very convenient for larger projects, so I included it here too.\nThe class stores a dictionary of string-list pairs in its state and provides add_record and get_values methods to easily access and update that dictionary.\n\nfrom collections import defaultdict\n\nclass Tracker:\n    def __init__(self):\n        self.tracker = defaultdict(list)\n    \n    def add_record(self, key, value):\n        self.tracker[key].append(value)\n    \n    def get_values(self, key):\n        if not key in self.tracker: raise KeyError(f\"Key {key} not in tracker\")\n\n        return self.tracker[key]\n\ntrain_one_epoch is a utility function that trains the model for one epoch (one pass through the data). It performs a forward pass, calculates and tracks the error, gets the gradients of parameters by performing a backward pass and passes those along with the params parameter to the optimizer that updates the weights of the model. The params parameter is a dictionary that is used to hyperparameter constants, such as the learning rate, to the optimizer.\n\ndef train_one_epoch(model, data, optimizer, tracker, params):\n    total_error = 0.0\n    \n    for x, y in data:\n        # Forward pass\n        y_pred = model.forward(x)\n        \n        # Calculate error\n        error = 0.5 * (y_pred - y) ** 2\n        total_error += error\n\n        # Backward pass\n        grad_slope, grad_intercept = model.backward(x, y, y_pred)\n        \n        # Step the optimizer\n        params[\"step\"] += 1\n        optimizer.step(model, grad_slope, grad_intercept, params)\n\n    tracker.add_record(\"error\", total_error / len(data))\n\nFinally, a utility function that brings everything together to execute an experiment. Our goal is to train the model for as little as possible, stopping when error drops below some desired threshold.\n\ndef perform_experiment(optimizer, params, desired_error=1e-8):\n    # Initialization\n    model = LinearModel()\n    data = list(zip(x, y))\n    tracker = Tracker()\n    \n    params[\"step\"] = 0\n    while True:\n        # Train for an epoch\n        train_one_epoch(model, data, optimizer, tracker, params)\n        \n        # Check if we reached desired accuracy\n        if tracker.get_values(\"error\")[-1] &lt;= desired_error:\n            break\n        \n        # Improper hyperparameter values might cause the models to diverge,\n        # so we include this here to stop execution if this happens\n        if np.isnan(tracker.get_values(\"error\")[-1]):\n            break\n    \n    # Print results and plot error over time\n    steps = params[\"step\"]\n    print(f\"Epochs: {steps // len(data)}\")\n    print(f\"Batches (of size 1): {steps}\")\n    print(f\"Slope: {model.slope}\")\n    print(f\"Intercept: {model.intercept}\")\n    plt.plot(tracker.get_values(\"error\"), linestyle=\"dotted\")\n    \n    return params[\"step\"] // len(data), params[\"step\"], model.slope, model.intercept\n\n\n\nVanilla SGD\n\nclass SGDOptimizer:\n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        model.slope -= lr * grad_slope\n        model.intercept -= lr * grad_intercept\n\n\noptimizer = SGDOptimizer()\nparams = { \"lr\": 1e-4 }\nperform_experiment(optimizer, params);\n\nEpochs: 3893\nBatches (of size 1): 389300\nSlope: -1.9999963788900519\nIntercept: 9.999738575620166\n\n\n\n\n\nBreaks with lr=1e-3\n\n\nSGD with momentum\n\nclass SGDWithMomentumOptimizer(SGDOptimizer):\n    def __init__(self):\n        self.slope_momentum = self.intercept_momentum = 0\n    \n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        miu = params[\"miu\"]\n        self.slope_momentum = miu * self.slope_momentum + (1 - miu) * grad_slope\n        model.slope -= lr * self.slope_momentum\n\n        self.intercept_momentum = miu * self.intercept_momentum + (1 - miu) * grad_intercept\n        model.intercept -= lr * self.intercept_momentum\n\n\noptimizer = SGDWithMomentumOptimizer()\nparams = { \"lr\": 1e-4, \"miu\": 0.9 }\nperform_experiment(optimizer, params);\n\nEpochs: 3844\nBatches (of size 1): 384400\nSlope: -1.9999951526911044\nIntercept: 9.999742134759243\n\n\n\n\n\n\noptimizer = SGDWithMomentumOptimizer()\nparams = { \"lr\": 1e-3, \"miu\": 0.9 }\nperform_experiment(optimizer, params);\n\nEpochs: 365\nBatches (of size 1): 36500\nSlope: -1.9999980020599053\nIntercept: 9.999929876584854\n\n\n\n\n\n\n\nRMSprop\n\nclass RMSpropOptimizer(SGDOptimizer):\n    def __init__(self, eps=1e-8):\n        self.eps = eps\n        self.slope_squared_grads = self.intercept_squared_grads = 0\n    \n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        decay_rate = params[\"decay_rate\"]\n        self.slope_squared_grads = decay_rate * self.slope_squared_grads + (1 - decay_rate) * grad_slope ** 2\n        model.slope -= lr * grad_slope / (np.sqrt(self.slope_squared_grads) + self.eps)\n\n        self.intercept_squared_grads = decay_rate * self.intercept_squared_grads + (1 - decay_rate) * grad_intercept ** 2\n        model.intercept -= lr * grad_intercept / (np.sqrt(self.intercept_squared_grads) + self.eps)\n\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 1e-4, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 2765\nBatches (of size 1): 276500\nSlope: -1.9999999858447193\nIntercept: 9.999996406289089\n\n\n\n\n\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 1e-3, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 302\nBatches (of size 1): 30200\nSlope: -1.9999998782649515\nIntercept: 9.999976783670757\n\n\n\n\n\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 3e-3, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 113\nBatches (of size 1): 11300\nSlope: -2.0000001115561656\nIntercept: 10.000020816756175\n\n\n\n\n\nWorks best with lr=3e-3 and decay_rate=0.99.\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 1e1, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 83323\nBatches (of size 1): 8332300\nSlope: -2.000000169655216\nIntercept: 10.000030570050711\n\n\n\n\n\n\n\nAdam\n\nclass Adam(SGDOptimizer):\n    def __init__(self, eps=1e-8):\n        self.eps = eps\n        \n        self.slope_momentum = self.intercept_momentum = 0\n        self.slope_squared_grads = self.intercept_squared_grads = 0\n    \n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        beta1, beta2 = params[\"betas\"]\n        step = params[\"step\"]\n        \n        self.slope_momentum = beta1 * self.slope_momentum + (1 - beta1) * grad_slope\n        slope_corrected_momentum = self.slope_momentum / (1 - beta1 ** step)\n        self.slope_squared_grads = beta2 * self.slope_squared_grads + (1 - beta2) * grad_slope ** 2\n        slope_corrected_squared_grads = self.slope_squared_grads / (1 - beta2 ** step)\n        model.slope -= lr * slope_corrected_momentum / (np.sqrt(slope_corrected_squared_grads) + self.eps)\n\n        self.intercept_momentum = beta1 * self.intercept_momentum + (1 - beta1) * grad_intercept\n        intercept_corrected_momentum = self.intercept_momentum / (1 - beta1 ** step)\n        self.intercept_squared_grads = beta2 * self.intercept_squared_grads + (1 - beta2) * grad_intercept ** 2\n        intercept_corrected_squared_grads = self.intercept_squared_grads / (1 - beta2 ** step)\n        model.intercept -= lr * intercept_corrected_momentum / (np.sqrt(intercept_corrected_squared_grads) + self.eps)\n\n\noptimizer = Adam()\nparams = { \"lr\": 1e-4, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params);\n\nEpochs: 2848\nBatches (of size 1): 284800\nSlope: -1.9999980335219065\nIntercept: 9.999814585095613\n\n\n\n\n\n\noptimizer = Adam()\nparams = { \"lr\": 1e0, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params);\n\nEpochs: 10\nBatches (of size 1): 1000\nSlope: -2.00000001786048\nIntercept: 10.000001361061178\n\n\n\n\n\nAdam seems to be of the order of 10 times faster than other optimization methods and works with much greater learning rates\n\noptimizer = Adam()\nparams = { \"lr\": 1e8, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params);\n\nEpochs: 43\nBatches (of size 1): 4300\nSlope: -2.000001151784137\nIntercept: 9.999987336010909\n\n\n\n\n\nIn this very easy case of parameters of a linear curve estimation it works with learning rates as large as 1e8, whereas the largest learning rate that still works among other methods is 1e1 for RMSprop. However, the graphs there showed that oscillations in the error were all over the place, while Adam barely oscillated at all.\n\noptimizer = Adam()\nparams = { \"lr\": 6e-2, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params, 1e-32);\n\nEpochs: 72\nBatches (of size 1): 7200\nSlope: -2.0\nIntercept: 10.0\n\n\n\n\n\nAlso, Adam is the only optimizer that was able to achieve an error as little as 1e-32. Even though out of the box the number of steps (in epochs) was of the same order as for other optimization methods, some hyperparameter tinkering allowed to top them by reducing the number of steps to 72."
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "",
    "text": "This article is adapted from fast.ai’s Machine Learning for Coders course, specifically, lesson 10. I would highly recommend checking this and other courses from fast.ai, it has numerous tips on how to do practical machine learning and deep learning.\nWe will be building a naive Bayes classifier in just 10 lines of code that will get over 98% accuracy on a spam message filtering task.\nWe will do this in the top-bottom approach, where we will first build the model and then dig deeper into the theory of how it works."
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#the-countvectorizer",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#the-countvectorizer",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "1. The CountVectorizer",
    "text": "1. The CountVectorizer\nNaive Bayes classifier uses what is called a bag of words approach. It simply means that we disregard any relationships between words and just look at how often they appear in the text that we want to classify.\n\nNote: I am not saying that bag of words is the best approach to do NLP, it is usually quite the opposite as nowadays we have tools like RNNs and Transformers that perform much better on NLP tasks. However, we use it here because it is a really simple approach that sometimes still gives reasonable results, as it did in this case!\n\nThis is exactly what we use a CountVectorizer for: it produces a term document matrix with frequencies of each word for each message.\nLet’s look at an example of what sklearn’s CountVectorizer is doing. Suppose that our messages are:\n\n\n\nmessage\nlabel\n\n\n\n\nThis is a good message\n0\n\n\nA good message\n0\n\n\nThis message is bad\n1\n\n\nThe message is bad\n1\n\n\n\nThen, what a CountVectorizer is going to do for us is produce the following matrix:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\n\n\nImportant: CountVectorizer just finds the vocabulary (list of all the unique words in our data) and then counts how often each word from the vocabulary is found in each message.\n\nNotice that for a larger dataset, our vocabulary might become very large which would mean that most of the cells in our term document matrix would be 0. For this reason, the sklearn implementation actually produces a sparse matrix which, instead of storing all the entries, stores only the location and values of non-zero entries, and since there are only so many of them, saves huge amounts of memory. How neat!\nYou must have noticed that I used a max_df=0.1 parameter for my CountVectorizer. This tells the vectorizer to ignore any words that appear in more than 10% on the documents as we can safely say that they are too common. I came with this number by trying different values and looking at vectorizer.stop_words_ to check how many and which words were ignored until I was satisfied. When you build your own model, make sure to play around with this and other parameters, such as min_df (opposite of max_df, used for very rare words), to find what works best for you! You can find more information on what parameters for CountVectorizer can be tinkered on the official docs.\nA trick that we want to do before moving on is to note that later we will want to calculate probabilities of each word appearing in spam or non-spam messages. But it might happen that certain words do not appear in a particular class at all and we might run into trouble because we will get feature probabilities of zero, and we don’t want that. To counter that, we will add a row of ones, like so:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\nrow of ones\n\n1\n1\n1\n1\n1\n1\n1\n\n\n\nIf we think more about, adding a row of ones is not that counter-intuitive at all, since the messages that we have so far only carry information up to this point in time, but if a word has not appeared in any of the messages so far, it is not at all impossible that it will not appear in the future, so adding the extra one takes care of that for us.\nSo that our feature probabilities will be:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\nrow of ones\n\n1\n1\n1\n1\n1\n1\n1\n\n\nP(feature|0)\n\n0.67\n0.67\n1\n1\n1\n0.33\n0.33\n\n\nP(feature|1)\n\n0.67\n1\n0.33\n0.33\n1\n0.67\n1\n\n\n\nIn code:\n\n\n# Create a vectorizer object that will ignore any\n# words that appear in more than 10% of messages.\nvectorizer = CountVectorizer(max_df=0.1)\n\n# Use the fit_transform method of the vectorizer to get\n# the term document matrix for the training set.\ntrain_term_doc = vectorizer.fit_transform(X_train)\n\n# Use the transform method of the vectorizer to get\n# the term document matrix for the validation set.\n# We do it this way so that train and validation sets\n# are have the same vocabularies so that we could make predictions.\nval_term_doc = vectorizer.transform(X_val)"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#bayes-theorem",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#bayes-theorem",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "2. Bayes’ theorem",
    "text": "2. Bayes’ theorem\nNow, we can get to the essence of the model which is to apply Bayes’ theorem. I am not going to give the usual form of the formula, but instead one that will illustrate how we will be using it. Our goal is, given a particular message, figure out whether it is spam or not. Hence, the formula for us is going to take the form:\n\\(P(\\text{spam} \\mid \\text{message}) = \\frac{P(\\text{message} \\mid \\text{spam}) \\cdot P(\\text{spam})}{P(\\text{message})}\\)\nBut we can use a trick: instead of trying to predict whether it is spam or not, let’s look at which class is a message more likely, i.e. \\(\\frac{P(\\text{spam} \\mid \\text{message})}{P(\\text{non-spam} \\mid \\text{message})}\\). In that case, the formula will become:\n\\(\\text{ratio} = \\frac{P(\\text{spam} \\mid \\text{message})}{P(\\text{non-spam} \\mid \\text{message})} = \\frac{P(\\text{message} \\mid \\text{spam}) \\cdot P(\\text{spam})}{P(\\text{message} \\mid \\text{non-spam}) \\cdot P(\\text{non-spam})}\\)\nReferring to our previous example, the ratios would then be:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\nrow of ones\n\n1\n1\n1\n1\n1\n1\n1\n\n\nP(feature|0)\n\n0.67\n0.67\n1\n1\n1\n0.33\n0.33\n\n\nP(feature|1)\n\n0.67\n1\n0.33\n0.33\n1\n0.67\n1\n\n\nratio\n\n1\n1.5\n0.33\n0.33\n1\n2\n3\n\n\n\n\nImportant: As you can see, ratios are greater than 1 for features that are more likely to be in spam messages and lower than one otherwise.\n\nThen, to further simplify things, we make the naive Bayes assumption, which says that probability of any word appearing in a message is independent of probabilities of other words appearing in that same message.\n\nNote: Obviously, this is a very naive assumption and that is most certainly not the case, but it turns out to work quite well.\n\nUnder the naive assumption, the probabilities like \\(P(\\text{message} \\mid \\text{spam})\\) can be factorized into a product of probabilities of individual features appearing in a message, so that:\n\\(P(\\text{message} \\mid \\text{spam}) = \\prod_{i=1}^{n}{P(\\text{features[i]} \\mid \\text{spam})}\\)\nand similarly for \\(P(\\text{message} \\mid \\text{non-spam})\\). Then, our big formula becomes:\n\\(\\text{ratio} = \\frac{P(\\text{spam} \\mid \\text{message})}{P(\\text{non-spam} \\mid \\text{message})} = \\frac{\\prod_{i=1}^{n}{P(\\text{features[i]} \\mid \\text{spam})}}{\\prod_{i=1}^{n}{P(\\text{features[i]} \\mid \\text{non-spam})}} \\cdot \\frac{P(\\text{spam})}{P(\\text{non-spam})}\\)"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#a-few-final-tricks",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#a-few-final-tricks",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "3. A few final tricks",
    "text": "3. A few final tricks\nWe are almost done, now we just want to apply a few tricks to make our calculations easier. First, notice that multiplying lots of probabilities together is going to result into a very small number and we might run out of floating point precision, so we can take the natural logarithm instead to handle this. Note that in this case we compare ratios not with 1, but with 0 (because log(1)=0) and that by the properties of logarithms all the products are going to turn into sums, which makes everything even simpler!\nFinally, we notice that to make predictions, we can just perform matrix multiplication on the validation term document matrix and our derived vector of ratios and add (remember, we are in log space!) the ratio of priors.\nPutting it all together:\n\n\n# Calculate P(feature|1) and P(feature|0).\n# This plus ones are there to constitute the\n# row of ones discussed above\np = train_term_doc[y_train == 1].sum(0) + 1\nq = train_term_doc[y_train == 0].sum(0) + 1\n\n# Calculate the ratios according to our derived formulae\nratio = np.log((p / p.sum()) / (q / q.sum()))\n\n# Calculate the log of ratio of priors\nb = np.log((y_train == 1).sum() / (y_train == 0).sum())\n\n# Make some predictions on the validation set\npre_preds = val_term_doc @ ratio.T + b\npreds = pre_preds.T &gt; 0 # Greater than 0 because we are working in log space\n(preds == y_val).mean() # Accuracy\n\n0.9856502242152466"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#try-n-grams",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#try-n-grams",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "Try n-grams",
    "text": "Try n-grams\n\nvectorizer = CountVectorizer(ngram_range=(1, 3), max_df=0.1)\ntrain_term_doc = vectorizer.fit_transform(X_train)\nval_term_doc = vectorizer.transform(X_val)\n\n\np = train_term_doc[y_train == 1].sum(0) + 1\nq = train_term_doc[y_train == 0].sum(0) + 1\nratio = np.log((p / p.sum()) / (q / q.sum()))\nb = np.log((y_train == 1).sum() / (y_train == 0).sum())\n\n\npre_preds = val_term_doc @ ratio.T + b\npreds = pre_preds.T &gt; 0\n(preds == y_val).mean()\n\n0.9874439461883409\n\n\nTurns out the model gives even better accuracy with bigrams and trigrams included. But watch out! Checking the confusion matrices, we see that the model is now perfect on non-spam messages, but the error on spam messages has increased. This might not be what we want, so we have to be careful with interpreting our models!\n\nconfusion_matrix(y_val, preds.T, normalize=None)\n\narray([[968,   2],\n       [ 12, 133]])\n\n\n\nconfusion_matrix(y_val, preds.T, normalize=\"true\")\n\narray([[0.99793814, 0.00206186],\n       [0.08275862, 0.91724138]])"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#binarized-version",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#binarized-version",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "Binarized version",
    "text": "Binarized version\nYou can also try the binarized version of the term document matrix (i.e. instead of frequencies we look at whether a word is present or not)\n\npre_preds = val_term_doc.sign() @ ratio.T + b\npreds = pre_preds.T &gt; 0\n(preds == y_val).mean()\n\n0.9874439461883409"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#learning-the-parameters-with-logistic-regression",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#learning-the-parameters-with-logistic-regression",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "Learning the parameters with logistic regression",
    "text": "Learning the parameters with logistic regression\nFinally, you might try taking it to the next level and learning the parameters with a logistic regression instead of using the theoretical ones. Check the parameters C for regularization and dual=True for when you term document matrix is much wider than it is tall.\n\nm = LogisticRegression(C=1e1, dual=False)\nm.fit(train_term_doc, y_train)\npreds = m.predict(val_term_doc)\n(preds == y_val).mean()\n\n0.9811659192825112\n\n\n\n\n# Binarized version\nm = LogisticRegression(C=1e1, dual=False)\nm.fit(train_term_doc.sign(), y_train)\npreds = m.predict(val_term_doc.sign())\n(preds == y_val).mean()\n\n0.9802690582959641"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html",
    "href": "posts/basic-panorama/basic-panorama.html",
    "title": "Basic panorama stitching",
    "section": "",
    "text": "In this notebook we will find that we can use knowledge of computer vision to create our own panoramas! We only need to take a few photos with a smartphone or camera, but we have to make sure that we only change the angle of the phone/camera, and not the position! This notebook then takes care of everything else.\nThis notebook is based on the Computer Vision course by Andreas Geiger."
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#preliminaries",
    "href": "posts/basic-panorama/basic-panorama.html#preliminaries",
    "title": "Basic panorama stitching",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom pathlib import Path\n\n\nLet’s first import the some images that we want to stich together:\n\nPATH_TO_YOUR_IMAGES = Path(\"path/to/your/images\")\n\n\n# Load images\nimages = sorted([f for f in PATH_TO_YOUR_IMAGES.iterdir()])\nimages = map(str, images)\nimages = map(cv2.imread, images)\nimages = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images]\n\nlen(images)\n\n10\n\n\n\nimages = images[2:4] + images[5:]\n\nLet’s have a look at the images:\n\nfig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n\nfor img, ax in zip(images, axes.flatten()):\n    ax.imshow(img)\n    ax.axis(\"off\")\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#finding-keypoints",
    "href": "posts/basic-panorama/basic-panorama.html#finding-keypoints",
    "title": "Basic panorama stitching",
    "section": "Finding keypoints",
    "text": "Finding keypoints\nTo estimate the homography matrix, we need correspondence pairs between images. The following is a function for this based on feature matching:\n\ndef get_keypoints(img1, img2):\n    orb = cv2.ORB_create(nfeatures=2000)\n\n    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n    bf = cv2.BFMatcher_create(cv2.NORM_HAMMING)\n\n    # Find matching points\n    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n    good = []\n    for m, n in matches:\n        if m.distance &lt; 0.5 * n.distance:\n            good.append(m)\n    \n    p_source = np.float32([keypoints1[good_match.queryIdx].pt for good_match in good]).reshape(-1, 2)\n    p_target = np.float32([keypoints2[good_match.trainIdx].pt for good_match in good]).reshape(-1, 2)\n    \n    N = p_source.shape[0]\n    p_source = np.concatenate([p_source, np.ones((N, 1))], axis=-1)\n    p_target = np.concatenate([p_target, np.ones((N, 1))], axis=-1)\n    \n    return p_source, p_target\n\nLet’s now look at some correspondence pairs. For this, the we use the draw_matches function:\n\ndef draw_matches(img1, points_source, img2, points_target):\n    ''' Returns an image with matches drawn onto the images.\n    '''\n    r, c = img1.shape[:2]\n    r1, c1 = img2.shape[:2]\n\n    output_img = np.zeros((max([r, r1]), c + c1, 3), dtype='uint8')\n    output_img[:r, :c, :] = np.dstack([img1])\n    output_img[:r1, c:c + c1, :] = np.dstack([img2])\n\n    for p1, p2 in zip(points_source, points_target):\n        (x1, y1) = p1[:2]\n        (x2, y2) = p2[:2]\n\n        cv2.circle(output_img, (int(x1), int(y1)), 10, (0, 255, 255), 10)\n        cv2.circle(output_img, (int(x2) + c, int(y2)), 10, (0, 255, 255), 10)\n\n        cv2.line(output_img, (int(x1), int(y1)), (int(x2) + c, int(y2)), (0, 255, 255), 5)\n\n    return output_img\n\nWe calculate the keypoints:\n\nkeypoint_pairs = [get_keypoints(img1, img2) for img1, img2 in zip(images[:-1], images[1:])]\nsource_points = [pair[0] for pair in keypoint_pairs]\ntarget_points = [pair[1] for pair in keypoint_pairs]\n\nlen(source_points), len(target_points)\n\n(6, 6)\n\n\nCheck how many keypoints we have:\n\nfor i in range(len(source_points)):\n    print(source_points[i].shape, target_points[i].shape)\n\n(305, 3) (305, 3)\n(188, 3) (188, 3)\n(500, 3) (500, 3)\n(482, 3) (482, 3)\n(388, 3) (388, 3)\n(476, 3) (476, 3)\n\n\nVisualise the keypoints:\n\nmatches_to_show = 200\n\nfor img1, p_source, img2, p_target in zip(images[:-1], source_points, images[1:], target_points):\n    f = plt.figure(figsize=(20, 10))\n    vis = draw_matches(img1, p_source[:matches_to_show], img2, p_target[:matches_to_show])\n    plt.axis(\"off\")\n    plt.imshow(vis)"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#estimating-the-homography-matrix",
    "href": "posts/basic-panorama/basic-panorama.html#estimating-the-homography-matrix",
    "title": "Basic panorama stitching",
    "section": "Estimating the homography matrix",
    "text": "Estimating the homography matrix\nAfter looking at the correspondences, let’s stitch the images together! In order to stitch together the images, we need a function to return the 2x9 homography matrix A_i matrix for a given 2D correspondence pair xi_vector and xi_prime_vector (which are 3D homogeneous vectors).\n\ndef get_Ai(xi_vector, xi_prime_vector):\n    ''' Returns the A_i matrix discussed in the lecture for input vectors.\n    \n    Args:\n        xi_vector (array): the x_i vector in homogeneous coordinates\n        xi_vector_prime (array): the x_i_prime vector in homogeneous coordinates\n    '''\n    assert xi_vector.shape == (3, ) and xi_prime_vector.shape == (3, )\n\n    Ai = np.zeros((2, 9))\n    Ai[0, 3:6] = -xi_prime_vector[2] * xi_vector\n    Ai[0, 6:9] = xi_prime_vector[1] * xi_vector\n    Ai[1, 0:3] = xi_prime_vector[2] * xi_vector\n    Ai[1, 6:9] = -xi_prime_vector[0] * xi_vector\n\n    assert(Ai.shape == (2, 9))\n    \n    return Ai\n\nUsing get_Ai, write a function get_A which returns the A matrix of size 2Nx9:\n\ndef get_A(points_source, points_target):\n    ''' Returns the A matrix discussed in the lecture.\n    \n    Args:\n        points_source (array): 3D homogeneous points from source image\n        points_target (array): 3D homogeneous points from target image\n    '''\n    N = points_source.shape[0]\n\n    # Insert your code here\n    A = np.vstack([\n        get_Ai(src, target) for src, target in zip(points_source, points_target)\n    ])\n    \n    assert(A.shape == (2*N, 9))\n    return A\n\nNext, implement the function get_homography which returns the homography H for point correspondence pairs. We obtain H by performing the Direct Linear Transformation (DLT) algorithm:\n\ndef get_homography(points_source, points_target):\n    ''' Returns the homography H.\n    \n    Args:\n        points_source (array): 3D homogeneous points from source image\n        points_target (array): 3D homogeneous points from target image        \n    '''\n\n    # Insert your code here\n    A = get_A(points_source, points_target)\n    _, _, V_T = np.linalg.svd(A)\n    H = V_T.T[:, -1].reshape((3, 3))\n\n    assert H.shape == (3, 3)\n    \n    return H\n\nWe need a function which takes in the two images and the calculated homography and it returns the stiched image in a format which we can display easy with matplotlib. This function is provided in the following.\n\ndef stitch_images(img1, img2, H):\n    ''' Stitches together the images via given homography H.\n\n    Args:\n        img1 (array): image 1\n        img2 (array): image 2\n        H (array): homography\n    '''\n\n    rows1, cols1 = img1.shape[:2]\n    rows2, cols2 = img2.shape[:2]\n\n    list_of_points_1 = np.float32([[0,0], [0, rows1],[cols1, rows1], [cols1, 0]]).reshape(-1, 1, 2)\n    temp_points = np.float32([[0,0], [0,rows2], [cols2,rows2], [cols2,0]]).reshape(-1,1,2)\n\n    list_of_points_2 = cv2.perspectiveTransform(temp_points, H)\n    list_of_points = np.concatenate((list_of_points_1,list_of_points_2), axis=0)\n\n    [x_min, y_min] = np.int32(list_of_points.min(axis=0).ravel() - 0.5)\n    [x_max, y_max] = np.int32(list_of_points.max(axis=0).ravel() + 0.5)\n\n    translation_dist = [-x_min,-y_min]\n\n    H_translation = np.array([[1, 0, translation_dist[0]], [0, 1, translation_dist[1]], [0, 0, 1]])\n    H_final = H_translation.dot(H)\n\n    output_img = cv2.warpPerspective(img2, H_final, (x_max-x_min, y_max-y_min))\n    \n    output_img[translation_dist[1]:rows1+translation_dist[1], translation_dist[0]:cols1+translation_dist[0]] = img1\n    output_img = output_img[translation_dist[1]:rows1+translation_dist[1]:, :]\n\n    # Find top and bottom rows\n    top_row = output_img.nonzero()[0].min()\n    bottom_row = output_img.nonzero()[0].max()\n\n    top_max = output_img[top_row, :].nonzero()[0].max()\n    bottom_max = output_img[bottom_row, :].nonzero()[0].max()\n   \n    # Cut width\n    output_img = output_img[:, :min(top_max, bottom_max)]\n\n    return output_img\n\nWith this, we can stitch two images together and see how it looks:\n\nfor img1, p_source, img2, p_target in zip(images[:-1], source_points, images[1:], target_points):\n    H = get_homography(p_target, p_source)\n    stitched_image = stitch_images(img1, img2, H)\n\n    fig = plt.figure(figsize=(15, 10))\n    plt.axis(\"off\")\n    plt.imshow(stitched_image)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we repeat this process iteratively until all images are stitched together:\n\ndef get_panorama(images):\n    n_repeats = len(images) - 1\n\n    current_images = images\n\n    for _ in range(n_repeats):\n        new_images = []\n\n        keypoint_pairs = [get_keypoints(img1, img2) for img1, img2 in zip(current_images[:-1], current_images[1:])]\n        source_points = [pair[0] for pair in keypoint_pairs]\n        target_points = [pair[1] for pair in keypoint_pairs]\n\n        for img1, p_source, img2, p_target in zip(current_images[:-1], source_points, current_images[1:], target_points):\n            H = get_homography(p_target, p_source)\n            stitched_image = stitch_images(img1, img2, H)\n\n            new_images.append(stitched_image)\n\n        current_images = new_images\n    \n    assert len(current_images) == 1\n\n    return current_images[0]\n\n\npanorama = get_panorama(images)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(panorama)\nplt.axis(\"off\");"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#conclusion",
    "href": "posts/basic-panorama/basic-panorama.html#conclusion",
    "title": "Basic panorama stitching",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s it! We now have a very basic panorama stitcher.\nThis approach is quite naive and I am sure that real-world algorithms to produce panoramas are much more sophisticated. Therefore, I am open to hear any feedback or suggestions that you may have!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is Augustas Macijauskas and I am a recent master’s in machine learning graduate at the University of Cambridge. When not working on LLM interpretability and alignment, I enjoy doing sports and playing a musical instrument."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Cambridge | Cambridge, United Kingdom\nMPhil in Machine Learning | September 2022 - October 2023\nUniversity of Manchester | Manchester, United Kingdom\nBSc Mathematics | September 2019 - June 2022\nKaunas University of Technology Gymnasium | Kaunas, Lithuania\nHigh School Diploma | September 2017 - June 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nBPTI | Vilnius, Lithuania\nMachine Learning Research Assistant | September 2020 - September 2022\nMachine Learning Intern | July 2020 - September 2020\nGenus AI | Vilnius, Lithuania\nData Science Intern | July 2021 - September 2021\nbackpaqr | Vilnius, Lithuania\nSoftware Engineering Intern | June 2019 - August 2019"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Eliciting latent knowledge from language reward models\n\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\nLLMs\n\n\ncambridge\n\n\n\n\nBlog post about my master’s project research at the University of Cambridge.\n\n\n\n\n\n\nOct 4, 2023\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n  \n\n\n\n\nA list of notes on the “ChatGPT Prompt Engineering for Developers” course\n\n\n\n\n\n\n\nprompt engineering\n\n\n\n\nA collection of personal notes.\n\n\n\n\n\n\nSep 13, 2023\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n  \n\n\n\n\nBasic panorama stitching\n\n\n\n\n\n\n\nmachine learning\n\n\ncomputer vision\n\n\n\n\nA look at homographies and how they can be used to stitch photos into a panorama.\n\n\n\n\n\n\nJul 9, 2022\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n  \n\n\n\n\nImplementing optimisation algorithms to optimise parameters of linear functions\n\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\noptimization\n\n\n\n\nA sandbox notebook which I used to play around with optimizers.\n\n\n\n\n\n\nMay 1, 2021\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n  \n\n\n\n\nMulti-output classification for captcha recognition using fastai\n\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\nclassification\n\n\nfastai\n\n\nvision\n\n\n\n\nImplementing a multi-output model to recognize easy captcha images using the fastai library.\n\n\n\n\n\n\nMar 31, 2021\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n  \n\n\n\n\nElegant implementation of Naive Bayes in just 10 lines of code\n\n\n\n\n\n\n\nmachine learning\n\n\nnlp\n\n\n\n\nA tutorial on how to use a few tricks to implement a Naive Bayes model in just a few lines of code.\n\n\n\n\n\n\nFeb 1, 2021\n\n\nAugustas Macijauskas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/captcha-fastai/captcha-fastai.html",
    "href": "posts/captcha-fastai/captcha-fastai.html",
    "title": "Multi-output classification for captcha recognition using fastai",
    "section": "",
    "text": "Special thanks to my dear friends Julius and Laurynas who joined me to work on this project.\nWe worked on and off for more than a month on this project, and I must say that it was not an easy, but definitely an extremely rewarding experience considering how much I’ve learnt. It was also a good chance to hone my problem solving skills and endurance, as we switched approaches (and even libraries!) at least three times, spent substantial amounts of time reading through various resources and codebases and trying to adapt the codes to our purpose through trial and error.\nThe key takeaways are:\n\nDo not ever use old captchas and encourage others not to, as there are too many ways to break them nowadays.\nFastai has a great mid-level API that allows for customization for almost any use, but getting the hang of it might be tricky. Here are some resources that tremendously helped us:\n\nLooking at fastai docs and tutorials and reading through source code\nA walk with fastai2 playlist that has excellent material on using the fastai mid-level API\nfastai v2 walk-thru playlist on YouTube where Jeremy Howard talks how and why the library is built the way it is\n\nPerseverance is key - great ideas and solutions usually do not come overnight, but breaking the problem into smaller pieces and continously improving on each of those is a good way to go."
  },
  {
    "objectID": "posts/captcha-fastai/captcha-fastai.html#check-validation-set-results",
    "href": "posts/captcha-fastai/captcha-fastai.html#check-validation-set-results",
    "title": "Multi-output classification for captcha recognition using fastai",
    "section": "Check validation set results",
    "text": "Check validation set results\nCheck the model’s performance on a batch of data:\n\nlearn.show_results()"
  },
  {
    "objectID": "posts/captcha-fastai/captcha-fastai.html#get-failed-captchas",
    "href": "posts/captcha-fastai/captcha-fastai.html#get-failed-captchas",
    "title": "Multi-output classification for captcha recognition using fastai",
    "section": "Get failed captchas",
    "text": "Get failed captchas\nGet predictions:\n\ninputs, _, targets, decoded_preds = learn.get_preds(with_input=True, with_decoded=True)\ninputs.size(), targets.size(), decoded_preds.size()\n\n\n\n\n(torch.Size([1991, 3, 64, 64]), torch.Size([1991, 4]), torch.Size([1991, 4]))\n\n\nIndices of failed captchas are:\n\nfailed_idxs = (~(decoded_preds == targets).all(dim=1)).nonzero().view(-1)\n\nShow those captchas to check why the model failed:\n\ndls.show_results(b=(inputs[failed_idxs], targets[failed_idxs]), out=decoded_preds[failed_idxs])\n\n\n\n\nWe can see that the model does some slight errors, but it only fails on 4 captchas out of a validation set of 1991 images which is an amazing result. I hope that it is now obvious that using captchas to protect a website from bots is not a good idea.\nCheck the model’s confidence when it makes errors:\n\nfor idx in failed_idxs.detach().cpu().numpy().flatten():\n    decoded_pred, pred, model_output = learn.predict(dls.valid_ds[idx][0])\n    probs = torch.softmax(model_output, dim=0)[pred].diag()\n    print(f\"Actual: {''.join([i2l[v.item()] for v in dls.valid_ds[idx][1]])}, predicted: {decoded_pred}, probs: {probs}\")\n\n\n\n\nActual: W4KN, predicted: WAKN, probs: tensor([0.0805, 0.0574, 0.0800, 0.0804])\nActual: B56M, predicted: B5GM, probs: tensor([0.0775, 0.0805, 0.0517, 0.0805])\nActual: M3AH, predicted: MBAH, probs: tensor([0.0805, 0.0492, 0.0803, 0.0805])\nActual: 3WZP, predicted: 3W2P, probs: tensor([0.0805, 0.0795, 0.0597, 0.0804])\n\n\n\n\n\n\n\n\n\n\n\n\nNote: In each case the probabilities for the failed letters were lower."
  },
  {
    "objectID": "posts/thesis/thesis.html",
    "href": "posts/thesis/thesis.html",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "",
    "text": "Hello, World!\nThis blog post discusses the main ideas behind my thesis for the MPhil in Machine Learning and Machine Intelligence degree at the University of Cambridge. You can read the full thesis here, or check the associated GitHub repository.\nThe main idea behind the project is trying to build a reward models that reward “truthfulness” in a scalable fashion, which current state-of-the-art methods, such as reinforcement learning from human feedback (RLHF), are not capable of (note that we use quatations because we defined “truthfulness” in a narrow sense and mean only the performance on binary question-aswering tasks, see the thesis pdf for more details). Specifically, methods that discover latent knowledge, such as CCS, are used to determine whether a piece of input text is truthful or not. Such linear probes are then combined with pre-trained language models to make up reward models, which are used in reinforcement learning RL fine-tuning to improve the “truthfulness” of large language models (LLMs).\nThese reward models can be trained by using transformed versions of existing datasets, thus relaxing the requirement to collect large numbers of human preference data, as is usual in RLHF. We find that using our reward models along with a few regularization techniques (discussed below) can already be used to improve the “truthfulness” of pre-trained LLMs by up to 1.6%, as measured on the TruthfulQA benchmark. Importantly, such an improvement is achieved without sacrificing the models’ performance on more general NLP tasks (we evaluate on the Open LLM Leaderboard tasks).\nAlthough our method serves as a proof of concept on how hallucinations in LLMs could be tackled in the future, it still has many limitations. For one, the current best DLK methods still have a long way to go in terms of robustness. Moreover, our method only tackles the narrow definition of “truthfulness”, and even though the accuracy on TruthfulQA improves too, many would argue that it is still not a very good proxy for actually reducing levels of hallucination in LLMs. Finally, we found that the pre-trained models that we would fine-tune using RL had to be already quite capable, otherwise our method would not work.\n\n\nThere are four main steps to run the method on new data: 1. Split the dataset and prepare it for reward model training and RL fine-tuning. 1. Train a reward model. 1. Performing RL fine-tuning on some pre-trained LLM. 1. Evaluate the fine-tuned LLM on both target and general NLP tasks.\nSteps 1 and 4 are mostly boring and you can find more details about them in the README of the GitHub repository, so we are going to focus on the theory and main code bits for steps 2 and 3.\n\n\n\nAs discussed in more detail in chapter 3 of the thesis, the reward model is made up of a pre-trained language model with a probe attached at the end.\n\n\n\nThe architecture of the reward model\n\n\nThe reward model takes as an input a question \\(q_i\\) with a binary answer (e.g. “Yes”/“No”), creates a contrastive pair from it and then this contrastive pair \\((x_i^+, x_i^-)\\) is used to compute a reward (a number between 0 and 1). The reward is computed by recording activations of the last token in a layer of a language model, denoted \\(\\mathrm{\\textbf{emb}}(x_i^+)\\) and \\(\\mathrm{\\textbf{emb}}(x_i^-)\\). We would try all layers of a language model and pick the one that worked the best. Finally, the embeddings are passed to a logistic classificer which is of the form: \\[p(q_i) = \\sigma(\\textbf{w}^\\mathrm{T}(\\mathrm{\\textbf{emb}}(x_i^+) - \\mathrm{\\textbf{emb}}(x_i^-)))\\] which is the only module with trainable parameters, the vector \\(\\textbf{w}\\). Here, \\(\\sigma\\) is the sigmoid activation function. This output probability denotes the probability that the question \\(q_i\\) is “truthful” which is what we use as the reward.\nThere are a few other intricacies, such as how to prompt for “truthfulness” (custom prompts are needed), or how to actually find the optimal parameters vector \\(\\textbf{w}\\), but I will sugeest interested readers to refer to the thesis pdf.\n\n\n\nOnce we have a reward model, we can plug into an RL algorithm to perform fine-tuning. We used the proximal policy optimization algorithm, as implement in the Transformer Reinforcement Learning (TRL) library from Hugging Face. We found that a few pieces of regularization had to be applied to stabilize the training process. The tricks are: 1. Prompting - we found that a specialized prompt had to be devised for each model for the method to work (we mostly focused on the 7B Vicuna models). 1. Maximum number of new tokens - we found that setting the number of new tokens to two was enough in our case since answers to our binary questions were short. Additionally, we applied output post-processing to strip any undesirable tokens (see the code below). 1. Encouraging the models to only output in the desired format - we want the models to only respond with “Yes”/“No”, but even with specialized prompts the models would still sometimes generate different responses. To tackle this, we tweaked the reward to be -1 if the model does not respond in the desired format, and we would give the usual score from the reward model if the output was what the model was asked for. This encouraged the model to converge to only responding with the required format over time.\nTo illustrate these concepts, the finel RL training loop looked roughly like the following:\n\nimport torch\nimport string\n\n\nCHARACTERS_TO_FILTER = string.punctuation + \" \\n\"\n\n\ndef is_answer_yes_no(answer):\n    return answer in [\"Yes\", \"No\"]\n\n\ndef postprocess_response(response):\n    while response and response[-1] in CHARACTERS_TO_FILTER:\n        response = response[:-1]\n    return response\n\n\ndef train(\n    ppo_trainer,\n    tokenizer,\n    generation_kwargs,\n    get_rewards,\n    script_args, config,\n):\n    n_epochs = config.steps // len(ppo_trainer.dataloader)\n\n    for epoch in range(1, n_epochs + 1):\n        loop = tqdm(\n            enumerate(ppo_trainer.dataloader, 1),\n            total=len(ppo_trainer.dataloader), leave=False\n        )\n        for batch_idx, batch in loop:\n            # Get the input tensors\n            question_tensors = batch[\"input_ids\"]\n\n            # Get the generations\n            response_tensors = ppo_trainer.generate(\n                question_tensors,\n                return_prompt=False,\n                batch_size=script_args.generator_batch_size,\n                **generation_kwargs,\n            )\n            responses = tokenizer.batch_decode(\n                response_tensors, skip_special_tokens=True,\n                spaces_between_special_tokens=False\n            )\n\n            # Postprocess the responses\n            if script_args.postprocess_responses:\n                responses = [postprocess_response(x) for x in responses]\n            batch[\"response\"] = responses\n\n            # Compute the rewards (scores)\n            texts = [q + \" \" + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n            rewards = get_rewards(texts)\n\n            # Replace reward for undesired answers to -1\n            mask = [not is_answer_yes_no(x) for x in batch[\"response\"]]\n            mask = torch.tensor(mask, dtype=torch.bool) # cast to tensor\n            rewards[mask] = -1\n\n            # Make the rewards a list of tensors\n            rewards = [x for x in rewards]\n\n            # Run PPO step\n            stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n            ppo_trainer.log_stats(stats, batch, rewards)\n\nAnd the generation_kwargs look like the following:\n\ngeneration_kwargs = {\n    \"top_k\": 0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.pad_token_id,\n    \"eos_token_id\": 100_000,\n    \"pad_to_multiple_of\": 8,\n    \"max_new_tokens\": 2,\n}\n\n\n\n\nI hope you found this blog as interesting as it was for me to work on this project. I feel like I have learnt a lot during it, for example, I joined multiple ML communities and got involved in discussions with very smart and ambitious people. Perhaps my proudest achievement is making my first open-source contribution to the elk library (link), as well as reported multiple bugs to the big-refactor branch of Language Model Evaluation Harness (link).\nI am excited to dive deeper into LLMs-related topics in the future. Feel free to reach out if you have any opportunities on offer!"
  },
  {
    "objectID": "posts/thesis/thesis.html#main-steps",
    "href": "posts/thesis/thesis.html#main-steps",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "",
    "text": "There are four main steps to run the method on new data: 1. Split the dataset and prepare it for reward model training and RL fine-tuning. 1. Train a reward model. 1. Performing RL fine-tuning on some pre-trained LLM. 1. Evaluate the fine-tuned LLM on both target and general NLP tasks.\nSteps 1 and 4 are mostly boring and you can find more details about them in the README of the GitHub repository, so we are going to focus on the theory and main code bits for steps 2 and 3."
  },
  {
    "objectID": "posts/thesis/thesis.html#reward-model-training",
    "href": "posts/thesis/thesis.html#reward-model-training",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "",
    "text": "As discussed in more detail in chapter 3 of the thesis, the reward model is made up of a pre-trained language model with a probe attached at the end.\n\n\n\nThe architecture of the reward model\n\n\nThe reward model takes as an input a question \\(q_i\\) with a binary answer (e.g. “Yes”/“No”), creates a contrastive pair from it and then this contrastive pair \\((x_i^+, x_i^-)\\) is used to compute a reward (a number between 0 and 1). The reward is computed by recording activations of the last token in a layer of a language model, denoted \\(\\mathrm{\\textbf{emb}}(x_i^+)\\) and \\(\\mathrm{\\textbf{emb}}(x_i^-)\\). We would try all layers of a language model and pick the one that worked the best. Finally, the embeddings are passed to a logistic classificer which is of the form: \\[p(q_i) = \\sigma(\\textbf{w}^\\mathrm{T}(\\mathrm{\\textbf{emb}}(x_i^+) - \\mathrm{\\textbf{emb}}(x_i^-)))\\] which is the only module with trainable parameters, the vector \\(\\textbf{w}\\). Here, \\(\\sigma\\) is the sigmoid activation function. This output probability denotes the probability that the question \\(q_i\\) is “truthful” which is what we use as the reward.\nThere are a few other intricacies, such as how to prompt for “truthfulness” (custom prompts are needed), or how to actually find the optimal parameters vector \\(\\textbf{w}\\), but I will sugeest interested readers to refer to the thesis pdf."
  },
  {
    "objectID": "posts/thesis/thesis.html#rl-fine-tuning",
    "href": "posts/thesis/thesis.html#rl-fine-tuning",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "",
    "text": "Once we have a reward model, we can plug into an RL algorithm to perform fine-tuning. We used the proximal policy optimization algorithm, as implement in the Transformer Reinforcement Learning (TRL) library from Hugging Face. We found that a few pieces of regularization had to be applied to stabilize the training process. The tricks are: 1. Prompting - we found that a specialized prompt had to be devised for each model for the method to work (we mostly focused on the 7B Vicuna models). 1. Maximum number of new tokens - we found that setting the number of new tokens to two was enough in our case since answers to our binary questions were short. Additionally, we applied output post-processing to strip any undesirable tokens (see the code below). 1. Encouraging the models to only output in the desired format - we want the models to only respond with “Yes”/“No”, but even with specialized prompts the models would still sometimes generate different responses. To tackle this, we tweaked the reward to be -1 if the model does not respond in the desired format, and we would give the usual score from the reward model if the output was what the model was asked for. This encouraged the model to converge to only responding with the required format over time.\nTo illustrate these concepts, the finel RL training loop looked roughly like the following:\n\nimport torch\nimport string\n\n\nCHARACTERS_TO_FILTER = string.punctuation + \" \\n\"\n\n\ndef is_answer_yes_no(answer):\n    return answer in [\"Yes\", \"No\"]\n\n\ndef postprocess_response(response):\n    while response and response[-1] in CHARACTERS_TO_FILTER:\n        response = response[:-1]\n    return response\n\n\ndef train(\n    ppo_trainer,\n    tokenizer,\n    generation_kwargs,\n    get_rewards,\n    script_args, config,\n):\n    n_epochs = config.steps // len(ppo_trainer.dataloader)\n\n    for epoch in range(1, n_epochs + 1):\n        loop = tqdm(\n            enumerate(ppo_trainer.dataloader, 1),\n            total=len(ppo_trainer.dataloader), leave=False\n        )\n        for batch_idx, batch in loop:\n            # Get the input tensors\n            question_tensors = batch[\"input_ids\"]\n\n            # Get the generations\n            response_tensors = ppo_trainer.generate(\n                question_tensors,\n                return_prompt=False,\n                batch_size=script_args.generator_batch_size,\n                **generation_kwargs,\n            )\n            responses = tokenizer.batch_decode(\n                response_tensors, skip_special_tokens=True,\n                spaces_between_special_tokens=False\n            )\n\n            # Postprocess the responses\n            if script_args.postprocess_responses:\n                responses = [postprocess_response(x) for x in responses]\n            batch[\"response\"] = responses\n\n            # Compute the rewards (scores)\n            texts = [q + \" \" + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n            rewards = get_rewards(texts)\n\n            # Replace reward for undesired answers to -1\n            mask = [not is_answer_yes_no(x) for x in batch[\"response\"]]\n            mask = torch.tensor(mask, dtype=torch.bool) # cast to tensor\n            rewards[mask] = -1\n\n            # Make the rewards a list of tensors\n            rewards = [x for x in rewards]\n\n            # Run PPO step\n            stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n            ppo_trainer.log_stats(stats, batch, rewards)\n\nAnd the generation_kwargs look like the following:\n\ngeneration_kwargs = {\n    \"top_k\": 0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.pad_token_id,\n    \"eos_token_id\": 100_000,\n    \"pad_to_multiple_of\": 8,\n    \"max_new_tokens\": 2,\n}"
  },
  {
    "objectID": "posts/thesis/thesis.html#conclusion",
    "href": "posts/thesis/thesis.html#conclusion",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "",
    "text": "I hope you found this blog as interesting as it was for me to work on this project. I feel like I have learnt a lot during it, for example, I joined multiple ML communities and got involved in discussions with very smart and ambitious people. Perhaps my proudest achievement is making my first open-source contribution to the elk library (link), as well as reported multiple bugs to the big-refactor branch of Language Model Evaluation Harness (link).\nI am excited to dive deeper into LLMs-related topics in the future. Feel free to reach out if you have any opportunities on offer!"
  }
]