[
  {
    "objectID": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html",
    "href": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html",
    "title": "Cubic spline interpolation exercise",
    "section": "",
    "text": "I somehow stumbled upon cubic spline interpolation quite accidentally, but it caught my interest with all of the neat linear algebra and scientific computing techniques behind it. This got me excited to revise materials from my undergrad classes and try to implement the method in Python from scratch.\nResources:\n\nThe scipy scipy implementation of the method.\nCubic Spline Interpolation on Wikiversity.\n\n\n\nToggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom scipy.interpolate import CubicSpline\n\n# Ensures we can render plotly plots with quarto\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n\n\n\n\n\n\nx_train = np.linspace(0, 10, 11)\ny_train = np.sin(x_train) + np.random.normal(0, 0.5, 11)\n\n\ncs = CubicSpline(x_train, y_train, bc_type=\"natural\", extrapolate=True)\n\nx_test = np.linspace(-1, 11, 121)\ny_test = cs(x_test)\n\n\n# Create a figure\nfig = go.Figure()\n\n# Add scatter plot\nfig.add_trace(go.Scatter(x=x_train, y=y_train, mode='markers', name='Original points'))\n\n# Add line plot\nfig.add_trace(go.Scatter(x=x_test, y=y_test, mode='lines', name='Fitted line'))\n\n# Add titles and labels\nfig.update_layout(title=\"Scipy interpolation\", xaxis_title=\"x\", yaxis_title=\"y\")\n\n# Show the plot\nfig.show()\n\n                                                \n\n\n\n\n\n\ndef tridiagonal_linear_system_solver(d_lower, d_main, d_upper, b):\n    \"\"\"\n    Solve a tridiagonal linear system given the main, lower, and upper diagonals, as well as the vector b. The Thomas\n    algorithm is used.\n    \"\"\"\n\n    n = len(d_main)\n\n    # Forward sweep\n    for i in range(1, n):\n        w = d_lower[i - 1] / d_main[i - 1]\n        d_main[i] -= w * d_upper[i - 1]\n        b[i] -= w * b[i - 1]\n\n    # Back substitution\n    x = np.zeros(n)\n    x[-1] = b[-1] / d_main[-1]\n    for i in range(n - 2, -1, -1):\n        x[i] = (b[i] - d_upper[i] * x[i + 1]) / d_main[i]\n\n    return x\n\n\nclass CubicSplineCustom:\n\n    def __init__(self, x, y):\n        if not np.all(np.diff(x) &gt; 0):\n            raise ValueError(\"x values must be in ascending order.\")\n\n        self.x = np.array(x)\n        self.y = np.array(y)\n        self._find_coeffs()\n\n    def _find_coeffs(self):\n        # Find the relevant diagonals and from self.x and self.y assuming natural boundary conditions\n        d_lower, d_main, d_upper, b, h = self._compute_diagonals_and_b()\n\n        # Compute the M_i's for i = 1, n-1 (since M_0 and M_n are assumed to be 0). This will require solving a\n        # tridiagonal linear system\n        ms = tridiagonal_linear_system_solver(d_lower, d_main, d_upper, b)\n\n        # Compute the coefficients of the cubic polynomials\n        self.c = self._compute_coefficients_from_second_derivatives(ms, h)\n\n    def _compute_diagonals_and_b(self):\n        x, y = self.x, self.y\n        h = np.diff(x)\n\n        # Compute the diagonals for the tridiagonal matrix\n        d_lower = h.copy()\n        d_lower[-1] = 0  # one of the naturals BCs\n        d_upper = h.copy()\n        d_upper[0] = 0\n\n        d_main = 2 * np.ones(len(x))\n        d_main[1:-1] *= h[:-1] + h[1:]\n\n        b = np.zeros(len(x))\n        y_diff = np.diff(y)\n        b[1:-1] = 6 * (y_diff[1:] / h[1:] - y_diff[:-1] / h[:-1])\n\n        return d_lower, d_main, d_upper, b, h\n\n    def _compute_coefficients_from_second_derivatives(self, ms, h):\n        coeffs = np.zeros((4, len(self.x) - 1))\n        coeffs[0, :] = (ms[1:] - ms[:-1]) / (6 * h)\n        coeffs[1, :] = ms[:-1] / 2\n        coeffs[2, :] = (self.y[1:] - self.y[:-1]) / h - (ms[1:] + 2 * ms[:-1]) * h / 6\n        coeffs[3, :] = self.y[:-1]\n\n        return coeffs\n\n    def _get_index(self, x):\n        \"\"\"Performs binary search\"\"\"\n        low, high = 0, len(self.x) - 1\n\n        while low &lt; high:\n            mid = (low + high) // 2\n            if self.x[mid] &lt;= x:\n                low = mid + 1\n            else:\n                high = mid\n\n        return min(max(low - 1, 0), len(self.x) - 2)\n\n    def interpolate(self, x: float):\n        # Find which polynomial is appropriate and evaluate at x\n        idx = self._get_index(x)\n        dx = x - self.x[idx]\n        return (self.c[0, idx] * dx + self.c[1, idx]) * dx**2 + self.c[2, idx] * dx + self.c[3, idx]\n\n\ncustom_spline = CubicSplineCustom(x_train, y_train)\n\nassert cs.c.shape == custom_spline.c.shape\nassert np.allclose(cs.c, custom_spline.c)\nnp.sqrt(((cs.c - custom_spline.c) ** 2).mean())\n\n2.4075003542614415e-16\n\n\n\nassert np.allclose(custom_spline.interpolate(-1), cs(-1))\nassert np.allclose(custom_spline.interpolate(86500), cs(86500))\n\n\n# Create a figure\nfig = go.Figure()\n\n# Add scatter plot\nfig.add_trace(go.Scatter(x=x_train, y=y_train, mode=\"markers\", name=\"Original data\"))\n\n# Add line plot\nfig.add_trace(go.Scatter(x=x_test, y=y_test, mode=\"lines\", name=\"Fitted line\"))\n\ny_test_custom = [custom_spline.interpolate(x) for x in x_test]\nassert np.allclose(y_test, y_test_custom)\nfig.add_trace(\n    go.Scatter(x=x_test, y=y_test_custom, mode=\"lines\", name=\"Fitted line (custom)\", line=dict(dash=\"longdash\"))\n)\n\n# Add titles and labels\nfig.update_layout(title=\"Scipy and custom interpolations\", xaxis_title=\"x\", yaxis_title=\"y\")\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#imports",
    "href": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#imports",
    "title": "Cubic spline interpolation exercise",
    "section": "",
    "text": "Toggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom scipy.interpolate import CubicSpline\n\n# Ensures we can render plotly plots with quarto\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\""
  },
  {
    "objectID": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#cubic-spline-interpolation-with-scipy",
    "href": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#cubic-spline-interpolation-with-scipy",
    "title": "Cubic spline interpolation exercise",
    "section": "",
    "text": "x_train = np.linspace(0, 10, 11)\ny_train = np.sin(x_train) + np.random.normal(0, 0.5, 11)\n\n\ncs = CubicSpline(x_train, y_train, bc_type=\"natural\", extrapolate=True)\n\nx_test = np.linspace(-1, 11, 121)\ny_test = cs(x_test)\n\n\n# Create a figure\nfig = go.Figure()\n\n# Add scatter plot\nfig.add_trace(go.Scatter(x=x_train, y=y_train, mode='markers', name='Original points'))\n\n# Add line plot\nfig.add_trace(go.Scatter(x=x_test, y=y_test, mode='lines', name='Fitted line'))\n\n# Add titles and labels\nfig.update_layout(title=\"Scipy interpolation\", xaxis_title=\"x\", yaxis_title=\"y\")\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#custom-implementation-of-cubic-spline-interpolation",
    "href": "notes/cubic-spline-interpolation-exercise/cubic-spline-interpolation-exercise.html#custom-implementation-of-cubic-spline-interpolation",
    "title": "Cubic spline interpolation exercise",
    "section": "",
    "text": "def tridiagonal_linear_system_solver(d_lower, d_main, d_upper, b):\n    \"\"\"\n    Solve a tridiagonal linear system given the main, lower, and upper diagonals, as well as the vector b. The Thomas\n    algorithm is used.\n    \"\"\"\n\n    n = len(d_main)\n\n    # Forward sweep\n    for i in range(1, n):\n        w = d_lower[i - 1] / d_main[i - 1]\n        d_main[i] -= w * d_upper[i - 1]\n        b[i] -= w * b[i - 1]\n\n    # Back substitution\n    x = np.zeros(n)\n    x[-1] = b[-1] / d_main[-1]\n    for i in range(n - 2, -1, -1):\n        x[i] = (b[i] - d_upper[i] * x[i + 1]) / d_main[i]\n\n    return x\n\n\nclass CubicSplineCustom:\n\n    def __init__(self, x, y):\n        if not np.all(np.diff(x) &gt; 0):\n            raise ValueError(\"x values must be in ascending order.\")\n\n        self.x = np.array(x)\n        self.y = np.array(y)\n        self._find_coeffs()\n\n    def _find_coeffs(self):\n        # Find the relevant diagonals and from self.x and self.y assuming natural boundary conditions\n        d_lower, d_main, d_upper, b, h = self._compute_diagonals_and_b()\n\n        # Compute the M_i's for i = 1, n-1 (since M_0 and M_n are assumed to be 0). This will require solving a\n        # tridiagonal linear system\n        ms = tridiagonal_linear_system_solver(d_lower, d_main, d_upper, b)\n\n        # Compute the coefficients of the cubic polynomials\n        self.c = self._compute_coefficients_from_second_derivatives(ms, h)\n\n    def _compute_diagonals_and_b(self):\n        x, y = self.x, self.y\n        h = np.diff(x)\n\n        # Compute the diagonals for the tridiagonal matrix\n        d_lower = h.copy()\n        d_lower[-1] = 0  # one of the naturals BCs\n        d_upper = h.copy()\n        d_upper[0] = 0\n\n        d_main = 2 * np.ones(len(x))\n        d_main[1:-1] *= h[:-1] + h[1:]\n\n        b = np.zeros(len(x))\n        y_diff = np.diff(y)\n        b[1:-1] = 6 * (y_diff[1:] / h[1:] - y_diff[:-1] / h[:-1])\n\n        return d_lower, d_main, d_upper, b, h\n\n    def _compute_coefficients_from_second_derivatives(self, ms, h):\n        coeffs = np.zeros((4, len(self.x) - 1))\n        coeffs[0, :] = (ms[1:] - ms[:-1]) / (6 * h)\n        coeffs[1, :] = ms[:-1] / 2\n        coeffs[2, :] = (self.y[1:] - self.y[:-1]) / h - (ms[1:] + 2 * ms[:-1]) * h / 6\n        coeffs[3, :] = self.y[:-1]\n\n        return coeffs\n\n    def _get_index(self, x):\n        \"\"\"Performs binary search\"\"\"\n        low, high = 0, len(self.x) - 1\n\n        while low &lt; high:\n            mid = (low + high) // 2\n            if self.x[mid] &lt;= x:\n                low = mid + 1\n            else:\n                high = mid\n\n        return min(max(low - 1, 0), len(self.x) - 2)\n\n    def interpolate(self, x: float):\n        # Find which polynomial is appropriate and evaluate at x\n        idx = self._get_index(x)\n        dx = x - self.x[idx]\n        return (self.c[0, idx] * dx + self.c[1, idx]) * dx**2 + self.c[2, idx] * dx + self.c[3, idx]\n\n\ncustom_spline = CubicSplineCustom(x_train, y_train)\n\nassert cs.c.shape == custom_spline.c.shape\nassert np.allclose(cs.c, custom_spline.c)\nnp.sqrt(((cs.c - custom_spline.c) ** 2).mean())\n\n2.4075003542614415e-16\n\n\n\nassert np.allclose(custom_spline.interpolate(-1), cs(-1))\nassert np.allclose(custom_spline.interpolate(86500), cs(86500))\n\n\n# Create a figure\nfig = go.Figure()\n\n# Add scatter plot\nfig.add_trace(go.Scatter(x=x_train, y=y_train, mode=\"markers\", name=\"Original data\"))\n\n# Add line plot\nfig.add_trace(go.Scatter(x=x_test, y=y_test, mode=\"lines\", name=\"Fitted line\"))\n\ny_test_custom = [custom_spline.interpolate(x) for x in x_test]\nassert np.allclose(y_test, y_test_custom)\nfig.add_trace(\n    go.Scatter(x=x_test, y=y_test_custom, mode=\"lines\", name=\"Fitted line (custom)\", line=dict(dash=\"longdash\"))\n)\n\n# Add titles and labels\nfig.update_layout(title=\"Scipy and custom interpolations\", xaxis_title=\"x\", yaxis_title=\"y\")\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is Augustas Macijauskas and I am a recent master’s in machine learning graduate at the University of Cambridge. When not working on LLM interpretability and alignment, I enjoy doing sports and playing a musical instrument."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Cambridge | Cambridge, United Kingdom\nMPhil in Machine Learning | September 2022 - October 2023\nUniversity of Manchester | Manchester, United Kingdom\nBSc Mathematics | September 2019 - June 2022\nKaunas University of Technology Gymnasium | Kaunas, Lithuania\nHigh School Diploma | September 2017 - June 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nBPTI | Vilnius, Lithuania\nMachine Learning Research Assistant | September 2020 - September 2022\nMachine Learning Intern | July 2020 - September 2020\nGenus AI | Vilnius, Lithuania\nData Science Intern | July 2021 - September 2021\nbackpaqr | Vilnius, Lithuania\nSoftware Engineering Intern | June 2019 - August 2019"
  },
  {
    "objectID": "posts/optimization-algorithms/optimisation-algorithms.html",
    "href": "posts/optimization-algorithms/optimisation-algorithms.html",
    "title": "Implementing optimisation algorithms to optimise parameters of linear functions",
    "section": "",
    "text": "Imports\nToggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nData\nLet’s create 100 x values uniformly distributed between 0 and 100. Note the usage of a random seed for reproducibility:\n\nnp.random.seed(0)\nx = np.random.uniform(0, 100, size=100)\n\nCalculate y values for the given x values. I chose a slope of -2 and an intercept of 10 and decided not to add noise to make the task easier:\n\nslope = -2\nintercept = 10\ny = slope * x + intercept\n\nLet’s plot the points to check that everything looks as it should be:\n\nplt.scatter(x, y)\nplt.plot(x, slope * x + intercept, color=\"red\");\n\n\n\n\n\n\n\n\n\n\nPrepare for the experiments\nLet’s create some classes and functions that will help us to quickly experiment with different optimization algorithms.\nWe start by creating our simple LinearModel whose parameters we’ll try to optimize. Here we choose to initialize both the slope and the intercept with the value 1, but this should not matter too much for this problem.\nThe forward method performs the forward pass of the model to get a prediction for a given x, i.e. just return ax+b.\nThe backward method is used to calculate gradients of the parameters given x, y and y_pred. These are usually calculated using automatic differentiation in deep learning libraries, such as PyTorch, but here we do everything by hand:\n\nclass LinearModel:\n    def __init__(self, initial_slope=1, initial_intercept=1):\n        self.slope = initial_slope\n        self.intercept = initial_intercept\n    \n    def forward(self, x):\n        return self.slope * x + self.intercept\n    \n    def backward(self, x, y, y_pred):\n        return x * (y_pred - y), (y_pred - y)\n\nThe Tracker utility class will be used to track different values while experimenting. It can be considered an overkill for such a simple task (and I later ended up tracking only one value with it), but it can be very convenient for larger projects, so I included it here too.\nThe class stores a dictionary of string-list pairs in its state and provides add_record and get_values methods to easily access and update that dictionary.\n\nfrom collections import defaultdict\n\nclass Tracker:\n    def __init__(self):\n        self.tracker = defaultdict(list)\n    \n    def add_record(self, key, value):\n        self.tracker[key].append(value)\n    \n    def get_values(self, key):\n        if not key in self.tracker: raise KeyError(f\"Key {key} not in tracker\")\n\n        return self.tracker[key]\n\ntrain_one_epoch is a utility function that trains the model for one epoch (one pass through the data). It performs a forward pass, calculates and tracks the error, gets the gradients of parameters by performing a backward pass and passes those along with the params parameter to the optimizer that updates the weights of the model. The params parameter is a dictionary that is used to hyperparameter constants, such as the learning rate, to the optimizer.\n\ndef train_one_epoch(model, data, optimizer, tracker, params):\n    total_error = 0.0\n    \n    for x, y in data:\n        # Forward pass\n        y_pred = model.forward(x)\n        \n        # Calculate error\n        error = 0.5 * (y_pred - y) ** 2\n        total_error += error\n\n        # Backward pass\n        grad_slope, grad_intercept = model.backward(x, y, y_pred)\n        \n        # Step the optimizer\n        params[\"step\"] += 1\n        optimizer.step(model, grad_slope, grad_intercept, params)\n\n    tracker.add_record(\"error\", total_error / len(data))\n\nFinally, a utility function that brings everything together to execute an experiment. Our goal is to train the model for as little as possible, stopping when error drops below some desired threshold.\n\ndef perform_experiment(optimizer, params, desired_error=1e-8):\n    # Initialization\n    model = LinearModel()\n    data = list(zip(x, y))\n    tracker = Tracker()\n    \n    params[\"step\"] = 0\n    while True:\n        # Train for an epoch\n        train_one_epoch(model, data, optimizer, tracker, params)\n        \n        # Check if we reached desired accuracy\n        if tracker.get_values(\"error\")[-1] &lt;= desired_error:\n            break\n        \n        # Improper hyperparameter values might cause the models to diverge,\n        # so we include this here to stop execution if this happens\n        if np.isnan(tracker.get_values(\"error\")[-1]):\n            break\n    \n    # Print results and plot error over time\n    steps = params[\"step\"]\n    print(f\"Epochs: {steps // len(data)}\")\n    print(f\"Batches (of size 1): {steps}\")\n    print(f\"Slope: {model.slope}\")\n    print(f\"Intercept: {model.intercept}\")\n    plt.plot(tracker.get_values(\"error\"), linestyle=\"dotted\")\n    \n    return params[\"step\"] // len(data), params[\"step\"], model.slope, model.intercept\n\n\n\nVanilla SGD\n\nclass SGDOptimizer:\n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        model.slope -= lr * grad_slope\n        model.intercept -= lr * grad_intercept\n\n\noptimizer = SGDOptimizer()\nparams = { \"lr\": 1e-4 }\nperform_experiment(optimizer, params);\n\nEpochs: 3893\nBatches (of size 1): 389300\nSlope: -1.9999963788900519\nIntercept: 9.999738575620166\n\n\n\n\n\n\n\n\n\nBreaks with lr=1e-3\n\n\nSGD with momentum\n\nclass SGDWithMomentumOptimizer(SGDOptimizer):\n    def __init__(self):\n        self.slope_momentum = self.intercept_momentum = 0\n    \n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        miu = params[\"miu\"]\n        self.slope_momentum = miu * self.slope_momentum + (1 - miu) * grad_slope\n        model.slope -= lr * self.slope_momentum\n\n        self.intercept_momentum = miu * self.intercept_momentum + (1 - miu) * grad_intercept\n        model.intercept -= lr * self.intercept_momentum\n\n\noptimizer = SGDWithMomentumOptimizer()\nparams = { \"lr\": 1e-4, \"miu\": 0.9 }\nperform_experiment(optimizer, params);\n\nEpochs: 3844\nBatches (of size 1): 384400\nSlope: -1.9999951526911044\nIntercept: 9.999742134759243\n\n\n\n\n\n\n\n\n\n\noptimizer = SGDWithMomentumOptimizer()\nparams = { \"lr\": 1e-3, \"miu\": 0.9 }\nperform_experiment(optimizer, params);\n\nEpochs: 365\nBatches (of size 1): 36500\nSlope: -1.9999980020599053\nIntercept: 9.999929876584854\n\n\n\n\n\n\n\n\n\n\n\nRMSprop\n\nclass RMSpropOptimizer(SGDOptimizer):\n    def __init__(self, eps=1e-8):\n        self.eps = eps\n        self.slope_squared_grads = self.intercept_squared_grads = 0\n    \n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        decay_rate = params[\"decay_rate\"]\n        self.slope_squared_grads = decay_rate * self.slope_squared_grads + (1 - decay_rate) * grad_slope ** 2\n        model.slope -= lr * grad_slope / (np.sqrt(self.slope_squared_grads) + self.eps)\n\n        self.intercept_squared_grads = decay_rate * self.intercept_squared_grads + (1 - decay_rate) * grad_intercept ** 2\n        model.intercept -= lr * grad_intercept / (np.sqrt(self.intercept_squared_grads) + self.eps)\n\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 1e-4, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 2765\nBatches (of size 1): 276500\nSlope: -1.9999999858447193\nIntercept: 9.999996406289089\n\n\n\n\n\n\n\n\n\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 1e-3, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 302\nBatches (of size 1): 30200\nSlope: -1.9999998782649515\nIntercept: 9.999976783670757\n\n\n\n\n\n\n\n\n\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 3e-3, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 113\nBatches (of size 1): 11300\nSlope: -2.0000001115561656\nIntercept: 10.000020816756175\n\n\n\n\n\n\n\n\n\nWorks best with lr=3e-3 and decay_rate=0.99.\n\noptimizer = RMSpropOptimizer()\nparams = { \"lr\": 1e1, \"decay_rate\": 0.99 }\nperform_experiment(optimizer, params);\n\nEpochs: 83323\nBatches (of size 1): 8332300\nSlope: -2.000000169655216\nIntercept: 10.000030570050711\n\n\n\n\n\n\n\n\n\n\n\nAdam\n\nclass Adam(SGDOptimizer):\n    def __init__(self, eps=1e-8):\n        self.eps = eps\n        \n        self.slope_momentum = self.intercept_momentum = 0\n        self.slope_squared_grads = self.intercept_squared_grads = 0\n    \n    def step(self, model, grad_slope, grad_intercept, params):\n        lr = params[\"lr\"]\n        beta1, beta2 = params[\"betas\"]\n        step = params[\"step\"]\n        \n        self.slope_momentum = beta1 * self.slope_momentum + (1 - beta1) * grad_slope\n        slope_corrected_momentum = self.slope_momentum / (1 - beta1 ** step)\n        self.slope_squared_grads = beta2 * self.slope_squared_grads + (1 - beta2) * grad_slope ** 2\n        slope_corrected_squared_grads = self.slope_squared_grads / (1 - beta2 ** step)\n        model.slope -= lr * slope_corrected_momentum / (np.sqrt(slope_corrected_squared_grads) + self.eps)\n\n        self.intercept_momentum = beta1 * self.intercept_momentum + (1 - beta1) * grad_intercept\n        intercept_corrected_momentum = self.intercept_momentum / (1 - beta1 ** step)\n        self.intercept_squared_grads = beta2 * self.intercept_squared_grads + (1 - beta2) * grad_intercept ** 2\n        intercept_corrected_squared_grads = self.intercept_squared_grads / (1 - beta2 ** step)\n        model.intercept -= lr * intercept_corrected_momentum / (np.sqrt(intercept_corrected_squared_grads) + self.eps)\n\n\noptimizer = Adam()\nparams = { \"lr\": 1e-4, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params);\n\nEpochs: 2848\nBatches (of size 1): 284800\nSlope: -1.9999980335219065\nIntercept: 9.999814585095613\n\n\n\n\n\n\n\n\n\n\noptimizer = Adam()\nparams = { \"lr\": 1e0, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params);\n\nEpochs: 10\nBatches (of size 1): 1000\nSlope: -2.00000001786048\nIntercept: 10.000001361061178\n\n\n\n\n\n\n\n\n\nAdam seems to be of the order of 10 times faster than other optimization methods and works with much greater learning rates\n\noptimizer = Adam()\nparams = { \"lr\": 1e8, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params);\n\nEpochs: 43\nBatches (of size 1): 4300\nSlope: -2.000001151784137\nIntercept: 9.999987336010909\n\n\n\n\n\n\n\n\n\nIn this very easy case of parameters of a linear curve estimation it works with learning rates as large as 1e8, whereas the largest learning rate that still works among other methods is 1e1 for RMSprop. However, the graphs there showed that oscillations in the error were all over the place, while Adam barely oscillated at all.\n\noptimizer = Adam()\nparams = { \"lr\": 6e-2, \"betas\": (0.9, 0.999) }\nperform_experiment(optimizer, params, 1e-32);\n\nEpochs: 72\nBatches (of size 1): 7200\nSlope: -2.0\nIntercept: 10.0\n\n\n\n\n\n\n\n\n\nAlso, Adam is the only optimizer that was able to achieve an error as little as 1e-32. Even though out of the box the number of steps (in epochs) was of the same order as for other optimization methods, some hyperparameter tinkering allowed to top them by reducing the number of steps to 72."
  },
  {
    "objectID": "posts/thesis/thesis.html",
    "href": "posts/thesis/thesis.html",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "",
    "text": "Hello, World!\nThis blog post discusses the main ideas behind my thesis for the MPhil in Machine Learning and Machine Intelligence degree at the University of Cambridge. You can read the full thesis here, or check the associated GitHub repository.\nThe main idea behind the project is trying to build a reward models that reward “truthfulness” in a scalable fashion, which current state-of-the-art methods, such as reinforcement learning from human feedback (RLHF), are not capable of (note that we use quatations because we defined “truthfulness” in a narrow sense and mean only the performance on binary question-aswering tasks, see the thesis pdf for more details). Specifically, methods that discover latent knowledge, such as CCS, are used to determine whether a piece of input text is truthful or not. Such linear probes are then combined with pre-trained language models to make up reward models, which are used in reinforcement learning RL fine-tuning to improve the “truthfulness” of large language models (LLMs).\nThese reward models can be trained by using transformed versions of existing datasets, thus relaxing the requirement to collect large numbers of human preference data, as is usual in RLHF. We find that using our reward models along with a few regularization techniques (discussed below) can already be used to improve the “truthfulness” of pre-trained LLMs by up to 1.6%, as measured on the TruthfulQA benchmark. Importantly, such an improvement is achieved without sacrificing the models’ performance on more general NLP tasks (we evaluate on the Open LLM Leaderboard tasks).\nAlthough our method serves as a proof of concept on how hallucinations in LLMs could be tackled in the future, it still has many limitations. For one, the current best DLK methods still have a long way to go in terms of robustness. Moreover, our method only tackles the narrow definition of “truthfulness”, and even though the accuracy on TruthfulQA improves too, many would argue that it is still not a very good proxy for actually reducing levels of hallucination in LLMs. Finally, we found that the pre-trained models that we would fine-tune using RL had to be already quite capable, otherwise our method would not work."
  },
  {
    "objectID": "posts/thesis/thesis.html#main-steps",
    "href": "posts/thesis/thesis.html#main-steps",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "Main steps",
    "text": "Main steps\nThere are four main steps to run the method on new data: 1. Split the dataset and prepare it for reward model training and RL fine-tuning. 1. Train a reward model. 1. Performing RL fine-tuning on some pre-trained LLM. 1. Evaluate the fine-tuned LLM on both target and general NLP tasks.\nSteps 1 and 4 are mostly boring and you can find more details about them in the README of the GitHub repository, so we are going to focus on the theory and main code bits for steps 2 and 3."
  },
  {
    "objectID": "posts/thesis/thesis.html#reward-model-training",
    "href": "posts/thesis/thesis.html#reward-model-training",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "Reward model training",
    "text": "Reward model training\nAs discussed in more detail in chapter 3 of the thesis, the reward model is made up of a pre-trained language model with a probe attached at the end.\n\n\n\nThe architecture of the reward model\n\n\nThe reward model takes as an input a question \\(q_i\\) with a binary answer (e.g. “Yes”/“No”), creates a contrastive pair from it and then this contrastive pair \\((x_i^+, x_i^-)\\) is used to compute a reward (a number between 0 and 1). The reward is computed by recording activations of the last token in a layer of a language model, denoted \\(\\mathrm{\\textbf{emb}}(x_i^+)\\) and \\(\\mathrm{\\textbf{emb}}(x_i^-)\\). We would try all layers of a language model and pick the one that worked the best. Finally, the embeddings are passed to a logistic classificer which is of the form: \\[p(q_i) = \\sigma(\\textbf{w}^\\mathrm{T}(\\mathrm{\\textbf{emb}}(x_i^+) - \\mathrm{\\textbf{emb}}(x_i^-)))\\] which is the only module with trainable parameters, the vector \\(\\textbf{w}\\). Here, \\(\\sigma\\) is the sigmoid activation function. This output probability denotes the probability that the question \\(q_i\\) is “truthful” which is what we use as the reward.\nThere are a few other intricacies, such as how to prompt for “truthfulness” (custom prompts are needed), or how to actually find the optimal parameters vector \\(\\textbf{w}\\), but I will sugeest interested readers to refer to the thesis pdf."
  },
  {
    "objectID": "posts/thesis/thesis.html#rl-fine-tuning",
    "href": "posts/thesis/thesis.html#rl-fine-tuning",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "RL fine-tuning",
    "text": "RL fine-tuning\nOnce we have a reward model, we can plug into an RL algorithm to perform fine-tuning. We used the proximal policy optimization algorithm, as implement in the Transformer Reinforcement Learning (TRL) library from Hugging Face. We found that a few pieces of regularization had to be applied to stabilize the training process. The tricks are:\n\nPrompting - we found that a specialized prompt had to be devised for each model for the method to work (we mostly focused on the 7B Vicuna models).\nMaximum number of new tokens - we found that setting the number of new tokens to two was enough in our case since answers to our binary questions were short. Additionally, we applied output post-processing to strip any undesirable tokens (see the code below).\nEncouraging the models to only output in the desired format - we want the models to only respond with “Yes”/“No”, but even with specialized prompts the models would still sometimes generate different responses. To tackle this, we tweaked the reward to be -1 if the model does not respond in the desired format, and we would give the usual score from the reward model if the output was what the model was asked for. This encouraged the model to converge to only responding with the required format over time.\n\nTo illustrate these concepts, the finel RL training loop looked roughly like the following:\n\nimport torch\nimport string\n\n\nCHARACTERS_TO_FILTER = string.punctuation + \" \\n\"\n\n\ndef is_answer_yes_no(answer):\n    return answer in [\"Yes\", \"No\"]\n\n\ndef postprocess_response(response):\n    while response and response[-1] in CHARACTERS_TO_FILTER:\n        response = response[:-1]\n    return response\n\n\ndef train(\n    ppo_trainer,\n    tokenizer,\n    generation_kwargs,\n    get_rewards,\n    script_args, config,\n):\n    n_epochs = config.steps // len(ppo_trainer.dataloader)\n\n    for epoch in range(1, n_epochs + 1):\n        loop = tqdm(\n            enumerate(ppo_trainer.dataloader, 1),\n            total=len(ppo_trainer.dataloader), leave=False\n        )\n        for batch_idx, batch in loop:\n            # Get the input tensors\n            question_tensors = batch[\"input_ids\"]\n\n            # Get the generations\n            response_tensors = ppo_trainer.generate(\n                question_tensors,\n                return_prompt=False,\n                batch_size=script_args.generator_batch_size,\n                **generation_kwargs,\n            )\n            responses = tokenizer.batch_decode(\n                response_tensors, skip_special_tokens=True,\n                spaces_between_special_tokens=False\n            )\n\n            # Postprocess the responses\n            if script_args.postprocess_responses:\n                responses = [postprocess_response(x) for x in responses]\n            batch[\"response\"] = responses\n\n            # Compute the rewards (scores)\n            texts = [q + \" \" + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n            rewards = get_rewards(texts)\n\n            # Replace reward for undesired answers to -1\n            mask = [not is_answer_yes_no(x) for x in batch[\"response\"]]\n            mask = torch.tensor(mask, dtype=torch.bool) # cast to tensor\n            rewards[mask] = -1\n\n            # Make the rewards a list of tensors\n            rewards = [x for x in rewards]\n\n            # Run PPO step\n            stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n            ppo_trainer.log_stats(stats, batch, rewards)\n\nAnd the generation_kwargs look like the following:\n\ngeneration_kwargs = {\n    \"top_k\": 0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.pad_token_id,\n    \"eos_token_id\": 100_000,\n    \"pad_to_multiple_of\": 8,\n    \"max_new_tokens\": 2,\n}"
  },
  {
    "objectID": "posts/thesis/thesis.html#conclusion",
    "href": "posts/thesis/thesis.html#conclusion",
    "title": "Eliciting latent knowledge from language reward models",
    "section": "Conclusion",
    "text": "Conclusion\nI hope you found this blog as interesting as it was for me to work on this project. I feel like I have learnt a lot during it, for example, I joined multiple ML communities and got involved in discussions with very smart and ambitious people. Perhaps my proudest achievement is making my first open-source contribution to the elk library (link), as well as reported multiple bugs to the big-refactor branch of Language Model Evaluation Harness (link).\nI am excited to dive deeper into LLMs-related topics in the future. Feel free to reach out if you have any opportunities on offer!"
  },
  {
    "objectID": "posts/captcha-fastai/captcha-fastai.html",
    "href": "posts/captcha-fastai/captcha-fastai.html",
    "title": "Multi-output classification for captcha recognition using fastai",
    "section": "",
    "text": "Special thanks to my dear friends Julius and Laurynas who joined me to work on this project.\nWe worked on and off for more than a month on this project, and I must say that it was not an easy, but definitely an extremely rewarding experience considering how much I’ve learnt. It was also a good chance to hone my problem solving skills and endurance, as we switched approaches (and even libraries!) at least three times, spent substantial amounts of time reading through various resources and codebases and trying to adapt the codes to our purpose through trial and error.\nThe key takeaways are:\n\nDo not ever use old captchas and encourage others not to, as there are too many ways to break them nowadays.\nFastai has a great mid-level API that allows for customization for almost any use, but getting the hang of it might be tricky. Here are some resources that tremendously helped us:\n\nLooking at fastai docs and tutorials and reading through source code\nA walk with fastai2 playlist that has excellent material on using the fastai mid-level API\nfastai v2 walk-thru playlist on YouTube where Jeremy Howard talks how and why the library is built the way it is\n\nPerseverance is key - great ideas and solutions usually do not come overnight, but breaking the problem into smaller pieces and continously improving on each of those is a good way to go."
  },
  {
    "objectID": "posts/captcha-fastai/captcha-fastai.html#check-validation-set-results",
    "href": "posts/captcha-fastai/captcha-fastai.html#check-validation-set-results",
    "title": "Multi-output classification for captcha recognition using fastai",
    "section": "Check validation set results",
    "text": "Check validation set results\nCheck the model’s performance on a batch of data:\n\nlearn.show_results()"
  },
  {
    "objectID": "posts/captcha-fastai/captcha-fastai.html#get-failed-captchas",
    "href": "posts/captcha-fastai/captcha-fastai.html#get-failed-captchas",
    "title": "Multi-output classification for captcha recognition using fastai",
    "section": "Get failed captchas",
    "text": "Get failed captchas\nGet predictions:\n\ninputs, _, targets, decoded_preds = learn.get_preds(with_input=True, with_decoded=True)\ninputs.size(), targets.size(), decoded_preds.size()\n\n\n\n\n(torch.Size([1991, 3, 64, 64]), torch.Size([1991, 4]), torch.Size([1991, 4]))\n\n\nIndices of failed captchas are:\n\nfailed_idxs = (~(decoded_preds == targets).all(dim=1)).nonzero().view(-1)\n\nShow those captchas to check why the model failed:\n\ndls.show_results(b=(inputs[failed_idxs], targets[failed_idxs]), out=decoded_preds[failed_idxs])\n\n\n\n\n\n\n\n\nWe can see that the model does some slight errors, but it only fails on 4 captchas out of a validation set of 1991 images which is an amazing result. I hope that it is now obvious that using captchas to protect a website from bots is not a good idea.\nCheck the model’s confidence when it makes errors:\n\nfor idx in failed_idxs.detach().cpu().numpy().flatten():\n    decoded_pred, pred, model_output = learn.predict(dls.valid_ds[idx][0])\n    probs = torch.softmax(model_output, dim=0)[pred].diag()\n    print(f\"Actual: {''.join([i2l[v.item()] for v in dls.valid_ds[idx][1]])}, predicted: {decoded_pred}, probs: {probs}\")\n\n\n\n\nActual: W4KN, predicted: WAKN, probs: tensor([0.0805, 0.0574, 0.0800, 0.0804])\nActual: B56M, predicted: B5GM, probs: tensor([0.0775, 0.0805, 0.0517, 0.0805])\nActual: M3AH, predicted: MBAH, probs: tensor([0.0805, 0.0492, 0.0803, 0.0805])\nActual: 3WZP, predicted: 3W2P, probs: tensor([0.0805, 0.0795, 0.0597, 0.0804])\n\n\n\n\n\n\n\n\n\n\n\n\nNote: In each case the probabilities for the failed letters were lower."
  },
  {
    "objectID": "posts/embedding-visualisation/embedding-visualisation.html",
    "href": "posts/embedding-visualisation/embedding-visualisation.html",
    "title": "Visualising large language model embeddings",
    "section": "",
    "text": "In a recent post, I shared a tool developed alongside a dear friend of mine, designed for visualizing the tokenizers of language models. Following that post, I received inquiries about my views on visualizing the embeddings learned by Large Language Models (LLMs) during their pre-training phase. This post aims to serve as a guide detailing one possible way to visualizing the high-dimensional embeddings of LLMs, while also delving into the interesting patters that become apparent through such visualizations. The findings are quite intriguing, so continue reading to discover more!\n\nImports\nToggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nimport re\n\nimport plotly.graph_objects as go\nfrom sklearn.manifold import TSNE\nfrom transformers import AutoModel, AutoTokenizer\n\n# Ensures we can render plotly plots with quarto\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n\nCode\n# Put default plotly colors into a variable\nimport plotly.express as px\nDEFAULT_PLOTLY_COLORS = px.colors.qualitative.Plotly\n\n# Put full month names into a constant\nimport calendar\nMONTHS = [month for month in calendar.month_name if month]\n\nCOUNTRY_TOKENS = [\n    \"state\", \"states\", \"international\", \"world\", \"united\", \"washington\",\n    \"california\", \"uk\", \"america\", \"american\", \"british\", \"australia\",\n    \"australian\", \"canada\", \"english\", \"french\", \"german\", \"russian\",\n    \"european\", \"europe\", \"france\", \"germany\", \"england\", \"london\",\n    \"york\", \"japanese\", \"chinese\", \"japan\", \"china\", \"indian\", \"india\"\n]\n\n\n\n\nDimensionality reduction\nAs you can see below, the code uses very standard libraries and functions. We begin by loading a model and extracting the tensor containing the embedding vectors:\n\nmodel_name = \"google-bert/bert-base-cased\"\nmodel = AutoModel.from_pretrained(model_name)\n\n\nembedding_vectors = model.embeddings.word_embeddings.weight.data\nembedding_vectors.shape\n\ntorch.Size([28996, 768])\n\n\nWe also extract the unique tokens in the tokenizer:\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nvocab = tokenizer.get_vocab()\n\ntokens = sorted(tokenizer.get_vocab().items(), key=lambda item: item[1])\ntokens = [item[0] for item in tokens]\ntokens[:5]\n\n['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]']\n\n\nThe essence of the code is below. We use the t-SNE dimensionality reduction technique to reduce the dimensionality of the embedding vectors to 2.\nAlternatively, one could use the UMAP8 library to compute the embeddings. The code runs much faster for large embedding matrices, but we are only going to use a subset of the embedding matrix in this post, so t-SNE will do just fine. However, I will leave the code for the interested reader to try out:\nimport umap\nreducer = umap.UMAP()\ndata_2d = reducer.fit_transform(embedding_vectors[NUM_TOKENS_TO_SKIP:NUM_TOKENS_TO_SKIP+NUM_TOKENS_TO_VISUALISE])\n\nNUM_TOKENS_TO_VISUALISE = 2000\nNUM_TOKENS_TO_SKIP = 100\n\n\n# Performing t-SNE to reduce the dataset to 2 dimensions\ntsne = TSNE(n_components=2, random_state=42)\ndata_2d = tsne.fit_transform(\n    embedding_vectors[NUM_TOKENS_TO_SKIP:NUM_TOKENS_TO_SKIP+NUM_TOKENS_TO_VISUALISE]\n)\ndata_2d.shape\n\n(2000, 2)\n\n\nThe code to visualise the embeddings is below. The first cell contains mundane code to color the various token groups, toggle the cell if you are interested to see it.\n\n\nCode\nrelevant_tokens = tokens[NUM_TOKENS_TO_SKIP:NUM_TOKENS_TO_SKIP + NUM_TOKENS_TO_VISUALISE]\n\n# Define default colors\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[0], token) for token in relevant_tokens\n]\n\n# Color numbers\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[1] if token.isdigit() else color, token) for color, token in colors\n]\n\n# Color tokens that start with ##\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[2] if token.startswith(\"##\") else color, token) for color, token in colors\n]\n\n# Color months\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[3] if token in MONTHS else color, token) for color, token in colors\n]\n\n# Color special tokens\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[4] if token in tokenizer.special_tokens_map.values() else color, token)\n    for color, token in colors\n]\n\n# Color single letters\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[5] if re.match(r\"^[a-zA-Z]$\", token) else color, token) for color, token in colors\n]\n\n# Color country tokens\ncolors = [\n    (DEFAULT_PLOTLY_COLORS[6] if token.lower() in COUNTRY_TOKENS or token == \"US\" else color, token)\n    for color, token in colors\n]\n\n# Leave just the colors\ncolors = [color for color, _ in colors]\n\n# Define cluster names\ncluster_names = [\n    \"default\", \"numbers\", \"tokens that start with ##\", \"months\",\n    \"special tokens\", \"single letters\", \"countries\",\n]\nunique_colors = DEFAULT_PLOTLY_COLORS[: len(cluster_names)]\n\n\n\nfig = go.Figure(\n    data=go.Scatter(\n        x=data_2d[:, 0],\n        y=data_2d[:, 1],\n        mode=\"markers\",\n        marker=dict(color=colors),\n        hovertext=relevant_tokens,\n        hoverinfo=\"text\",\n        showlegend=False\n    )\n)\n\n# Create dummy traces to have a nice legend\nfor name, color in zip(cluster_names, unique_colors):\n    fig.add_trace(go.Scatter(\n        x=[None], y=[None],\n        mode=\"markers\", marker=dict(color=color), name=name\n    ))\n\nfig.update_layout(title=\"Token embeddings\", legend=dict(title=\"Clusters\"))\n\nfig.show()\n\n                                                \n\n\nWe can see that there is a lot of semantic structure in the learnt embeddings! For instance, I have highlighted the clusters for names of the months, countries/nationalities, numbers, etc. Of course, one could have expected this to happen given all the theory behind methods like word2vec, but it is still exciting that the semantic structure is still present in the embedding layer even though the model much deeper and the LLM pre-training objective is completely different from word2vec!\n\n\nConclusion\nI was very pleased to see that, as expected, the learnt patters have a nice semantic structure. I encourage the interested reader to explore the ideas in this post in more depth, e.g. try a different pre-trained LLM, try the UMAP dimensionality reduction method instead of t-SNE, or just play around with the number of embedded vectors and try to find more semantic clusters.\nThank you for reading! I hope you enjoyed the post and am looking forward to hearing your feedback.\n\n\nNext steps\nI would like to explore the idea of clustering these embeddings in the future and trying to look for what clusters emerge and what tokens get grouped into the same cluster. Again, please be encouraged to try it yourself and let me know about the results!"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Miscellaneous notes",
    "section": "",
    "text": "Cubic spline interpolation exercise\n\n\n\n\n\n\nscientific computing\n\n\nnumerical methods\n\n\n\nLarge language models (predictably) learn to represent the semantic meaning of sentences.\n\n\n\n\n\nApr 13, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising contextualised large language model embeddings with context\n\n\n\n\n\n\ndeep learning\n\n\nLLMs\n\n\nvisualisation\n\n\n\nLarge language models (predictably) learn to represent the semantic meaning of sentences.\n\n\n\n\n\nApr 10, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/tokenizers-why-weird-space/why-weird-space.html",
    "href": "posts/tokenizers-why-weird-space/why-weird-space.html",
    "title": "Why is the Ġ Unicode character all over the place in LLM tokenizers?",
    "section": "",
    "text": "TLDR: WHY do tokenizers represent spaces \" \" with the Unicode character Ġ?\nHello, World!\nWhile working on a different blog post where I plan to dive deeper into training LLM tokenizers using the Hugging Face tokenizers library, I could not help but notice the Ġ Unicode character being omnipresent in the vocabularies of seemingly all tokenizers based on the byte pair encoding (BPE) algorithm. I also noticed that it has something to do with replacing the space \" \" character, for example, a Llama 3 8B Instruct tokenizer would produce the following outputs:\nI have tried looking for an answer online, but found out that people were as confused as I was, below are a few issues on GitHub only:\nThere are many other similar questions, but all of the answers seem to simply suggest that it is a feature of the BPE algorithm, but neither of the discussions answered why should such a feature exists in the first place.\nThis blog post is my attempt at finding a more satisfying answer which (spoiler alert!) seems to be that replacing spaces with the character Ġ could well be a historical artifact and not at all a strict design requirement. Read on to see what I came up with!"
  },
  {
    "objectID": "posts/tokenizers-why-weird-space/why-weird-space.html#test-on-a-more-diverse-dataset",
    "href": "posts/tokenizers-why-weird-space/why-weird-space.html#test-on-a-more-diverse-dataset",
    "title": "Why is the Ġ Unicode character all over the place in LLM tokenizers?",
    "section": "Test on a more diverse dataset",
    "text": "Test on a more diverse dataset\n\nfrom datasets import load_dataset\n\nsplit = \"train\"\nenglish_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=split)\nkorean_dataset = load_dataset(\"lcw99/wikipedia-korean-20221001\", split=split)\ncode_dataset = load_dataset(\"code_search_net\", \"python\", split=split, trust_remote_code=True)\ncode_dataset = code_dataset.rename_column(\"whole_func_string\", \"text\")  # Rename whole_func_string to text\n\nn = 10000\nfinal_dataset = (\n    english_dataset.shuffle(42).select(range(n))[\"text\"] +\n    korean_dataset.shuffle(42).select(range(n))[\"text\"] +\n    code_dataset.shuffle(42).select(range(n))[\"text\"]\n)\nprint(f\"{len(final_dataset)=}\")\n\nlen(final_dataset)=30000\n\n\nLet’s make sure that the Korean data is in there somewhere:\n\nfinal_dataset[10001][:50]\n\n'김혜리(1969년 12월 23일 ~ )는 대한민국의 배우이며, 1988년 미스코리아 선 출'\n\n\nNow run the tests!\n\nfor idx, test_string in tqdm(enumerate(final_dataset), total=len(final_dataset)):\n    expected_output = tokenizer_original.encode(test_string)\n    output = tokenizer_custom.encode(test_string)\n\n    if output != expected_output:\n        print(f\"{idx=}\")"
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html",
    "href": "posts/tokenizer-problems/tokenizer-problems.html",
    "title": "Good tokenizers is all you need",
    "section": "",
    "text": "Although it might seem like the title is just me giving in to the popular trends in naming things in the ML space, it is not actually that misleading. Tokenization is a crucial part of the whole language modelling pipeline. Yet, we will see in this blog post that there all sorts of problems that tokenization can cause that one might not be aware of.\nThis post is inspired by the amazing video on LLM tokenizers by Andrej Karpathy.\nKarpathy’s list of problems (taken from the following notes):\nI will use the trailtoken tool that I have recently built with a collaborator to inspect why these problems occur. I encourage you to play around with it, especially if you build tokenizers from scratch yourself. You might be surprised with how easily problems can occur if one is not careful enough!\nLet’s dive in!"
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#spelling",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#spelling",
    "title": "Good tokenizers is all you need",
    "section": "Spelling",
    "text": "Spelling\nTokenization makes it hard for the LLMs to spell. For example, one of the best open-source LLMs, Mistral-7B-Instruct, has a very hard time spelling the word antidisestablishmentarianism:\n\n\n\nThe mistralai/Mistral-7B-Instruct-v0.1 LLM misspelling the word antidisestablishmentarianism\n\n\nHowever, if we use trailtoken to inspect how the said word is tokenized by the Mistral-7B tokenizer, we see that it is actually split into 8 seemingly random tokens:\n\n\n\nVisualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes the word antidisestablishmentarianism\n\n\nIt is unlikely that the model has seen them occurring together during training, so it no surprise that it finds it hard separating out the letters constituting each token.\nOn the other hand, the OpenAI models seem to handle the task with ease (see these following links for gpt-3.5-turbo and gpt-4-turbo). Even then, this is mostly a result of the capabilities of these models, as the word is still tokenized into 6 tokens which are again more or less arbitrary:\n\n\n\nVisualisation of how the tokenizer of gpt-3.5-turbo tokenizes the word antidisestablishmentarianism\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere and below I use the Xenova/gpt-3.5-turbo tokenizer on trailtoken because it is an open-source implementation of the gpt-3.5-turbo tokenizer."
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#string-operations",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#string-operations",
    "title": "Good tokenizers is all you need",
    "section": "String operations",
    "text": "String operations\nSimilarly to spelling words, LLMs find it hard to to simple string operations, such as reversing words. A well-known example is asking LLMs to reverse the word lollipop. Mistral-7B fails miserably on the task:\n\n\n\nmistralai/Mistral-7B-Instruct-v0.1 LLM failing to reverse the word lollipop\n\n\nIf we look at how the word is tokenized, we see that it ends up being only 4 tokens, so it is no wonder that the LLMs find it hard solving this task:\n\n\n\nVisualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes the word lollipop\n\n\nThe screenshot above hints at a “hack” that can be used to help LLMs: separate out the letters so that they end up as separate tokens after tokenization. Even then, few-shot examples are required to make it work (and it also did not work when I separated the letters using dashes '-' or spaces ' '):\n\n\n\nmistralai/Mistral-7B-Instruct-v0.1 LLM succeeding at reversing the word lollipop\n\n\n\n\n\n\n\n\nCaution\n\n\n\nInterestingly, gpt-4 fails reversing the string too, so it is truly a problem caused by tokenization!\n\n\nFor the curious reader, there are many other seemingly “simple” operations that LLMs struggle with. One such example is try to count the number of letter in a word:\n\n\n\nGPT-4 LLM failing to count the number of letters in words\n\n\nCan you think of any other tasks where LLMs might struggle?"
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#non-english-languages-and-code",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#non-english-languages-and-code",
    "title": "Good tokenizers is all you need",
    "section": "Non-English languages and code",
    "text": "Non-English languages and code\nIt turns out that not all tokenizers are equally good at tokenizing non-English text, be it materials in foreign languages or code. I put these two together because they are very similar in nature, which is that training tokenizers on unbalanced text corpuses makes them worse at tokenizing the under-represented pieces of text (I might write a separate post on this, so stay tuned!).\nLet’s see this in action. The following piece of text is taken from Karpathy’s video referenced above. The text states something like “Nice to meet you, I’m ChatGPT, a large-scale language model developed by OpenAI. If you have any questions, feel free to ask.” in Korean.\n\nstr = \"만나서 반가워요. 저는 OpenAI에서 개발한 대규모 언어 모델인 ChatGPT입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\"\nlen(str)\n\n72\n\n\nWe can see that the string is made up of 72 characters. Let’s see how the older GPT-3 tokenizer would handle this:\n\n\n\nVisualisation of how the tokenizer of gpt-3 tokenizes Korean\n\n\nWe find that the resulting number of tokens is 132, much more than the original length of the string! This is because the tokenizer probably did not see much Korean during training, so it did not learn to represent the Hanguls (Korean “letters”) in the Korean alphabet efficiently (we see that the first Hangul is represented by three tokens).\nOn the other hand, the GPT-4 tokenizer is much better at Korean, the resulting number of tokens is 61 (more 2x less!), meaning that the Korean Hanguls are better represented in the larger tokenizer vocabulary:\n\n\n\nVisualisation of how the tokenizer of gpt-4 tokenizes Korean\n\n\nHowever, the problem still remains. If we inspect how the English version of the same phrase is tokenizer, we find another 2x reduction in the number of tokens:\n\n\n\nVisualisation of how the tokenizer of gpt-4 tokenizes English\n\n\nTherefore, poor tokenization means that the “information density” of non-English tokens is much lower, so LLMs have to attend to longer sequences to process the same amount of information, though the fact that they have probably seen much less non-English data during training also significantly contributes to the problem.\nSame thing goes for code. We find that the original code string is 197 characters:\n\ncode_str = '''for i in range(1, 101):\n    if i % 3 == 0 and i % 5 == 0:\n        print(\"FizzBuzz\")\n    elif i % 3 == 0:\n        print(\"Fizz\")\n    elif i % 5 == 0:\n        print(\"Buzz\")\n    else:\n        print(i)\n'''\nlen(code_str)\n\n197\n\n\nThe GPT-3 tokenizer compresses the string reasonably well, but there are a few areas for improvement. For instance, we see that every space of indentation is a separate token, so we waste a lof of tokens. Similarly, keywords like elif are broken into two tokens which is not ideal.\n\n\n\nVisualisation of how the tokenizer of gpt-3 tokenizes code\n\n\nJust as before, the GPT-4 tokenizer is much better at this, as we see in the image below. The indentation spaces are now grouped into a single token, and the elif keyword is a single token as well. The resulting tokenization saves about 30 tokens for the given piece of code. Imagine what the savings would be for a very large codebase!\n\n\n\nVisualisation of how the tokenizer of gpt-4 tokenizes code\n\n\nThere is no doubt that these improvements in the tokenizer have contributed to the much improved coding and multilingual performance of the GPT-4 model."
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#arithmetic",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#arithmetic",
    "title": "Good tokenizers is all you need",
    "section": "Arithmetic",
    "text": "Arithmetic\nThis is going to be a brief one, but LLM tokenizers can do odd things when tokenizing numbers which can hinder their performance on simple arithmetic. For example, here is the GPT-2 tokenizer behaves on the following numbers:\n\n\n\nVisualisation of how the tokenizer of gpt2 tokenizes numbers\n\n\nWe find that in one case the decimal is one token, but becomes two tokens if an extra 0 is appended at the end. This would not cause any trouble to us humans, but could potentially confuse the LLM.\nHowever, most of the modern LLM tokenizers are quite robust at handling numbers."
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#special-data-formats",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#special-data-formats",
    "title": "Good tokenizers is all you need",
    "section": "Special data formats",
    "text": "Special data formats\nHow structured data tokenized is another source of peculiar behaviours in LLMs. For example the images below show how the Mistral-7B tokenizer handles JSON and YAML formats:\n\n\n\nVisualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes JSON\n\n\n\n\n\nVisualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes YAML\n\n\nAs we can see, we can save up on tokens (about 2x!) and overall complexity if we use YAML over JSON. Also, notice how using underscores in key names adds extra tokens, so being smart about how structured data is passed to the tokenizer can help reduce the number of tokens used and, therefore, the compute costs!"
  },
  {
    "objectID": "posts/tokenizer-problems/tokenizer-problems.html#other-problems",
    "href": "posts/tokenizer-problems/tokenizer-problems.html#other-problems",
    "title": "Good tokenizers is all you need",
    "section": "Other problems",
    "text": "Other problems\nFinally, I want to briefly touch upon the few remaining problems on Karpathy’s list which have either been solved completely or are very rare if one is careful enough.\nWhy did my LLM abruptly halt when it sees the string &lt;|endoftext|&gt;?\n&lt;|endoftext|&gt; is a special tokens which hints LLMs that they should stop generating text, so if you accidentally put that into the model, it might halt abruptly. Simple string processing techniques can help deal with such edge cases most of the time, though that sometimes leads to undesired behaviours, such as the one below which likely happens because OpenAI’s tokenizers remove special tokens from the input string during pre-processing:\n\n\n\ngpt-4 tokenizers remove special tokens from the input string during pre-processing\n\n\nWhat is this weird warning I get about a trailing whitespace?\nThe two images below will help us better understand what’s going on:\n\n\n\ngpt-4 tokenizer trailing whitespace issue\n\n\n\n\n\ngpt-4 tokenizer without trailing whitespace issue\n\n\nWe can see that in the first image, the trailing space becomes a separate token. However, the tokenizers often concatenate words the the spaces before them, just like the for word once in the second image. Since models are trained to predict the next token, during training the learn to predict the token once from the tokens that go before it up to the colon :, so the case where they have to predict the token once following a space ' ' is completely out of distrubution for them and can lead to all kinds of peculiar behaviours.\nWhy did the LLM break if I ask it about SolidGoldMagikarp?\nThis problem was quite hilarious, but the researchers at OpenAI trained their olders tokenizers on Reddit data and SolidGoldMagikarp is a Reddit user with lots of posts, so their username became a separate token in the LLMs vocabulary, but the same data did not make it into the training data, so the models behave more or less randomly if they ever see this token because they have never been trained to handle them."
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "",
    "text": "This article is adapted from fast.ai’s Machine Learning for Coders course, specifically, lesson 10. I would highly recommend checking this and other courses from fast.ai, it has numerous tips on how to do practical machine learning and deep learning.\nWe will be building a naive Bayes classifier in just 10 lines of code that will get over 98% accuracy on a spam message filtering task.\nWe will do this in the top-bottom approach, where we will first build the model and then dig deeper into the theory of how it works."
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#the-countvectorizer",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#the-countvectorizer",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "1. The CountVectorizer",
    "text": "1. The CountVectorizer\nNaive Bayes classifier uses what is called a bag of words approach. It simply means that we disregard any relationships between words and just look at how often they appear in the text that we want to classify.\n\nNote: I am not saying that bag of words is the best approach to do NLP, it is usually quite the opposite as nowadays we have tools like RNNs and Transformers that perform much better on NLP tasks. However, we use it here because it is a really simple approach that sometimes still gives reasonable results, as it did in this case!\n\nThis is exactly what we use a CountVectorizer for: it produces a term document matrix with frequencies of each word for each message.\nLet’s look at an example of what sklearn’s CountVectorizer is doing. Suppose that our messages are:\n\n\n\nmessage\nlabel\n\n\n\n\nThis is a good message\n0\n\n\nA good message\n0\n\n\nThis message is bad\n1\n\n\nThe message is bad\n1\n\n\n\nThen, what a CountVectorizer is going to do for us is produce the following matrix:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\n\n\nImportant: CountVectorizer just finds the vocabulary (list of all the unique words in our data) and then counts how often each word from the vocabulary is found in each message.\n\nNotice that for a larger dataset, our vocabulary might become very large which would mean that most of the cells in our term document matrix would be 0. For this reason, the sklearn implementation actually produces a sparse matrix which, instead of storing all the entries, stores only the location and values of non-zero entries, and since there are only so many of them, saves huge amounts of memory. How neat!\nYou must have noticed that I used a max_df=0.1 parameter for my CountVectorizer. This tells the vectorizer to ignore any words that appear in more than 10% on the documents as we can safely say that they are too common. I came with this number by trying different values and looking at vectorizer.stop_words_ to check how many and which words were ignored until I was satisfied. When you build your own model, make sure to play around with this and other parameters, such as min_df (opposite of max_df, used for very rare words), to find what works best for you! You can find more information on what parameters for CountVectorizer can be tinkered on the official docs.\nA trick that we want to do before moving on is to note that later we will want to calculate probabilities of each word appearing in spam or non-spam messages. But it might happen that certain words do not appear in a particular class at all and we might run into trouble because we will get feature probabilities of zero, and we don’t want that. To counter that, we will add a row of ones, like so:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\nrow of ones\n\n1\n1\n1\n1\n1\n1\n1\n\n\n\nIf we think more about, adding a row of ones is not that counter-intuitive at all, since the messages that we have so far only carry information up to this point in time, but if a word has not appeared in any of the messages so far, it is not at all impossible that it will not appear in the future, so adding the extra one takes care of that for us.\nSo that our feature probabilities will be:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\nrow of ones\n\n1\n1\n1\n1\n1\n1\n1\n\n\nP(feature|0)\n\n0.67\n0.67\n1\n1\n1\n0.33\n0.33\n\n\nP(feature|1)\n\n0.67\n1\n0.33\n0.33\n1\n0.67\n1\n\n\n\nIn code:\n\n\n# Create a vectorizer object that will ignore any\n# words that appear in more than 10% of messages.\nvectorizer = CountVectorizer(max_df=0.1)\n\n# Use the fit_transform method of the vectorizer to get\n# the term document matrix for the training set.\ntrain_term_doc = vectorizer.fit_transform(X_train)\n\n# Use the transform method of the vectorizer to get\n# the term document matrix for the validation set.\n# We do it this way so that train and validation sets\n# are have the same vocabularies so that we could make predictions.\nval_term_doc = vectorizer.transform(X_val)"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#bayes-theorem",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#bayes-theorem",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "2. Bayes’ theorem",
    "text": "2. Bayes’ theorem\nNow, we can get to the essence of the model which is to apply Bayes’ theorem. I am not going to give the usual form of the formula, but instead one that will illustrate how we will be using it. Our goal is, given a particular message, figure out whether it is spam or not. Hence, the formula for us is going to take the form:\n\\(P(\\text{spam} \\mid \\text{message}) = \\frac{P(\\text{message} \\mid \\text{spam}) \\cdot P(\\text{spam})}{P(\\text{message})}\\)\nBut we can use a trick: instead of trying to predict whether it is spam or not, let’s look at which class is a message more likely, i.e. \\(\\frac{P(\\text{spam} \\mid \\text{message})}{P(\\text{non-spam} \\mid \\text{message})}\\). In that case, the formula will become:\n\\(\\text{ratio} = \\frac{P(\\text{spam} \\mid \\text{message})}{P(\\text{non-spam} \\mid \\text{message})} = \\frac{P(\\text{message} \\mid \\text{spam}) \\cdot P(\\text{spam})}{P(\\text{message} \\mid \\text{non-spam}) \\cdot P(\\text{non-spam})}\\)\nReferring to our previous example, the ratios would then be:\n\n\n\nmessage\nlabel\nthis\nis\na\ngood\nmessage\nthe\nbad\n\n\n\n\nThis is a good message\n0\n1\n1\n1\n1\n1\n0\n0\n\n\nA good message\n0\n0\n0\n1\n1\n1\n0\n0\n\n\nThis message is bad\n1\n1\n1\n0\n0\n1\n0\n1\n\n\nThe message is bad\n1\n0\n1\n0\n0\n1\n1\n1\n\n\nrow of ones\n\n1\n1\n1\n1\n1\n1\n1\n\n\nP(feature|0)\n\n0.67\n0.67\n1\n1\n1\n0.33\n0.33\n\n\nP(feature|1)\n\n0.67\n1\n0.33\n0.33\n1\n0.67\n1\n\n\nratio\n\n1\n1.5\n0.33\n0.33\n1\n2\n3\n\n\n\n\nImportant: As you can see, ratios are greater than 1 for features that are more likely to be in spam messages and lower than one otherwise.\n\nThen, to further simplify things, we make the naive Bayes assumption, which says that probability of any word appearing in a message is independent of probabilities of other words appearing in that same message.\n\nNote: Obviously, this is a very naive assumption and that is most certainly not the case, but it turns out to work quite well.\n\nUnder the naive assumption, the probabilities like \\(P(\\text{message} \\mid \\text{spam})\\) can be factorized into a product of probabilities of individual features appearing in a message, so that:\n\\(P(\\text{message} \\mid \\text{spam}) = \\prod_{i=1}^{n}{P(\\text{features[i]} \\mid \\text{spam})}\\)\nand similarly for \\(P(\\text{message} \\mid \\text{non-spam})\\). Then, our big formula becomes:\n\\(\\text{ratio} = \\frac{P(\\text{spam} \\mid \\text{message})}{P(\\text{non-spam} \\mid \\text{message})} = \\frac{\\prod_{i=1}^{n}{P(\\text{features[i]} \\mid \\text{spam})}}{\\prod_{i=1}^{n}{P(\\text{features[i]} \\mid \\text{non-spam})}} \\cdot \\frac{P(\\text{spam})}{P(\\text{non-spam})}\\)"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#a-few-final-tricks",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#a-few-final-tricks",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "3. A few final tricks",
    "text": "3. A few final tricks\nWe are almost done, now we just want to apply a few tricks to make our calculations easier. First, notice that multiplying lots of probabilities together is going to result into a very small number and we might run out of floating point precision, so we can take the natural logarithm instead to handle this. Note that in this case we compare ratios not with 1, but with 0 (because log(1)=0) and that by the properties of logarithms all the products are going to turn into sums, which makes everything even simpler!\nFinally, we notice that to make predictions, we can just perform matrix multiplication on the validation term document matrix and our derived vector of ratios and add (remember, we are in log space!) the ratio of priors.\nPutting it all together:\n\n\n# Calculate P(feature|1) and P(feature|0).\n# This plus ones are there to constitute the\n# row of ones discussed above\np = train_term_doc[y_train == 1].sum(0) + 1\nq = train_term_doc[y_train == 0].sum(0) + 1\n\n# Calculate the ratios according to our derived formulae\nratio = np.log((p / p.sum()) / (q / q.sum()))\n\n# Calculate the log of ratio of priors\nb = np.log((y_train == 1).sum() / (y_train == 0).sum())\n\n# Make some predictions on the validation set\npre_preds = val_term_doc @ ratio.T + b\npreds = pre_preds.T &gt; 0 # Greater than 0 because we are working in log space\n(preds == y_val).mean() # Accuracy\n\n0.9856502242152466"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#try-n-grams",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#try-n-grams",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "Try n-grams",
    "text": "Try n-grams\n\nvectorizer = CountVectorizer(ngram_range=(1, 3), max_df=0.1)\ntrain_term_doc = vectorizer.fit_transform(X_train)\nval_term_doc = vectorizer.transform(X_val)\n\n\np = train_term_doc[y_train == 1].sum(0) + 1\nq = train_term_doc[y_train == 0].sum(0) + 1\nratio = np.log((p / p.sum()) / (q / q.sum()))\nb = np.log((y_train == 1).sum() / (y_train == 0).sum())\n\n\npre_preds = val_term_doc @ ratio.T + b\npreds = pre_preds.T &gt; 0\n(preds == y_val).mean()\n\n0.9874439461883409\n\n\nTurns out the model gives even better accuracy with bigrams and trigrams included. But watch out! Checking the confusion matrices, we see that the model is now perfect on non-spam messages, but the error on spam messages has increased. This might not be what we want, so we have to be careful with interpreting our models!\n\nconfusion_matrix(y_val, preds.T, normalize=None)\n\narray([[968,   2],\n       [ 12, 133]])\n\n\n\nconfusion_matrix(y_val, preds.T, normalize=\"true\")\n\narray([[0.99793814, 0.00206186],\n       [0.08275862, 0.91724138]])"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#binarized-version",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#binarized-version",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "Binarized version",
    "text": "Binarized version\nYou can also try the binarized version of the term document matrix (i.e. instead of frequencies we look at whether a word is present or not)\n\npre_preds = val_term_doc.sign() @ ratio.T + b\npreds = pre_preds.T &gt; 0\n(preds == y_val).mean()\n\n0.9874439461883409"
  },
  {
    "objectID": "posts/elegant-naive-bayes/elegant-naive-bayes.html#learning-the-parameters-with-logistic-regression",
    "href": "posts/elegant-naive-bayes/elegant-naive-bayes.html#learning-the-parameters-with-logistic-regression",
    "title": "Elegant implementation of Naive Bayes in just 10 lines of code",
    "section": "Learning the parameters with logistic regression",
    "text": "Learning the parameters with logistic regression\nFinally, you might try taking it to the next level and learning the parameters with a logistic regression instead of using the theoretical ones. Check the parameters C for regularization and dual=True for when you term document matrix is much wider than it is tall.\n\nm = LogisticRegression(C=1e1, dual=False)\nm.fit(train_term_doc, y_train)\npreds = m.predict(val_term_doc)\n(preds == y_val).mean()\n\n0.9811659192825112\n\n\n\n\n# Binarized version\nm = LogisticRegression(C=1e1, dual=False)\nm.fit(train_term_doc.sign(), y_train)\npreds = m.predict(val_term_doc.sign())\n(preds == y_val).mean()\n\n0.9802690582959641"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html",
    "href": "posts/basic-panorama/basic-panorama.html",
    "title": "Basic panorama stitching",
    "section": "",
    "text": "In this notebook we will find that we can use knowledge of computer vision to create our own panoramas! We only need to take a few photos with a smartphone or camera, but we have to make sure that we only change the angle of the phone/camera, and not the position! This notebook then takes care of everything else.\nThis notebook is based on the Computer Vision course by Andreas Geiger."
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#preliminaries",
    "href": "posts/basic-panorama/basic-panorama.html#preliminaries",
    "title": "Basic panorama stitching",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom pathlib import Path\n\n\nLet’s first import the some images that we want to stich together:\n\nPATH_TO_YOUR_IMAGES = Path(\"path/to/your/images\")\n\n\n# Load images\nimages = sorted([f for f in PATH_TO_YOUR_IMAGES.iterdir()])\nimages = map(str, images)\nimages = map(cv2.imread, images)\nimages = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images]\n\nlen(images)\n\n10\n\n\n\nimages = images[2:4] + images[5:]\n\nLet’s have a look at the images:\n\nfig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n\nfor img, ax in zip(images, axes.flatten()):\n    ax.imshow(img)\n    ax.axis(\"off\")\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#finding-keypoints",
    "href": "posts/basic-panorama/basic-panorama.html#finding-keypoints",
    "title": "Basic panorama stitching",
    "section": "Finding keypoints",
    "text": "Finding keypoints\nTo estimate the homography matrix, we need correspondence pairs between images. The following is a function for this based on feature matching:\n\ndef get_keypoints(img1, img2):\n    orb = cv2.ORB_create(nfeatures=2000)\n\n    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n    bf = cv2.BFMatcher_create(cv2.NORM_HAMMING)\n\n    # Find matching points\n    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n    good = []\n    for m, n in matches:\n        if m.distance &lt; 0.5 * n.distance:\n            good.append(m)\n    \n    p_source = np.float32([keypoints1[good_match.queryIdx].pt for good_match in good]).reshape(-1, 2)\n    p_target = np.float32([keypoints2[good_match.trainIdx].pt for good_match in good]).reshape(-1, 2)\n    \n    N = p_source.shape[0]\n    p_source = np.concatenate([p_source, np.ones((N, 1))], axis=-1)\n    p_target = np.concatenate([p_target, np.ones((N, 1))], axis=-1)\n    \n    return p_source, p_target\n\nLet’s now look at some correspondence pairs. For this, the we use the draw_matches function:\n\ndef draw_matches(img1, points_source, img2, points_target):\n    ''' Returns an image with matches drawn onto the images.\n    '''\n    r, c = img1.shape[:2]\n    r1, c1 = img2.shape[:2]\n\n    output_img = np.zeros((max([r, r1]), c + c1, 3), dtype='uint8')\n    output_img[:r, :c, :] = np.dstack([img1])\n    output_img[:r1, c:c + c1, :] = np.dstack([img2])\n\n    for p1, p2 in zip(points_source, points_target):\n        (x1, y1) = p1[:2]\n        (x2, y2) = p2[:2]\n\n        cv2.circle(output_img, (int(x1), int(y1)), 10, (0, 255, 255), 10)\n        cv2.circle(output_img, (int(x2) + c, int(y2)), 10, (0, 255, 255), 10)\n\n        cv2.line(output_img, (int(x1), int(y1)), (int(x2) + c, int(y2)), (0, 255, 255), 5)\n\n    return output_img\n\nWe calculate the keypoints:\n\nkeypoint_pairs = [get_keypoints(img1, img2) for img1, img2 in zip(images[:-1], images[1:])]\nsource_points = [pair[0] for pair in keypoint_pairs]\ntarget_points = [pair[1] for pair in keypoint_pairs]\n\nlen(source_points), len(target_points)\n\n(6, 6)\n\n\nCheck how many keypoints we have:\n\nfor i in range(len(source_points)):\n    print(source_points[i].shape, target_points[i].shape)\n\n(305, 3) (305, 3)\n(188, 3) (188, 3)\n(500, 3) (500, 3)\n(482, 3) (482, 3)\n(388, 3) (388, 3)\n(476, 3) (476, 3)\n\n\nVisualise the keypoints:\n\nmatches_to_show = 200\n\nfor img1, p_source, img2, p_target in zip(images[:-1], source_points, images[1:], target_points):\n    f = plt.figure(figsize=(20, 10))\n    vis = draw_matches(img1, p_source[:matches_to_show], img2, p_target[:matches_to_show])\n    plt.axis(\"off\")\n    plt.imshow(vis)"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#estimating-the-homography-matrix",
    "href": "posts/basic-panorama/basic-panorama.html#estimating-the-homography-matrix",
    "title": "Basic panorama stitching",
    "section": "Estimating the homography matrix",
    "text": "Estimating the homography matrix\nAfter looking at the correspondences, let’s stitch the images together! In order to stitch together the images, we need a function to return the 2x9 homography matrix A_i matrix for a given 2D correspondence pair xi_vector and xi_prime_vector (which are 3D homogeneous vectors).\n\ndef get_Ai(xi_vector, xi_prime_vector):\n    ''' Returns the A_i matrix discussed in the lecture for input vectors.\n    \n    Args:\n        xi_vector (array): the x_i vector in homogeneous coordinates\n        xi_vector_prime (array): the x_i_prime vector in homogeneous coordinates\n    '''\n    assert xi_vector.shape == (3, ) and xi_prime_vector.shape == (3, )\n\n    Ai = np.zeros((2, 9))\n    Ai[0, 3:6] = -xi_prime_vector[2] * xi_vector\n    Ai[0, 6:9] = xi_prime_vector[1] * xi_vector\n    Ai[1, 0:3] = xi_prime_vector[2] * xi_vector\n    Ai[1, 6:9] = -xi_prime_vector[0] * xi_vector\n\n    assert(Ai.shape == (2, 9))\n    \n    return Ai\n\nUsing get_Ai, write a function get_A which returns the A matrix of size 2Nx9:\n\ndef get_A(points_source, points_target):\n    ''' Returns the A matrix discussed in the lecture.\n    \n    Args:\n        points_source (array): 3D homogeneous points from source image\n        points_target (array): 3D homogeneous points from target image\n    '''\n    N = points_source.shape[0]\n\n    # Insert your code here\n    A = np.vstack([\n        get_Ai(src, target) for src, target in zip(points_source, points_target)\n    ])\n    \n    assert(A.shape == (2*N, 9))\n    return A\n\nNext, implement the function get_homography which returns the homography H for point correspondence pairs. We obtain H by performing the Direct Linear Transformation (DLT) algorithm:\n\ndef get_homography(points_source, points_target):\n    ''' Returns the homography H.\n    \n    Args:\n        points_source (array): 3D homogeneous points from source image\n        points_target (array): 3D homogeneous points from target image        \n    '''\n\n    # Insert your code here\n    A = get_A(points_source, points_target)\n    _, _, V_T = np.linalg.svd(A)\n    H = V_T.T[:, -1].reshape((3, 3))\n\n    assert H.shape == (3, 3)\n    \n    return H\n\nWe need a function which takes in the two images and the calculated homography and it returns the stiched image in a format which we can display easy with matplotlib. This function is provided in the following.\n\ndef stitch_images(img1, img2, H):\n    ''' Stitches together the images via given homography H.\n\n    Args:\n        img1 (array): image 1\n        img2 (array): image 2\n        H (array): homography\n    '''\n\n    rows1, cols1 = img1.shape[:2]\n    rows2, cols2 = img2.shape[:2]\n\n    list_of_points_1 = np.float32([[0,0], [0, rows1],[cols1, rows1], [cols1, 0]]).reshape(-1, 1, 2)\n    temp_points = np.float32([[0,0], [0,rows2], [cols2,rows2], [cols2,0]]).reshape(-1,1,2)\n\n    list_of_points_2 = cv2.perspectiveTransform(temp_points, H)\n    list_of_points = np.concatenate((list_of_points_1,list_of_points_2), axis=0)\n\n    [x_min, y_min] = np.int32(list_of_points.min(axis=0).ravel() - 0.5)\n    [x_max, y_max] = np.int32(list_of_points.max(axis=0).ravel() + 0.5)\n\n    translation_dist = [-x_min,-y_min]\n\n    H_translation = np.array([[1, 0, translation_dist[0]], [0, 1, translation_dist[1]], [0, 0, 1]])\n    H_final = H_translation.dot(H)\n\n    output_img = cv2.warpPerspective(img2, H_final, (x_max-x_min, y_max-y_min))\n    \n    output_img[translation_dist[1]:rows1+translation_dist[1], translation_dist[0]:cols1+translation_dist[0]] = img1\n    output_img = output_img[translation_dist[1]:rows1+translation_dist[1]:, :]\n\n    # Find top and bottom rows\n    top_row = output_img.nonzero()[0].min()\n    bottom_row = output_img.nonzero()[0].max()\n\n    top_max = output_img[top_row, :].nonzero()[0].max()\n    bottom_max = output_img[bottom_row, :].nonzero()[0].max()\n   \n    # Cut width\n    output_img = output_img[:, :min(top_max, bottom_max)]\n\n    return output_img\n\nWith this, we can stitch two images together and see how it looks:\n\nfor img1, p_source, img2, p_target in zip(images[:-1], source_points, images[1:], target_points):\n    H = get_homography(p_target, p_source)\n    stitched_image = stitch_images(img1, img2, H)\n\n    fig = plt.figure(figsize=(15, 10))\n    plt.axis(\"off\")\n    plt.imshow(stitched_image)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we repeat this process iteratively until all images are stitched together:\n\ndef get_panorama(images):\n    n_repeats = len(images) - 1\n\n    current_images = images\n\n    for _ in range(n_repeats):\n        new_images = []\n\n        keypoint_pairs = [get_keypoints(img1, img2) for img1, img2 in zip(current_images[:-1], current_images[1:])]\n        source_points = [pair[0] for pair in keypoint_pairs]\n        target_points = [pair[1] for pair in keypoint_pairs]\n\n        for img1, p_source, img2, p_target in zip(current_images[:-1], source_points, current_images[1:], target_points):\n            H = get_homography(p_target, p_source)\n            stitched_image = stitch_images(img1, img2, H)\n\n            new_images.append(stitched_image)\n\n        current_images = new_images\n    \n    assert len(current_images) == 1\n\n    return current_images[0]\n\n\npanorama = get_panorama(images)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(panorama)\nplt.axis(\"off\");"
  },
  {
    "objectID": "posts/basic-panorama/basic-panorama.html#conclusion",
    "href": "posts/basic-panorama/basic-panorama.html#conclusion",
    "title": "Basic panorama stitching",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s it! We now have a very basic panorama stitcher.\nThis approach is quite naive and I am sure that real-world algorithms to produce panoramas are much more sophisticated. Therefore, I am open to hear any feedback or suggestions that you may have!"
  },
  {
    "objectID": "posts/uom-project/uom-project.html",
    "href": "posts/uom-project/uom-project.html",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "",
    "text": "Hello, World!\nThis blog post presents my undergraduate project on the numerical solutions to the Navier-Stokes equations. You can find more information by reading the project report here, or check the associated GitHub repository."
  },
  {
    "objectID": "posts/uom-project/uom-project.html#main-ideas",
    "href": "posts/uom-project/uom-project.html#main-ideas",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "Main ideas",
    "text": "Main ideas\nThe idea is simple - using numerical methods to approximate solutions to the Navier-Stokes equations. The particular setup that I used was not too interesting and was chosen because it is a good toy example, you can find all of the details are in the PDF linked above. Here I will present only the main ideas:\n\nCompare different approaches to solve the problem: time-stepping methods versus finding the steady-state solution by using Newton-Raphson iteration.\nCompare different implementations of the methods and their running speeds and memory usage.\nLearn more about optimization and scientific computing and, most important of all, have fun!"
  },
  {
    "objectID": "posts/uom-project/uom-project.html#opportunities-for-growth",
    "href": "posts/uom-project/uom-project.html#opportunities-for-growth",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "Opportunities for growth",
    "text": "Opportunities for growth\nAs mentioned above, I had many opportunities to strengthen my understanding of optimization and scientific computing. In particular, I:\n\nLearnt about various (non)linear optimization methods: least- quares, Newton-Raphson iteration, Gauss-Newton method, LBFGS, etc.\nLearnt how these methods can be used through various Python packages (SciPy, PyTorch, but also JAX which I found to be amazing, expect more projects built on JAX in the future!).\nCompared the performance of the various methods using the different implementations.\nAlso learnt the basic of nbdev and really enjoyed using it, for example, it made testing and publishing the code extremely easy (just for fun, I put up the code on PyPI)."
  },
  {
    "objectID": "posts/uom-project/uom-project.html#findings",
    "href": "posts/uom-project/uom-project.html#findings",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "Findings",
    "text": "Findings\nYou can find many more findings in the linked PDF, but the most interesting findings to me seem to be:\n\nNewton-Raphson iteration is much faster than time-stepping methods (although that is only true in this particular case - it is not always possible to apply this method).\nAs can be seen in this notebook, PyTorch and JAX perform faster both on CPU and GPU, but that’s probably expected.\nOn a GPU, PyTorch and JAX perform mostly the same. I expected JAX to be faster due to the fact that it uses jitting, but maybe the scale of the problem was not large enough to see a real difference (might be an interesting direction for future work)."
  },
  {
    "objectID": "posts/uom-project/uom-project.html#limitations",
    "href": "posts/uom-project/uom-project.html#limitations",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "Limitations",
    "text": "Limitations\nI feel content with the project for now, but here is a (inexhaustive) list of things that could be improved:\n\nThe current solvers are specific (the specifics of the problem are hard-coded into them). It might be worth abstracting the parts that define the problem into a separate entity and make the solvers operate on that entity.\nThere is a lot of code that was written in a rush, so it is not always as elegant as it could be.\nMore tests could be added."
  },
  {
    "objectID": "posts/uom-project/uom-project.html#conclusion",
    "href": "posts/uom-project/uom-project.html#conclusion",
    "title": "Numerical solutions to the Navier-Stokes equations",
    "section": "Conclusion",
    "text": "Conclusion\nI found the project exciting and I hope you found it interesting reading this blog post. It was fun learning JAX and nbdev and highly recommend everyone to try them. I Do not hesitate to contact me if you have any questions. Until next time!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Why is the Ġ Unicode character all over the place in LLM tokenizers?\n\n\n\n\n\n\ndeep learning\n\n\nLLMs\n\n\ntokenization\n\n\n\nThere does not seem to be any good reasons for the Ġ character to be used in LLM tokenizers. This article explores the history of the Ġ character in LLM tokenizers and why it is used.\n\n\n\n\n\nApr 24, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nGood tokenizers is all you need\n\n\n\n\n\n\ndeep learning\n\n\nLLMs\n\n\ntokenization\n\n\n\nTokenization is (at least partially) the cause of a lot of the problems with large language models.\n\n\n\n\n\nApr 19, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising large language model embeddings\n\n\n\n\n\n\ndeep learning\n\n\nLLMs\n\n\nvisualisation\n\n\n\nDimensionality reduction techniques reveal interesting structure in the embeddings learnt by large language models.\n\n\n\n\n\nApr 4, 2024\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nEliciting latent knowledge from language reward models\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\nLLMs\n\n\ncambridge\n\n\n\nBlog post about my master’s research project at the University of Cambridge.\n\n\n\n\n\nOct 4, 2023\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical solutions to the Navier-Stokes equations\n\n\n\n\n\n\nscientific computing\n\n\n\nBlog post about my undergraduate project at the University of Manchester.\n\n\n\n\n\nOct 1, 2023\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nBasic panorama stitching\n\n\n\n\n\n\nmachine learning\n\n\ncomputer vision\n\n\n\nA look at homographies and how they can be used to stitch photos into a panorama.\n\n\n\n\n\nJul 9, 2022\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing optimisation algorithms to optimise parameters of linear functions\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\noptimization\n\n\n\nA sandbox notebook which I used to play around with optimizers.\n\n\n\n\n\nMay 1, 2021\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-output classification for captcha recognition using fastai\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\nclassification\n\n\nfastai\n\n\nvision\n\n\n\nImplementing a multi-output model to recognize easy captcha images using the fastai library.\n\n\n\n\n\nMar 31, 2021\n\n\nAugustas Macijauskas\n\n\n\n\n\n\n\n\n\n\n\n\nElegant implementation of Naive Bayes in just 10 lines of code\n\n\n\n\n\n\nmachine learning\n\n\nnlp\n\n\n\nA tutorial on how to use a few tricks to implement a Naive Bayes model in just a few lines of code.\n\n\n\n\n\nFeb 1, 2021\n\n\nAugustas Macijauskas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "",
    "text": "A follow up on this post."
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#imports",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#imports",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "Imports",
    "text": "Imports\nToggle cells below if you want to see what imports are being made.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer"
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#utils",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#utils",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "Utils",
    "text": "Utils\nUse [CLS] pooling according to this:\n\ndef compute_sentence_embedding(sentence: str, model, tokenizer):\n    sentence_tokenized = tokenizer(sentence, return_tensors=\"pt\")\n\n    print(f\"Num tokens: {sentence_tokenized[\"input_ids\"].shape[1]}\")\n\n    with torch.no_grad():\n        return model(**sentence_tokenized).last_hidden_state[0, 0, :]\n\n\ndef perform_distance_comparison(s1, s2, s3):\n    euclidean_dist_1 = torch.linalg.vector_norm(s1 - s2).item()\n    euclidean_dist_2 = torch.linalg.vector_norm(s1 - s3).item()\n\n    print(f\"|s1 - s2| = {euclidean_dist_1:.3f}\")\n    print(f\"|s1 - s3| = {euclidean_dist_2:.3f}\")\n    print(f\"|s1 - s2| &lt; |s1 - s3| = {euclidean_dist_1 &lt; euclidean_dist_2}\")\n\n    cosine_sim_1 = F.cosine_similarity(s1[None, :], s2[None, :])[0].item()\n    cosine_sim_2 = F.cosine_similarity(s1[None, :], s3[None, :])[0].item()\n\n    print(f\"sim(s1, s2) = {cosine_sim_1:.3f}\")\n    print(f\"sim(s1, s3) = {cosine_sim_2:.3f}\")\n    print(f\"sim(s1, s2) &gt; sim(s1, s3) = {cosine_sim_1 &gt; cosine_sim_2}\")"
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#easier-example",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#easier-example",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "Easier example",
    "text": "Easier example\n\nmodel_name = \"google-bert/bert-base-cased\"\nmodel = AutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\nsentence_1 = \"sensitive information\"\nsentence_2 = \"confidential details\"\nsentence_3 = \"sensitive individual\"\n\n\nsentence_1_transformers = compute_sentence_embedding(sentence_1, model, tokenizer)\nsentence_2_transformers = compute_sentence_embedding(sentence_2, model, tokenizer)\nsentence_3_transformers = compute_sentence_embedding(sentence_3, model, tokenizer)\n\nsentence_1_transformers.shape, sentence_2_transformers.shape, sentence_3_transformers.shape\n\nNum tokens: 4\nNum tokens: 4\nNum tokens: 4\n\n\n(torch.Size([768]), torch.Size([768]), torch.Size([768]))\n\n\n\n# Both should be true\nperform_distance_comparison(\n    sentence_1_transformers, sentence_2_transformers, sentence_3_transformers\n)\n\n|s1 - s2| = 4.981\n|s1 - s3| = 6.788\n|s1 - s2| &lt; |s1 - s3| = True\nsim(s1, s2) = 0.955\nsim(s1, s3) = 0.900\nsim(s1, s2) &gt; sim(s1, s3) = True"
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#harder-example",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#harder-example",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "Harder example",
    "text": "Harder example\n\nsentence_1 = \"your data removal request has been reviewed and concluded\"\nsentence_2 = \"the sensitive personal information has been deleted\"\nsentence_3 = \"she has been a sensitive person\"\n\n\nsentence_1_transformers = compute_sentence_embedding(sentence_1, model, tokenizer)\nsentence_2_transformers = compute_sentence_embedding(sentence_2, model, tokenizer)\nsentence_3_transformers = compute_sentence_embedding(sentence_3, model, tokenizer)\n\nsentence_1_transformers.shape, sentence_2_transformers.shape, sentence_3_transformers.shape\n\nNum tokens: 11\nNum tokens: 9\nNum tokens: 8\n\n\n(torch.Size([768]), torch.Size([768]), torch.Size([768]))\n\n\n\n# Both should be true\nperform_distance_comparison(\n    sentence_1_transformers, sentence_2_transformers, sentence_3_transformers\n)\n\n|s1 - s2| = 5.626\n|s1 - s3| = 7.529\n|s1 - s2| &lt; |s1 - s3| = True\nsim(s1, s2) = 0.941\nsim(s1, s3) = 0.893\nsim(s1, s2) &gt; sim(s1, s3) = True"
  },
  {
    "objectID": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#try-the-same-with-a-text-embedding-model",
    "href": "notes/contextualised-embedding-visualisation/contextualised-embedding-visualisation.html#try-the-same-with-a-text-embedding-model",
    "title": "Visualising contextualised large language model embeddings with context",
    "section": "Try the same with a text embedding model",
    "text": "Try the same with a text embedding model\n\nmodel_name = \"mixedbread-ai/mxbai-embed-large-v1\"\nmodel = AutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\nsentence_1_transformers = compute_sentence_embedding(sentence_1, model, tokenizer)\nsentence_2_transformers = compute_sentence_embedding(sentence_2, model, tokenizer)\nsentence_3_transformers = compute_sentence_embedding(sentence_3, model, tokenizer)\n\nsentence_1_transformers.shape, sentence_2_transformers.shape, sentence_3_transformers.shape\n\nNum tokens: 11\nNum tokens: 9\nNum tokens: 8\n\n\n(torch.Size([1024]), torch.Size([1024]), torch.Size([1024]))\n\n\n\nperform_distance_comparison(\n    sentence_1_transformers, sentence_2_transformers, sentence_3_transformers\n)\n\n|s1 - s2| = 14.404\n|s1 - s3| = 19.134\n|s1 - s2| &lt; |s1 - s3| = True\nsim(s1, s2) = 0.672\nsim(s1, s3) = 0.372\nsim(s1, s2) &gt; sim(s1, s3) = True"
  }
]