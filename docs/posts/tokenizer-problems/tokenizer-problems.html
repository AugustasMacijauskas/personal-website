<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Augustas Macijauskas">
<meta name="dcterms.date" content="2024-04-19">
<meta name="description" content="Tokenization is (at least partially) the cause of a lot of the problems with large language models.">

<title>Augustas Macijauskas - Good tokenizers is all you need</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/favicon.ico" rel="icon">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1M6C1LWNHY"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-1M6C1LWNHY', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean"
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Augustas Macijauskas - Good tokenizers is all you need">
<meta property="og:description" content="Tokenization is (at least partially) the cause of a lot of the problems with large language models.">
<meta property="og:image" content="dalle-3-perplexed-robot.webp">
<meta property="og:site_name" content="Augustas Macijauskas's blog">
<meta name="twitter:title" content="Augustas Macijauskas - Good tokenizers is all you need">
<meta name="twitter:description" content="Tokenization is (at least partially) the cause of a lot of the problems with large language models.">
<meta name="twitter:image" content="dalle-3-perplexed-robot.webp">
<meta name="twitter:creator" content="@augustasmac">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Augustas Macijauskas</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/AugustasMacijauskas/" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/augustas-macijauskas/" target="_blank"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/augustasmac" target="_blank"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Good tokenizers is all you need</h1>
                  <div>
        <div class="description">
          Tokenization is (at least partially) the cause of a lot of the problems with large language models.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">deep learning</div>
                <div class="quarto-category">LLMs</div>
                <div class="quarto-category">tokenization</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Augustas Macijauskas </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 19, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#good-tokenizers-is-all-you-need" id="toc-good-tokenizers-is-all-you-need" class="nav-link active" data-scroll-target="#good-tokenizers-is-all-you-need">Good tokenizers is all you need</a></li>
  <li><a href="#tokenizer-problems" id="toc-tokenizer-problems" class="nav-link" data-scroll-target="#tokenizer-problems">Tokenizer problems</a>
  <ul class="collapse">
  <li><a href="#spelling" id="toc-spelling" class="nav-link" data-scroll-target="#spelling">Spelling</a></li>
  <li><a href="#string-operations" id="toc-string-operations" class="nav-link" data-scroll-target="#string-operations">String operations</a></li>
  <li><a href="#non-english-languages-and-code" id="toc-non-english-languages-and-code" class="nav-link" data-scroll-target="#non-english-languages-and-code">Non-English languages and code</a></li>
  <li><a href="#arithmetic" id="toc-arithmetic" class="nav-link" data-scroll-target="#arithmetic">Arithmetic</a></li>
  <li><a href="#special-data-formats" id="toc-special-data-formats" class="nav-link" data-scroll-target="#special-data-formats">Special data formats</a></li>
  <li><a href="#other-problems" id="toc-other-problems" class="nav-link" data-scroll-target="#other-problems">Other problems</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">Further reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="good-tokenizers-is-all-you-need" class="level1">
<h1>Good tokenizers is all you need</h1>
<p>Although it might seem like the title is just me giving in to the popular trends in naming things in the ML space, it is not actually that misleading. Tokenization is a crucial part of the whole language modelling pipeline. Yet, we will see in this blog post that there all sorts of problems that tokenization can cause that one might not be aware of.</p>
<p>This post is inspired by the <a href="https://www.youtube.com/watch?v=zduSFxRajkE" target="_blank">amazing video</a> on LLM tokenizers by Andrej Karpathy.</p>
<p>Karpathy‚Äôs list of problems (taken from the <a href="https://github.com/karpathy/minbpe/blob/master/lecture.md" target="_blank">following notes</a>):</p>
<ul>
<li>Why can‚Äôt LLM spell words? <strong>Tokenization.</strong></li>
<li>Why can‚Äôt LLM do super simple string processing tasks like reversing a string? <strong>Tokenization.</strong></li>
<li>Why is LLM worse at non-English languages (e.g.&nbsp;Japanese)? <strong>Tokenization.</strong></li>
<li>Why is LLM bad at simple arithmetic? <strong>Tokenization.</strong></li>
<li>Why did GPT-2 have more than necessary trouble coding in Python? <strong>Tokenization.</strong></li>
<li>Why did my LLM abruptly halt when it sees the string <code>&lt;|endoftext|&gt;</code>? <strong>Tokenization.</strong></li>
<li>What is this weird warning I get about a <code>trailing whitespace</code>? <strong>Tokenization.</strong></li>
<li>Why did the LLM break if I ask it about <code>SolidGoldMagikarp</code>? <strong>Tokenization.</strong></li>
<li>Why should I prefer to use YAML over JSON with LLMs? <strong>Tokenization.</strong></li>
<li>Why is LLM not actually end-to-end language modeling? <strong>Tokenization.</strong></li>
<li>What is the real root of suffering? <strong>Tokenization.</strong></li>
</ul>
<p>I will use the <a href="https://augustasmacijauskas.github.io/trailtoken/" target="_blank"><code>trailtoken</code></a> tool that I have recently built with a collaborator to inspect why these problems occur. I encourage you to play around with it, especially if you build tokenizers from scratch yourself. You might be surprised with how easily problems can occur if one is not careful enough!</p>
<p>Let‚Äôs dive in!</p>
</section>
<section id="tokenizer-problems" class="level1">
<h1>Tokenizer problems</h1>
<section id="spelling" class="level2">
<h2 class="anchored" data-anchor-id="spelling">Spelling</h2>
<p>Tokenization makes it hard for the LLMs to spell. For example, one of the best open-source LLMs, <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" target="_blank"><code>Mistral-7B-Instruct</code></a>, has a very hard time spelling the word <code>antidisestablishmentarianism</code>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mistral-instruct-spelling.png" class="img-fluid figure-img"></p>
<figcaption>The mistralai/Mistral-7B-Instruct-v0.1 LLM misspelling the word antidisestablishmentarianism</figcaption>
</figure>
</div>
<p>However, if we use <code>trailtoken</code> to inspect how the said word is tokenized by the <code>Mistral-7B</code> tokenizer, we see that it is actually split into 8 seemingly random tokens:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mistral-trailtoken.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes the word antidisestablishmentarianism</figcaption>
</figure>
</div>
<p>It is unlikely that the model has seen them occurring together during training, so it no surprise that it finds it hard separating out the letters constituting each token.</p>
<p>On the other hand, the OpenAI models seem to handle the task with ease (see these following links for <a href="https://chat.openai.com/share/f56ea3ff-7e5b-4a65-9611-18143d3e17bc" target="_blank"><code>gpt-3.5-turbo</code></a> and <a href="https://chat.openai.com/share/d33744ee-10a0-4a4f-b097-66ecad6019f8" target="_blank"><code>gpt-4-turbo</code></a>). Even then, this is mostly a result of the capabilities of these models, as the word is still tokenized into 6 tokens which are again more or less arbitrary:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-3.5-turbo-trailtoken.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of gpt-3.5-turbo tokenizes the word antidisestablishmentarianism</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Here and below I use the <code>Xenova/gpt-3.5-turbo</code> tokenizer on trailtoken because it is an open-source implementation of the <code>gpt-3.5-turbo</code> tokenizer.</p>
</div>
</div>
</section>
<section id="string-operations" class="level2">
<h2 class="anchored" data-anchor-id="string-operations">String operations</h2>
<p>Similarly to spelling words, LLMs find it hard to to simple string operations, such as reversing words. A well-known example is asking LLMs to reverse the word <code>lollipop</code>. <code>Mistral-7B</code> fails miserably on the task:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mistral-lollipop-fail.png" class="img-fluid figure-img"></p>
<figcaption>mistralai/Mistral-7B-Instruct-v0.1 LLM failing to reverse the word lollipop</figcaption>
</figure>
</div>
<p>If we look at how the word is tokenized, we see that it ends up being only 4 tokens, so it is no wonder that the LLMs find it hard solving this task:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mistral-instruct-reverse.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes the word lollipop</figcaption>
</figure>
</div>
<p>The screenshot above hints at a ‚Äúhack‚Äù that can be used to help LLMs: separate out the letters so that they end up as separate tokens after tokenization. Even then, few-shot examples are required to make it work (and it also did not work when I separated the letters using dashes <code>'-'</code> or spaces <code>' '</code>):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mistral-lollipop-success.png" class="img-fluid figure-img"></p>
<figcaption>mistralai/Mistral-7B-Instruct-v0.1 LLM succeeding at reversing the word lollipop</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Interestingly, <code>gpt-4</code> <a href="https://chat.openai.com/share/f79bf298-94b6-4ebc-9f44-cdc176bf1a7d" target="_blank">fails</a> reversing the string too, so it is truly a problem caused by tokenization!</p>
</div>
</div>
<p>For the curious reader, there are many other seemingly ‚Äúsimple‚Äù operations that LLMs struggle with. One such example is try to count the number of letter in a word:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-4-count-letters.png" class="img-fluid figure-img"></p>
<figcaption>GPT-4 LLM failing to count the number of letters in words</figcaption>
</figure>
</div>
<p>Can you think of any other tasks where LLMs might struggle?</p>
</section>
<section id="non-english-languages-and-code" class="level2">
<h2 class="anchored" data-anchor-id="non-english-languages-and-code">Non-English languages and code</h2>
<p>It turns out that not all tokenizers are equally good at tokenizing non-English text, be it materials in foreign languages or code. I put these two together because they are very similar in nature, which is that training tokenizers on unbalanced text corpuses makes them worse at tokenizing the under-represented pieces of text (I might write a separate post on this, so stay tuned!).</p>
<p>Let‚Äôs see this in action. The following piece of text is taken from Karpathy‚Äôs video referenced above. The text states something like <em>‚ÄúNice to meet you, I‚Äôm ChatGPT, a large-scale language model developed by OpenAI. If you have any questions, feel free to ask.‚Äù</em> in Korean.</p>
<div id="cell-10" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="bu">str</span> <span class="op">=</span> <span class="st">"ÎßåÎÇòÏÑú Î∞òÍ∞ÄÏõåÏöî. Ï†ÄÎäî OpenAIÏóêÏÑú Í∞úÎ∞úÌïú ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏Ïù∏ ChatGPTÏûÖÎãàÎã§. Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÎ©¥ Î¨¥ÏóáÏù¥Îì† Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöî."</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="bu">len</span>(<span class="bu">str</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>72</code></pre>
</div>
</div>
<p>We can see that the string is made up of 72 characters. Let‚Äôs see how the older <code>GPT-3</code> tokenizer would handle this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-3-korean.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of gpt-3 tokenizes Korean</figcaption>
</figure>
</div>
<p>We find that the resulting number of tokens is 132, much more than the original length of the string! This is because the tokenizer probably did not see much Korean during training, so it did not learn to represent the Hanguls (Korean ‚Äúletters‚Äù) in the Korean alphabet efficiently (we see that the first Hangul is represented by three tokens).</p>
<p>On the other hand, the <code>GPT-4</code> tokenizer is much better at Korean, the resulting number of tokens is 61 (more 2x less!), meaning that the Korean Hanguls are better represented in the larger tokenizer vocabulary:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-4-korean.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of gpt-4 tokenizes Korean</figcaption>
</figure>
</div>
<p>However, the problem still remains. If we inspect how the English version of the same phrase is tokenizer, we find another 2x reduction in the number of tokens:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-4-english.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of gpt-4 tokenizes English</figcaption>
</figure>
</div>
<p>Therefore, poor tokenization means that the ‚Äúinformation density‚Äù of non-English tokens is much lower, so LLMs have to attend to longer sequences to process the same amount of information, though the fact that they have probably seen much less non-English data during training also significantly contributes to the problem.</p>
<p>Same thing goes for code. We find that the original code string is 197 characters:</p>
<div id="cell-13" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>code_str <span class="op">=</span> <span class="st">'''for i in range(1, 101):</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="st">    if i % 3 == 0 and i % 5 == 0:</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="st">        print("FizzBuzz")</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="st">    elif i % 3 == 0:</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="st">        print("Fizz")</span></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="st">    elif i % 5 == 0:</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="st">        print("Buzz")</span></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="st">    else:</span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="st">        print(i)</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="st">'''</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="bu">len</span>(code_str)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>197</code></pre>
</div>
</div>
<p>The <code>GPT-3</code> tokenizer compresses the string reasonably well, but there are a few areas for improvement. For instance, we see that every space of indentation is a separate token, so we waste a lof of tokens. Similarly, keywords like <code>elif</code> are broken into two tokens which is not ideal.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-3-code.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of gpt-3 tokenizes code</figcaption>
</figure>
</div>
<p>Just as before, the <code>GPT-4</code> tokenizer is much better at this, as we see in the image below. The indentation spaces are now grouped into a single token, and the <code>elif</code> keyword is a single token as well. The resulting tokenization saves about 30 tokens for the given piece of code. Imagine what the savings would be for a very large codebase!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-4-code.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of gpt-4 tokenizes code</figcaption>
</figure>
</div>
<p>There is no doubt that these improvements in the tokenizer have contributed to the much improved coding and multilingual performance of the <code>GPT-4</code> model.</p>
</section>
<section id="arithmetic" class="level2">
<h2 class="anchored" data-anchor-id="arithmetic">Arithmetic</h2>
<p>This is going to be a brief one, but LLM tokenizers can do odd things when tokenizing numbers which can hinder their performance on simple arithmetic. For example, here is the <code>GPT-2</code> tokenizer behaves on the following numbers:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt2-arithmetic.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of gpt2 tokenizes numbers</figcaption>
</figure>
</div>
<p>We find that in one case the decimal is one token, but becomes two tokens if an extra <code>0</code> is appended at the end. This would not cause any trouble to us humans, but could potentially confuse the LLM.</p>
<p>However, most of the modern LLM tokenizers are quite robust at handling numbers.</p>
</section>
<section id="special-data-formats" class="level2">
<h2 class="anchored" data-anchor-id="special-data-formats">Special data formats</h2>
<p>How structured data tokenized is another source of peculiar behaviours in LLMs. For example the images below show how the <code>Mistral-7B</code> tokenizer handles <code>JSON</code> and <code>YAML</code> formats:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mistral-json.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes JSON</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mistral-yaml.png" class="img-fluid figure-img"></p>
<figcaption>Visualisation of how the tokenizer of mistralai/Mistral-7B-Instruct-v0.1 tokenizes YAML</figcaption>
</figure>
</div>
<p>As we can see, we can save up on tokens (about 2x!) and overall complexity if we use <code>YAML</code> over <code>JSON</code>. Also, notice how using underscores in key names adds extra tokens, so being smart about how structured data is passed to the tokenizer can help reduce the number of tokens used and, therefore, the compute costs!</p>
</section>
<section id="other-problems" class="level2">
<h2 class="anchored" data-anchor-id="other-problems">Other problems</h2>
<p>Finally, I want to briefly touch upon the few remaining problems on Karpathy‚Äôs list which have either been solved completely or are very rare if one is careful enough.</p>
<p><strong>Why did my LLM abruptly halt when it sees the string <code>&lt;|endoftext|&gt;</code>?</strong></p>
<p><code>&lt;|endoftext|&gt;</code> is a special tokens which hints LLMs that they should stop generating text, so if you accidentally put that into the model, it might halt abruptly. Simple string processing techniques can help deal with such edge cases most of the time, though that sometimes leads to undesired behaviours, such as the one below which likely happens because OpenAI‚Äôs tokenizers remove special tokens from the input string during pre-processing:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-4-end-of-text.png" class="img-fluid figure-img"></p>
<figcaption>gpt-4 tokenizers remove special tokens from the input string during pre-processing</figcaption>
</figure>
</div>
<p><strong>What is this weird warning I get about a <code>trailing whitespace</code>?</strong></p>
<p>The two images below will help us better understand what‚Äôs going on:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-4-trailing-whitespace.png" class="img-fluid figure-img"></p>
<figcaption>gpt-4 tokenizer trailing whitespace issue</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-4-trailing-whitespace-2.png" class="img-fluid figure-img"></p>
<figcaption>gpt-4 tokenizer without trailing whitespace issue</figcaption>
</figure>
</div>
<p>We can see that in the first image, the trailing space becomes a separate token. However, the tokenizers often concatenate words the the spaces before them, just like the for word <code>once</code> in the second image. Since models are trained to predict the next token, during training the learn to predict the token <code>once</code> from the tokens that go before it up to the colon <code>:</code>, so the case where they have to predict the token <code>once</code> following a space <code>' '</code> is completely out of distrubution for them and can lead to all kinds of peculiar behaviours.</p>
<p><strong>Why did the LLM break if I ask it about <code>SolidGoldMagikarp</code>?</strong></p>
<p>This problem was quite hilarious, but the researchers at OpenAI trained their olders tokenizers on Reddit data and <code>SolidGoldMagikarp</code> is a Reddit user with lots of posts, so their username became a separate token in the LLMs vocabulary, but the same data did not make it into the training data, so the models behave more or less <strong>randomly</strong> if they ever see this token because they have never been trained to handle them.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>To summarize, the tokenization process of the language modelling pipeline should not be neglected and can lead to poor LLMs performance, undesired behaviours or even safety issues. However, I would like to challenge a few of Andrej Karpathy‚Äôs statements from the conclusion of the video. In particular:</p>
<ul>
<li>He suggested using OpenAI‚Äôs <code>tiktoken</code> library and use the <code>GPT-4</code> tokenizer. While this is true, I believe that there are also a lot of great tokenizers in the open-source realm, e.g.&nbsp;the new <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" target="_blank"><code>LLaMA 3</code></a> tokenizer seems to take most of the consideration from this post into account.</li>
<li>If one would like to train their own tokenizer, Andrej suggested using BPE with sentencepiece from Google. However, I think this neglects the amazing work by the Hugging Face team on their <code>tokenizers</code> library. It has functionality to both train tokenizers and run them in inference mode, it is super fast because it is written in Rust, and the documentation is also rather approachable in my opinion. In fact, I plan to do a deep dive on tokenization using this library where the end goal would be to replicate the <code>LLaMA 3</code> tokenizer as closely as possible.</li>
</ul>
<p><strong>Quick footnote:</strong> in his video on <a href="https://www.youtube.com/watch?v=2-SPH9hIKT8" target="_blank">on building LLMs in 2024</a> Thomas Wolfe video suggests that things are not as hopeless as they might seem and argues that as long as we are reasonably thoughtful about the tokenization process, the problems above can be avoided for the most part. He also adds that one should not spend too much time on this step of building a language model as it is not something that can bring one to building AGI üòÖ.</p>
</section>
<section id="further-reading" class="level1">
<h1>Further reading</h1>
<p>I just want to highlight some resources that I read in more or less detail while writing this article (they are also mentioned in Andrej Karpathy‚Äôs video):</p>
<ul>
<li>Wikipedia page on <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" target="_blank">byte pair encoding</a>.</li>
<li><a href="https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/" target="_blank">Integer tokenization is insane</a>.</li>
<li><a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation" target="_blank">SolidGoldMagikarp (plus, prompt generation)</a>.
<ul>
<li>I really like the very simple yet powerful idea of clustering the embeddings for each token learnt during LLM pre-training which allowed to find the various edge cases in model behaviour. I explore the underlying ideas in some detail myself in the following: <a href="../../posts/embedding-visualisation/embedding-visualisation.html" target="_blank">following</a> blog post.</li>
</ul></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>