{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Tokenizers deep dive\n",
    "description: Deep dive into understanding and building tokenizers with an end goal of replicating the LLaMA 3 tokenizer.\n",
    "author: Augustas Macijauskas\n",
    "date: \"2024/05/03\"\n",
    "image: dalle-3-tokenization-process-1.webp\n",
    "image-alt: LLM tokenization process as imaged by DALL-E 3\n",
    "toc: true\n",
    "categories: [deep learning, LLMs, tokenization]\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the conclusion of my <a href=\"../tokenizer-problems/tokenizer-problems.html\" target=\"_blank\">recent blog post</a> I argued that I disagree with Andrej Karpathy's claims about the current state of tokenizer availability and tooling. In particular:\n",
    "\n",
    "- I do not agree that there are no good alternatives to OpenAI's GPT-4 tokenizer and their `tiktoken` library, especially since the <a href=\"https://ai.meta.com/blog/meta-llama-3/\" target=\"_blank\">release of Llama 3</a>.\n",
    "- Also, it is not the case that there are no good tools to train tokenizers from scratch fast, as the `tokenizers` library from Hugging Face has the functionality for both training and running tokenizers in inference mode. It is built in Rust, so it is extremely fast, and has reasonably approachable documentation.\n",
    "\n",
    "This prompted me to do a deep dive into tokenizers and how one would go about building one from scratch. Inspired by the <a href=\"https://www.fast.ai/posts/2016-10-08-teaching-philosophy.html#good-education-starts-with-the-whole-game\" target=\"_blank\">top-down approach</a> from Jeremy Howard, whose courses I enjoy a lot, this blog post will start with the very basics of tokenizers and then focus on how to train a Llama 3-like tokenizer on your own data with as little code as possible. Finally, we will explore what influence different proportions of English/non-English/code data have on the final vocabulary learnt by the tokenizer, as well as discuss a paper on how tokenizer design choices impact the downstream performance of the LLM.\n",
    "\n",
    "In the second part, we will explore what influence different proportions of English/non-English/code data have on the final vocabulary learnt by the tokenizer, as well as discuss a paper on how tokenizer design choices impact the downstream performance of the LLM, so stay tuned! Now, let's jump right in!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-up: basics of tokenization\n",
    "\n",
    "I am mostly assuming that if you found and opened this article, you have at least a basic understanding of what tokenization is and what its purpose is, but let's do a quick revision just to make sure we are all on the same page.\n",
    "\n",
    "**Why do we need tokenization in the first place?**\n",
    "\n",
    "1. Majority of machine learning models operate on numbers, and (large) language models are no exception.\n",
    "2. Whether it is natural language, code, or something else, we want to be able to input strings into the language models.\n",
    "3. This is where tokenization comes in, it is a **process of converting strings of text into numbers** that can then be fed into language models.\n",
    "\n",
    ":::{.callout-note}\n",
    "This last point is actually the reason why the current state-of-the-art language models are not actually end-to-end systems.\n",
    ":::\n",
    "\n",
    "**How do tokenizers work on a high level?**\n",
    "\n",
    "1. First, the long input string is split into smaller chunks called *tokens*.\n",
    "1. Then, a *vocabulary*, which is a mapping from known tokens to integers, is used to convert the tokens to integer ids.\n",
    "1. These ids can be fed as input into language models.\n",
    "\n",
    "**What are some qualities that we want our tokenizers to have?**\n",
    "\n",
    "We want our tokenizers to be able to process various kinds of inputs. In this article, we will not be able to fully dig into the various design choices made when building tokenizers to achieve this, but here is a (likely non-exhaustive) list of qualities that we want the tokenizers to have:\n",
    "\n",
    "1. They should work on both English and non-English texts, both uppercase and lowercase, both Latin and non-Latin alphabet.\n",
    "1. They should be able to handle various kinds of special characters that we might find in the wild, such as emojis ü§ó.\n",
    "1. They should be able to handle code.\n",
    "1. (Optional) They should be able to tokenize unseen words without the need to include a special `<unk>` token in their vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does this look in code?**\n",
    "\n",
    "Before diving into training a tokenizer, let's illustrate some of the aspects described above in code. We will find that all of the above is very simple to do by using the `tokenizers` library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Oi5gbToTf2Nb"
   },
   "outputs": [],
   "source": [
    "#|code-fold: true\n",
    "#|output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from tokenizers import (\n",
    "    Tokenizer, Regex, models, pre_tokenizers, processors, decoders\n",
    ")\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801350"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|code-fold: true\n",
    "#|output: false\n",
    "\n",
    "# This code will only be used later when training a tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a tokenizer defined, let's see how we can tokenizer a piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'ƒ†world', ',', 'ƒ†we', \"'re\", 'ƒ†live', 'ƒ†√∞≈Å', '¬§', 'ƒπ', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello world, we're live ü§ó.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the string was split into smaller tokens. The `ƒ†` is how the `Llama 3` tokenizer represents spaces in its vocabulary, so you can actually think of the tokens as ` text`, ` to`, etc.\n",
    "\n",
    ":::{.callout-note}\n",
    "It turns out that it is very likely that nowadays the use of the `ƒ†` Unicode character is a historical artifact that started with the GPT-2 implementation. A very strong evidence for this claim is that the `GPT-4` tokenizer has ordinary spaces in its vocabulary. See @sec-next-steps for more.\n",
    ":::\n",
    "\n",
    "Next, let's turn these tokens into ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9906, 1917, 11, 584, 2351, 3974, 11410, 97, 245, 13]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As simple as that! We could now feed these `token_ids` into a language model, though it is important to note that in practice we would perform everything in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   9906,   1917,     11,    584,   2351,   3974,  11410,     97,\n",
      "            245,     13]]),\n",
      " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "pprint(inputs, sort_dicts=False)  # pprint stands for \"pretty print\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would then call a Hugging Face-compatible model like so:\n",
    "```python\n",
    "# Define the model before this line\n",
    "outputs = model(**inputs)\n",
    "```\n",
    "\n",
    ":::{.callout-note}\n",
    "For the sake of completeness, it is worth noting that one would only tokenize a single string on its own during inference. During training, one would either tokenize the whole dataset in batches before training or tokenize a batch of data on the fly. The former is usually more reliable and allows to restart training more easily if a crash occurs (see Thomas Wolf's <a href=\"https://www.youtube.com/watch?v=2-SPH9hIKT8\" target=\"_blank\">video</a>).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Llama 3 tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section discusses the code needed to train a new tokenizer from an old one using the `tokenizers` library. For now, we will assume that we already have a dataset to train on, and we will simply reuse the architecture of `Llama 3` tokenizer, so that we do not have to worry about the different tokenizer design choices.\n",
    "\n",
    "First, we define a function that takes a Hugging Face datasets and return a batched iterator over it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an iterator over the training split of the dataset\n",
    "def batch_iterator(dataset, batch_size=1000, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i:i+batch_size][\"text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we can go ahead and use it to train a new tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(dataset, verbose=False),\n",
    "    len(tokenizer.get_vocab()),\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length=128256\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer.add_tokens(list(tokenizer.get_added_vocab().keys()))\n",
    "\n",
    "print(f\"Vocab length={len(new_tokenizer.get_vocab())}\")\n",
    "\n",
    "# Save the new tokenizer\n",
    "new_tokenizer.save_pretrained(\"new-llama-tokenizer-english-only\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes some time (about 1 minute on my machine), but the result is a working tokenizer, as easy as that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling a Llama 3 tokenizer ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now dive a bit deeper and see how we could construct the tokenizer ourselves. This is where the different design decisions come into play.\n",
    "\n",
    "Essentially, there are up to five components (most of them being optional) that have to be specified to have a working tokenizer. We will cover them briefly here, but I encourage reading <a href=\"https://huggingface.co/docs/tokenizers/en/components\" target=\"_blank\">this</a> great piece of documentation to learn more about the various components available in the `tokenizers` library that can be used to assemble a tokenizer.\n",
    "\n",
    "The 5 components are:\n",
    "\n",
    "1. **Normalizer** (optional): pre-process the input string for a given use case, e.g. strip accents (`√© -> e`) or make the strings lowercase.\n",
    "1. **Pre-tokenizer** (optional): performs some initial splitting with the main intent being to prevent the existence of tokens that are too long, e.g. ones that connect multiple common words. We could avoid this by having a pre-tokenizer that splits across spaces.\n",
    "1. **Model**: an algorithm that performs the tokenization (i.e. takes strings, splits them an converts into tokens).\n",
    "1. **Post-processor** (optional): used for post-processing the tokenized string, e.g. we could add special tokens or apply some other template.\n",
    "1. **Decoder** (optional): helps using the tokenizer in the opposite directiom. i.e. mapping a list of token ids into readable text. In the simplest case, just the model's vocabulary is enough to perform the reversal, but certain normalizers or pre-tokenizers add special characters which are removed by the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this knowledge, we can now build our own `Llama 3` tokenizer! We can check <a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/tokenizer.json\" target=\"_blank\">this</a> file to learn what the hyperparamaters are and just copy them over. It turns out that `Llama 3` does not use a normalizer (this is because normalization usually makes tokenization an irreversible process, e.g. if you remove accents, you cannot recover them during decoding), while the pre-tokenizer consists of a regex splitter and a byte-level pre-tokenizer. To understand the former, see <a href=\"https://www.youtube.com/watch?v=zduSFxRajkE\" target=\"_blank\">this</a> Andrej Karpathy's video on tokenizers from `57:36` to `01:14:59`, while for the latter, BPE tokenizers are trained by starting with an initial vocabulary and then iteratively merging the most common byte pairs until the desired vocabulary size is reached. Back when `GPT-2` was created, instead of using the bytes corresponding to the first 256 Unicode characters, researchers decided to remap them to some other set of bytes. The reasons for this are not enrirely clear, but it is likely that using the set of the leading 256 bytes caused errors under their implementation.\n",
    "\n",
    ":::{.callout-note}\n",
    "This byte-level normalizer is actually the reason why the `ƒ†` characters occur, since the space character `\" \"` is mapped to the `ƒ†` character during normalization.\n",
    ":::\n",
    "\n",
    "Before jumping further into the model and decoder used by `Llama 3`, let's quickly see these concepts in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', ',', ' we', \"'re\", ' live', ' ü§ó.', ' The', ' number', ' is', ' ', '123', '4', '.', ' How', \"'s\", ' it', ' going', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt_4_pat = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "some_text = \"Hello world, we're live ü§ó. The number is 1234. How's it going!!!?\"\n",
    "print(re.findall(gpt_4_pat, some_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the regex splits by whitespace, numbers are no longer than 3 digits, and apostrophe abbreviations and punctuation are also separated out (the list is non-exhaustive). As the for `ByteLevel` pre-tokenizer, we can see that ordinary English characters remain unchanged, spaces are indeed replaced by the `ƒ†` Unicode character, and the emoji is completely replace by a combination of multiple bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Helloƒ†world,ƒ†we'reƒ†liveƒ†√∞≈Å¬§ƒπ.ƒ†Theƒ†numberƒ†isƒ†1234.ƒ†How'sƒ†itƒ†going!!!?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.ByteLevel(\n",
    "    add_prefix_space=False, trim_offsets=True, use_regex=False\n",
    ")\n",
    "pre_tokenization_result = pre_tokenizer.pre_tokenize_str(some_text)[0][0]\n",
    "pre_tokenization_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the components utilized by the Llama 3 tokenizer, the model employs the Byte Pair Encoding (BPE) algorithm. To avoid duplicating the excellent explanations already available, I encourage you to view the relevant sections of Andrej Karpathy's video or consult the corresponding <a href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding\" target=\"_blank\">Wikipedia entry</a>. The `ByteLevel` post-processor simply reverses the byte shifting that took place in the the pre-tokenization phase, as well as prepends the output string with the `<|begin_of_text|>` special token (this can be turned off by setting `add_special_tokens=False` when calling a tokenizer). Finally, the `ByteLevel` decoder is responsible for mapping token ids back to human-readable strings.\n",
    "\n",
    ":::{.callout-note}\n",
    "How these design choices are made when building a *completely* new tokenizer from scratch is out of scope of this article, but it turns out that, in practice, it is enough to simply use a BPE model and only tune dataset used for training, the vocabulary size and the regular expression used to split strings during pre-tokenization.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assembling our own `Llama 3` tokenizer**\n",
    "\n",
    "That was a lot to take in but we can finally assemble our own Llama 3 tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the most important part - the model\n",
    "tokenizer_custom = Tokenizer(models.BPE(ignore_merges=True))\n",
    "\n",
    "# Add a pre-tokenizer\n",
    "tokenizer_custom.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.Split(\n",
    "        pattern=Regex(\n",
    "            \"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\n",
    "        ),\n",
    "        behavior=\"isolated\",\n",
    "        invert=False,\n",
    "    ),\n",
    "    pre_tokenizers.ByteLevel(\n",
    "        add_prefix_space=False, trim_offsets=True, use_regex=False\n",
    "    )\n",
    "])\n",
    "\n",
    "# Add the post-processor\n",
    "tokenizer_custom.post_processor = processors.Sequence([\n",
    "    processors.ByteLevel(\n",
    "        add_prefix_space=True, trim_offsets=False, use_regex=True\n",
    "    ),\n",
    "    processors.TemplateProcessing(\n",
    "        single=\"<|begin_of_text|> $A\",\n",
    "        pair=\"<|begin_of_text|> $A <|begin_of_text|>:1 $B:1\",\n",
    "        special_tokens=[(\"<|begin_of_text|>\", 128000)],\n",
    "    ),\n",
    "])\n",
    "\n",
    "# And finally, the decoder\n",
    "tokenizer_custom.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "from tokenizers import trainers\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    # the `vocab_size` here actually means the number of merges\n",
    "    vocab_size=len(tokenizer.get_vocab()) - len(pre_tokenizers.ByteLevel.alphabet()),\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")\n",
    "\n",
    "tokenizer_custom.train_from_iterator(\n",
    "    batch_iterator(dataset, verbose=False), trainer=trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length=128256\n"
     ]
    }
   ],
   "source": [
    "tokenizer_custom.add_tokens(list(tokenizer.get_added_vocab().keys()))\n",
    "\n",
    "print(f\"Vocab length={len(tokenizer_custom.get_vocab())}\")\n",
    "\n",
    "# Save the new tokenizer\n",
    "tokenizer_custom.save(\"new-llama-tokenizer-custom-english-only/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the resulting tokenizer is indeed the same as the one that we have previously trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"new-llama-tokenizer-english-only/tokenizer.json\", \"r\") as f:\n",
    "    new_tokenizer_config = json.load(f)\n",
    "\n",
    "with open(\"new-llama-tokenizer-custom-english-only/tokenizer.json\", \"r\") as f:\n",
    "    custom_tokenizer_config = json.load(f)\n",
    "\n",
    "custom_tokenizer_config[\"model\"][\"merges\"] == new_tokenizer_config[\"model\"][\"merges\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Upon inspecting the corresponding tokenizer configuration files (<a href=\"./new-llama-tokenizer-english-only/tokenizer.json\" target=\"_blank\">file 1</a> and <a href=\"./new-llama-tokenizer-custom-english-only/tokenizer.json\" target=\"_blank\">file 2</a>) you might notice that the two vocabularies are not exactly the same which is because a tokenizer trained from an old one has all the *added tokens* prepended at the start of the vocabulary. This is why above we check that the merges (i.e. which tokens where merged and in what sequence) are the same for the two tokenizers, which they are.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cYlBfxxkkS5"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "And that is it, we have succesfully replicated the structure of the `Llama 3` tokenizer using the `tokenizers` library! As we can see, it is not that hard if we familiarize ourselves with the API of the `tokenizers` library and know where to find the configuration parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "There are many things that did not make it into this article which I briefly discuss here.\n",
    "\n",
    "1. **How are the design choices actually made and implemented in practice?** Above we just copied the configuration parameters, but how would we go about implementing a state-of-the-art tokenizer ourselves?\n",
    "    - I enjoyed the *Getting the most out of your tokenizer for pre-training and domain adaptation* paper which discusses these topics (<a href=\"https://arxiv.org/abs/2402.01035\" target=\"_blank\">link</a>).\n",
    "    - My takeaways are that one should:\n",
    "        1. Use BPE.\n",
    "        1. Use a diverse training set that is large enough to cover different languages and code well.\n",
    "        1. Choose vocabulary size to trade off text compression and memory overhead well (check the paper to understand what this means). A vocabulary size of 100k seems to be a good starting point, especially for smaller models.\n",
    "1. **The inner workings of the BPE algorithm itself, as well as how one would go about implementing it.** For a proper deep dive, looking into the internals of the BPE algorithm and its implementation would seem like a must, but I think that there are already plenty great resources on the topic. Here are some of them that helped me build good intuitions for what is going on:\n",
    "    - The aforementioned Karpathy's video is a must-see for every LLM practitioner.\n",
    "    - The implementation of the `GPT-2` tokenizer (<a href=\"https://github.com/openai/gpt-2/blob/master/src/encoder.py\">link</a>)\n",
    "    - The <a href=\"https://github.com/openai/tiktoken/blob/main/tiktoken/_educational.py\" target=\"_blank\">educational version</a> of the `tiktoken` library (note that it is meant to be easy to understand, but is not actually very efficient).\n",
    "    - [Much harder] **The sources code of <a href=\"https://github.com/openai/tiktoken\" target=\"_blank\">`tiktoken`</a> and <a href=\"https://github.com/huggingface/tokenizers\" target=\"_blank\">`tokenizers`</a> libraries.** Both of them are written in Rust ü•≤.\n",
    "1. **Tokenizer parameters used during the inference stage.** These mainly include `truncation` and `padding`, they are not inherently complex concepts, so I decided to exclude them here.\n",
    "    - Consult the Hugging Face documentation and various blog posts to learn more about them.\n",
    "\n",
    "I strongly encourage you to check out these resources to get a better sense of what the BPE algorithm is doing and other aspects of tokenizers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open questions (for the curious reader) {#sec-next-steps}\n",
    "\n",
    "I leave here some interesting questions and observations which arose to me while writing this article. I hope to come back to (at least some of) them at some point in the future, but if you end up going down the rabbit holes to find the answers, I would be curious to know them too! Here is the list:\n",
    "\n",
    "1. **Replacing spaces with the `ƒ†` Unicode character in BPE-based tokenizers is not necessary.** The evidence for this is that the `GPT-4` tokenizer no longer uses them, so there is a high chance that it is just a historical artifact that people got used to since the release of `GPT-2` code and never questioned it afterwards.\n",
    "    - Unless there is something that I am missing here, I believe that the community should ideally get rid of them over time.\n",
    "1. The <a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/raw/main/tokenizer.json\" target=\"_blank\">`Llama-3-8B-Instruct`</a> and <a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/raw/main/tokenizer.json\" target=\"_blank\">`Llama-3-70B-Instruct`</a> tokenizers are almost identical apart from one setting: the former has the `model.ignore_merges` key specified as `true`, while the latter does not have such a key specified. The question is simple, what difference does this make? A good starting point to answer this question could be <a href=\"https://github.com/huggingface/tokenizers/blob/71c2a8d01a56cd7bd28148c309e210c47dac78e7/tokenizers/src/models/bpe/model.rs#L466\" target=\"_blank\">this</a> piece of code.\n",
    "1. Running inference using the `tiktoken` library is **much** faster than with the `tokenizers` library.\n",
    "1. The first 100256 (256 initial + 100k merges) tokens of the `GPT-4` and `Llama 3` tokenizers are the same.\n",
    "    - OpenAI never released their training code, so how could Meta know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. Let's build the GPT Tokenizer (<a href=\"https://www.youtube.com/watch?v=zduSFxRajkE\" target=\"_blank\">video</a>) by Andrej Karpathy.\n",
    "1. A little guide to building Large Language Models in 2024 (<a href=\"https://www.youtube.com/watch?v=2-SPH9hIKT8\" target=\"_blank\">video</a>) by Thomas Wolf.\n",
    "1. Training a new tokenizer from an old one (<a href=\"https://huggingface.co/learn/nlp-course/en/chapter6/2\" target=\"_blank\">article</a>) on the Hugging Face course.\n",
    "1. Training from memory (part of <a href=\"https://huggingface.co/docs/tokenizers/en/training_from_memory\" target=\"_blank\">Hugging Face docs</a>)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "easy-captcha-fastai-v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
