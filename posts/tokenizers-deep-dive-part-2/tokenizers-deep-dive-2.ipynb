{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Tokenizers deep dive - Part 2\n",
    "description: Deep dive into understanding and building tokenizers with an end goal of replicating the LLaMA 3 tokenizer.\n",
    "author: Augustas Macijauskas\n",
    "date: \"2024/05/03\"\n",
    "image: dalle-3-tokenization-process-2.webp\n",
    "image-alt: LLM tokenization process as imaged by DALL-E 3\n",
    "toc: true\n",
    "categories: [deep learning, LLMs, tokenization]\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the conclusion of my <a href=\"../tokenizer-problems/tokenizer-problems.html\" target=\"_blank\">recent blog post</a> I argued that I disagree with Andrej Karpathy's claims about the current state of tokenizer availability and tooling. In particular:\n",
    "\n",
    "- I do not agree that there are no good alternatives to OpenAI's GPT-4 tokenizer and their `tiktoken` library, especially since the <a href=\"https://ai.meta.com/blog/meta-llama-3/\" target=\"_blank\">release of Llama 3</a>.\n",
    "- Also, it is not the case that there are no good tools to train tokenizers from scratch fast, as the `tokenizers` library from Hugging Face has the functionality for both training and running tokenizers in inference mode. It is built in Rust, so it is extremely fast, and has reasonably approachable documentation.\n",
    "\n",
    "This prompted me to do a deep dive into tokenizers and how one would go about building one from scratch. Inspired by the <a href=\"https://www.fast.ai/posts/2016-10-08-teaching-philosophy.html#good-education-starts-with-the-whole-game\" target=\"_blank\">top-down approach</a> from Jeremy Howard, whose courses I enjoy a lot, this blog post will start with the very basics of tokenizers and then focus on how to train a Llama 3-like tokenizer on your own data with as little code as possible. Finally, we will explore what influence different proportions of English/non-English/code data have on the final vocabulary learnt by the tokenizer, as well as discuss a paper on how tokenizer design choices impact the downstream performance of the LLM.\n",
    "\n",
    "In the second part, we will explore what influence different proportions of English/non-English/code data have on the final vocabulary learnt by the tokenizer, as well as discuss a paper on how tokenizer design choices impact the downstream performance of the LLM, so stay tuned! Now, let's jump right in!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-up: basics of tokenization\n",
    "\n",
    "I am mostly assuming that if you found and opened this article, you have at least a basic understanding of what tokenization is and what its purpose is, but let's do a quick revision just to make sure we are all on the same page.\n",
    "\n",
    "**Why do we need tokenization in the first place?**\n",
    "\n",
    "1. Majority of machine learning models operate on numbers, and (large) language models are no exception.\n",
    "2. Whether it is natural language, code, or something else, we want to be able to input strings into the language models.\n",
    "3. This is where tokenization comes in, it is a **process of converting strings of text into numbers** that can then be fed into language models.\n",
    "\n",
    ":::{.callout-note}\n",
    "This last point is actually the reason why the current state-of-the-art language models are not actually end-to-end systems.\n",
    ":::\n",
    "\n",
    "**How do tokenizers work on a high level?**\n",
    "\n",
    "1. First, the long input string is split into smaller chunks called *tokens*.\n",
    "1. Then, a *vocabulary*, which is a mapping from known tokens to integers, is used to convert the tokens to integer ids.\n",
    "1. These ids can be fed as input into language models.\n",
    "\n",
    "**What are some qualities that we want our tokenizers to have?**\n",
    "\n",
    "We want our tokenizers to be able to process various kinds of inputs. In this article, we will not be able to fully dig into the various design choices made when building tokenizers to achieve this, but here is a (likely non-exhaustive) list of qualities that we want the tokenizers to have:\n",
    "\n",
    "1. They should work on both English and non-English texts, both uppercase and lowercase, both Latin and non-Latin alphabet.\n",
    "1. They should be able to handle various kinds of special characters that we might find in the wild, such as emojis ü§ó.\n",
    "1. They should be able to handle code.\n",
    "1. (Optional) They should be able to tokenize unseen words without the need to include a special `<unk>` token in their vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does this look in code?**\n",
    "\n",
    "Before diving into training a tokenizer, let's illustrate some of the aspects described above in code. We will find that all of the above is very simple to do by using the `tokenizers` library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Oi5gbToTf2Nb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#|code-fold: true\n",
    "#|output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801350"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: false\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a tokenizer defined, let's see how we can tokenizer a piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some', 'ƒ†text', 'ƒ†to', 'ƒ†be', 'ƒ†token', 'ized']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Some text to be tokenized\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the string was split into smaller tokens. The `ƒ†` is how the `Llama 3` tokenizer represents spaces in its vocabulary, so you can actually think of the tokens as ` text`, ` to`, etc.\n",
    "\n",
    ":::{.callout-note}\n",
    "It turns out that it is very likely that nowadays the use of the `ƒ†` Unicode character is a historical artifact that started with the GPT-2 implementation. A very strong evidence for this claim is that the `GPT-4` tokenizer has ordinary spaces in its vocabulary. See @sec-next-steps for more.\n",
    ":::\n",
    "\n",
    "Next, let's turn these tokens into ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8538, 1495, 311, 387, 4037, 1534]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As simple as that! We could now feed these `token_ids` into a language model, though it is important to note that in practice we would perform everything in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[8538, 1495,  311,  387, 4037, 1534]]),\n",
      " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "pprint(inputs, sort_dicts=False)  # pprint stands for pretty print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would then call a Hugging Face-compatible model like so:\n",
    "```python\n",
    "# Define the model before this row\n",
    "outputs = model(**inputs)\n",
    "```\n",
    "\n",
    ":::{.callout-note}\n",
    "For the sake of completeness, it is worth noting that one would only tokenize a single string on its own during inference. During training, one would either tokenize the whole dataset in batches before training or tokenize a batch of data on the fly. The former is usually more reliable and allows to restart training more easily if a crash occurs (see Thomas Wolf's <a href=\"https://www.youtube.com/watch?v=2-SPH9hIKT8\" target=\"_blank\">video</a>).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Llama 3 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an iterator over the training split of the dataset\n",
    "def batch_iterator(dataset, batch_size=1000, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1801350\n",
      "\n",
      "\n",
      "\n",
      "Vocab length=128256\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(dataset[\"train\"], verbose=True),\n",
    "    len(tokenizer.get_vocab()),\n",
    ")\n",
    "\n",
    "print(f\"Vocab length={len(new_tokenizer.get_vocab())}\")\n",
    "new_tokenizer.save_pretrained(\"new-llama-tokenizer-english-only\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create some test data which we will use as a proxy for how efficient tokenizers trained with different data are in tokenizing various kinds of texts. We will have an English phrase, the same phrase in Korean, and a piece of code in our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_string = \"\"\"for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\"\"\"\n",
    "\n",
    "test_strings = {\n",
    "    \"english\": \"Nice to meet you, I'm ChatGPT, a large-scale language model developed by OpenAI. If you have any questions, feel free to ask.\",\n",
    "    \"korean\": \"ÎßåÎÇòÏÑú Î∞òÍ∞ÄÏõåÏöî. Ï†ÄÎäî OpenAIÏóêÏÑú Í∞úÎ∞úÌïú ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏Ïù∏ ChatGPTÏûÖÎãàÎã§. Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÎ©¥ Î¨¥ÏóáÏù¥Îì† Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöî.\",\n",
    "    \"code\": code_string,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english_only': {'code': 129, 'english': 36, 'korean': 136}}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "results = defaultdict(dict)\n",
    "\n",
    "for language, test_str in test_strings.items():\n",
    "    results[\"english_only\"][language] = new_tokenizer(test_str, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "\n",
    "\n",
    "pprint(dict(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add some Korean and code data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607256 412178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2820784"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "\n",
    "korean_dataset = load_dataset(\"lcw99/wikipedia-korean-20221001\")\n",
    "code_dataset = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\n",
    "code_dataset = code_dataset.rename_column(\"whole_func_string\", \"text\")  # Rename whole_func_string to text\n",
    "print(len(korean_dataset[\"train\"]), len(code_dataset[\"train\"]))\n",
    "\n",
    "# This is only a rough estimate\n",
    "# [0.38661505 0.45621601 0.15716893]\n",
    "# proportions = np.array(\n",
    "    # [sum(len(x.split()) for x in ds[\"train\"][\"text\"]) for ds in [dataset, korean_dataset, code_dataset]]\n",
    "# )\n",
    "# print(proportions / proportions.sum())\n",
    "\n",
    "final_dataset = concatenate_datasets(\n",
    "    [dataset[\"train\"], korean_dataset[\"train\"], code_dataset[\"train\"]]\n",
    ")\n",
    "final_dataset = final_dataset.shuffle(seed=42)\n",
    "len(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2820784\n"
     ]
    }
   ],
   "source": [
    "# new_tokenizer_2 = tokenizer.train_new_from_iterator(\n",
    "#     batch_iterator(final_dataset, verbose=True),\n",
    "#     len(tokenizer.get_vocabx()),\n",
    "# )\n",
    "\n",
    "# print(f\"Vocab length={len(new_tokenizer_2.get_vocab())}\")\n",
    "# new_tokenizer_2.save_pretrained(\"new-llama-tokenizer-english-korean-code\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer_2 = AutoTokenizer.from_pretrained(\"new-llama-tokenizer-all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all': {'code': 75, 'english': 34, 'korean': 30},\n",
      " 'english_only': {'code': 129, 'english': 36, 'korean': 136}}\n"
     ]
    }
   ],
   "source": [
    "for language, test_str in test_strings.items():\n",
    "    results[\"all\"][language] = new_tokenizer_2(test_str, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "\n",
    "\n",
    "pprint(dict(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">For those who would like to dig deeper into training their own tokenizers in a similar setting (English + Code + Multilingual) and what metrics to follow, I suggest the following paper: https://arxiv.org/pdf/2402.01035.pdf</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final notes\n",
    "\n",
    "1. Normalization is not used to make tokenization process reversible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._tokenizer.normalizer is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps (for the curious reader) {#sec-next-steps}\n",
    "\n",
    "I leave here some questions arose while writing this article that I could not find simple answers to. I hope to come back to them at some point in the future, but if you end up going down the rabbit holes to find the answers, I would be curious to know the answers! Here is the list:\n",
    "\n",
    "1. Why are spaces replaced with the `ƒ†` special character in BPE-based tokenizer? The same Thomas Wolf <a href=\"https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475\" target=\"_blank\">argues</a> that this was done do avoid digesting spaces which are used in the standard BPE algorithm. However, it is unclear whether we could get by without doing this nowadays. Answering this question would probably require diving deeper into how the BPE algorithm works. Sounds like a fun direction to explore in the future!\n",
    "    - <font color=\"red\">Another source <a href=\"https://github.com/openai/gpt-2/blob/master/src/encoder.py#L9\" target=\"_blank\">on G dot</a></font>\n",
    "1. The <a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/raw/main/tokenizer.json\" target=\"_blank\">`Llama-3-8B-Instruct`</a> and <a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/raw/main/tokenizer.json\" target=\"_blank\">`Llama-3-70B-Instruct`</a> tokenizers are almost identical apart from one setting: the former has the `model.ignore_merges` key specified as `true`, while the latter does not have such a key specified. The question is simple, what difference does this make? A good starting point to answer this question could be <a href=\"https://github.com/huggingface/tokenizers/blob/71c2a8d01a56cd7bd28148c309e210c47dac78e7/tokenizers/src/models/bpe/model.rs#L466\" target=\"_blank\">this</a> piece of code.\n",
    "1. What are some metrics to track to evaluate the quality of a trained tokenizer?\n",
    "1. <font color=\"red\">The regex used for splitting is not ideal (unicode aposthrophe)</font>\n",
    "1. <font color=\"red\">Original BPE: https://github.com/rsennrich/subword-nmt and https://aclanthology.org/P16-1162.pdf</font>\n",
    "1. <font color=\"red\">FastBPE https://github.com/Yikai-Liao/efficient_bpe</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cYlBfxxkkS5"
   },
   "source": [
    "# Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. Let's build the GPT Tokenizer (<a href=\"https://www.youtube.com/watch?v=zduSFxRajkE\" target=\"_blank\">video</a>) by Andrej Karpathy.\n",
    "1. A little guide to building Large Language Models in 2024 (<a href=\"https://www.youtube.com/watch?v=2-SPH9hIKT8\" target=\"_blank\">video</a>) by Thomas Wolf.\n",
    "1. Training a new tokenizer from an old one (<a href=\"https://huggingface.co/learn/nlp-course/en/chapter6/2\" target=\"_blank\">article</a>) on the Hugging Face course.\n",
    "1. Another Implementation (faster and more effecient) of BPE Training Algorithm (<a href=\"https://github.com/huggingface/tokenizers/issues/1400\" target=\"_blank\">GitHub issue</a>)\n",
    "1. Getting the most out of your tokenizer for pre-training and domain adaptation (<a href=\"https://arxiv.org/pdf/2402.01035.pdf\" target=\"_blank\">arXiv PDF</a>)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "easy-captcha-fastai-v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
