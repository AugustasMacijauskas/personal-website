{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Eliciting latent knowledge from language reward models\n",
    "description: Blog post about my master's project research at the University of Cambridge.\n",
    "author: Augustas Macijauskas\n",
    "date: \"2023/10/04\"\n",
    "image: cambridge.jpg\n",
    "image-alt: King's College Cambridge\n",
    "toc: true\n",
    "categories: [machine learning, deep learning, LLMs, cambridge]\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cYlBfxxkkS5"
   },
   "source": [
    "# Eliciting latent knowledge from language reward models\n",
    "\n",
    "Hello, World!\n",
    "\n",
    "This blog post discusses the main ideas behind my thesis for the MPhil in Machine Learning and Machine Intelligence degree at the University of Cambridge. You can read the full thesis <a href=\"mlmi-thesis.pdf\" target=\"_blank\">here</a>, or check the associated <a href=\"https://github.com/AugustasMacijauskas/mlmi-thesis\" target=\"_blank\">GitHub repository</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind the project is trying to build a reward models that reward \"truthfulness\" in a scalable fashion, which current state-of-the-art methods, such as _reinforcement learning from human feedback_ (RLHF), are not capable of (note that we use quatations because we defined \"truthfulness\" in a narrow sense and mean only the performance on binary question-aswering tasks, see the thesis pdf for more details). Specifically, methods that _discover latent knowledge_, such as <a href=\"https://arxiv.org/abs/2212.03827\" target=\"_blank\">CCS</a>, are used to determine whether a piece of input text is truthful or not. Such linear _probes_ are then combined with pre-trained language models to make up reward models, which are used in _reinforcement learning_ RL fine-tuning to improve the \"truthfulness\" of _large language models_ (LLMs).\n",
    "\n",
    "These reward models can be trained by using transformed versions of existing datasets, thus relaxing the requirement to collect large numbers of human preference data, as is usual in RLHF. We find that using our reward models along with a few regularization techniques (discussed below) can already be used to improve the \"truthfulness\" of pre-trained LLMs by **up to 1.6%**, as measured on the TruthfulQA benchmark. Importantly, such an improvement is achieved without sacrificing the models' performance on more general NLP tasks (we evaluate on the <a href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\" target=\"_blank\">_Open LLM Leaderboard_</a> tasks).\n",
    "\n",
    "Although our method serves as a proof of concept on how hallucinations in LLMs could be tackled in the future, it still has many limitations. For one, the current best DLK methods still have a long way to go in terms of robustness. Moreover, our method only tackles the narrow definition of \"truthfulness\", and even though the accuracy on TruthfulQA improves too, many would argue that it is still not a very good proxy for actually reducing levels of hallucination in LLMs. Finally, we found that the pre-trained models that we would fine-tune using RL had to be already quite capable, otherwise our method would not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main steps\n",
    "\n",
    "There are four main steps to run the method on new data:\n",
    "1. Split the dataset and prepare it for reward model training and RL fine-tuning.\n",
    "1. Train a reward model.\n",
    "1. Performing RL fine-tuning on some pre-trained LLM.\n",
    "1. Evaluate the fine-tuned LLM on both target and general NLP tasks.\n",
    "\n",
    "Steps 1 and 4 are mostly boring and you can find more details about them in the README of the GitHub repository, so we are going to focus on the theory and main code bits for steps 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward model training\n",
    "\n",
    "As discussed in more detail in chapter 3 of the thesis, the reward model is made up of a pre-trained language model with a probe attached at the end.\n",
    "\n",
    "![The architecture of the reward model](reward_model.png)\n",
    "\n",
    "The reward model takes as an input a question $q_i$ with a binary answer (e.g. \"Yes\"/\"No\"), creates a contrastive pair from it and then this contrastive pair $(x_i^+, x_i^-)$ is used to compute a reward (a number between 0 and 1). The reward is computed by recording activations of the last token in a layer of a language model, denoted $\\mathrm{\\textbf{emb}}(x_i^+)$ and $\\mathrm{\\textbf{emb}}(x_i^-)$. We would try all layers of a language model and pick the one that worked the best. Finally, the embeddings are passed to a logistic classificer which is of the form:\n",
    "$$p(q_i) = \\sigma(\\textbf{w}^\\mathrm{T}(\\mathrm{\\textbf{emb}}(x_i^+) - \\mathrm{\\textbf{emb}}(x_i^-)))$$\n",
    "which is the only module with trainable parameters, the vector $\\textbf{w}$. Here, $\\sigma$ is the sigmoid activation function. This output probability denotes the probability that the question $q_i$ is \"truthful\" which is what we use as the reward.\n",
    "\n",
    "There are a few other intricacies, such as how to prompt for \"truthfulness\" (custom prompts are needed), or how to actually find the optimal parameters vector $\\textbf{w}$, but I will sugeest interested readers to refer to the thesis pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL fine-tuning\n",
    "\n",
    "Once we have a reward model, we can plug into an RL algorithm to perform fine-tuning. We used the _proximal policy optimization_ algorithm, as implement in the _Transformer Reinforcement Learning_ (<a href=\"https://github.com/huggingface/trl\" target=\"_blank\">TRL</a>) library from Hugging Face. We found that a few pieces of regularization had to be applied to stabilize the training process. The tricks are:\n",
    "\n",
    "1. **Prompting** - we found that a specialized prompt had to be devised for each model for the method to work (we mostly focused on the 7B <a href=\"https://lmsys.org/blog/2023-03-30-vicuna/\" target=\"_blank\">Vicuna</a> models).\n",
    "1. **Maximum number of new tokens** - we found that setting the number of new tokens to two was enough in our case since answers to our binary questions were short. Additionally, we applied output post-processing to strip any undesirable tokens (see the code below).\n",
    "1. **Encouraging the models to only output in the desired format** - we want the models to only respond with \"Yes\"/\"No\", but even with specialized prompts the models would still sometimes generate different responses. To tackle this, we tweaked the reward to be -1 if the model does not respond in the desired format, and we would give the usual score from the reward model if the output was what the model was asked for. This encouraged the model to converge to only responding with the required format over time.\n",
    "\n",
    "To illustrate these concepts, the finel RL training loop looked roughly like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "\n",
    "CHARACTERS_TO_FILTER = string.punctuation + \" \\n\"\n",
    "\n",
    "\n",
    "def is_answer_yes_no(answer):\n",
    "    return answer in [\"Yes\", \"No\"]\n",
    "\n",
    "\n",
    "def postprocess_response(response):\n",
    "    while response and response[-1] in CHARACTERS_TO_FILTER:\n",
    "        response = response[:-1]\n",
    "    return response\n",
    "\n",
    "\n",
    "def train(\n",
    "    ppo_trainer,\n",
    "    tokenizer,\n",
    "    generation_kwargs,\n",
    "    get_rewards,\n",
    "    script_args, config,\n",
    "):\n",
    "    n_epochs = config.steps // len(ppo_trainer.dataloader)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loop = tqdm(\n",
    "            enumerate(ppo_trainer.dataloader, 1),\n",
    "            total=len(ppo_trainer.dataloader), leave=False\n",
    "        )\n",
    "        for batch_idx, batch in loop:\n",
    "            # Get the input tensors\n",
    "            question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "            # Get the generations\n",
    "            response_tensors = ppo_trainer.generate(\n",
    "                question_tensors,\n",
    "                return_prompt=False,\n",
    "                batch_size=script_args.generator_batch_size,\n",
    "                **generation_kwargs,\n",
    "            )\n",
    "            responses = tokenizer.batch_decode(\n",
    "                response_tensors, skip_special_tokens=True,\n",
    "                spaces_between_special_tokens=False\n",
    "            )\n",
    "\n",
    "            # Postprocess the responses\n",
    "            if script_args.postprocess_responses:\n",
    "                responses = [postprocess_response(x) for x in responses]\n",
    "            batch[\"response\"] = responses\n",
    "\n",
    "            # Compute the rewards (scores)\n",
    "            texts = [q + \" \" + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "            rewards = get_rewards(texts)\n",
    "\n",
    "            # Replace reward for undesired answers to -1\n",
    "            mask = [not is_answer_yes_no(x) for x in batch[\"response\"]]\n",
    "            mask = torch.tensor(mask, dtype=torch.bool) # cast to tensor\n",
    "            rewards[mask] = -1\n",
    "\n",
    "            # Make the rewards a list of tensors\n",
    "            rewards = [x for x in rewards]\n",
    "\n",
    "            # Run PPO step\n",
    "            stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "            ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `generation_kwargs` look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": 100_000,\n",
    "    \"pad_to_multiple_of\": 8,\n",
    "    \"max_new_tokens\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I hope you found this blog as interesting as it was for me to work on this project. I feel like I have learnt a lot during it, for example, I joined multiple ML communities and got involved in discussions with very smart and ambitious people. Perhaps my proudest achievement is making my first open-source contribution to the `elk` library (<a href=\"https://github.com/EleutherAI/elk\" target=\"_blank\">link</a>), as well as reported multiple bugs to the _big-refactor_ branch of _Language Model Evaluation Harness_ (<a href=\"https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor\" target=\"_blank\">link</a>).\n",
    "\n",
    "I am excited to dive deeper into LLMs-related topics in the future. Feel free to reach out if you have any opportunities on offer!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "easy-captcha-fastai-v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
